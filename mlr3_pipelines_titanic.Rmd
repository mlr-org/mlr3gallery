---
title: mlr3pipelines on titanic
author: Florian Pfisterer
date: "3/19/2020"
slug: basics_titanic
categories: []
tags: ['imputation', 'random forest', 'classification', 'mlr3pipelines', 'feature-engineering']
packages: ['mlr3', 'mlr3data', mlr3learners', 'mlr3pipelines', 'skimr', 'DataExplorer', 'stringi', 'ggplot2']
---

```{r, include = FALSE, warning = FALSE, message = FALSE}
# Just some preparation
knitr::opts_chunk$set(
  cache = FALSE,
  collapse = TRUE,
  comment = "#>"
)
if (require("data.table")) data.table::setDTthreads(1)
options(width = 90)
set.seed(8008135)
lgr::get_logger("mlr3")$set_threshold("warn")
library(skimr)
```

This is the second past of the titanic use case series.
In this section we will focus on the usage of `mlr3pipelines`.

## Imputation

A very simple way to do this to just impute a constant value for each 
feature, we could i.e. impute every `character` or `factor` column  with `missing` and 
every numeric column with `-999`.
And depending on the model, this might actually be fine.
This approach has a few drawbacks though:

* `-999` could be a real value in the data.
* imputing `-999` skews the distribution of the data, which might result in bad models.

As a result, instead of imputing a constant value, we will do two things:
* Draw samples from each numeric features' histogram using `PipeOpImputeHist`
* Add an additional column for each `variable` that indicates whether a value was missing or not.
  If the information that a value was missing is important, this column contains this information.

This imputation scheme is called 'imputation with constants' and is already implemented in `mlr3pipelines`.
It can be done using `PipeOpImputeConstant`.

Before imputation, our data looks as follows:

```{r}
task$missings()
```


Let's first deal with the categorical variables:

```{r}
po_newlvl <- po("imputenewlvl")
task_newlvl <- po_newlvl$train(list(task))[[1]]
```

Note that we use the `PipeOp` in an unusual way, which is why the syntax does not look
very clean. We'll learn how to use a full graph below.

First, let's look at the result:

```{r}
task_newlvl$missings()
```

Cool! `embarked` does not have missing values anymore. Note that `PipeOpImputeNewLvl` by default
affects `character`, `factor` and `ordered` columns.


For the `numeric` features we want to do two things, impute values and add an indicator column.
In order to do this, we need a more complicated structure, a `Graph`.

Our `po_indicator` creates the indicator column. We tell it to only do this for `numeric` and `integer` columns
via its `param_vals`, and additionally tell it to create a numeric column (0 = "not missing", 1 = "missing").

```{r}
po_indicator <- po("missind",
  param_vals = list(affect_columns = selector_type(c("numeric", "integer")), type = "numeric")
)
```

Now we can simultaneously impute features from the histogram and create indicator columns.
This can be achieved using the `gunion` function, which puts two operations in parallel:

```{r}
gr <- gunion(list(po_indicator, po("imputehist")))
gr <- gr %>>% po("featureunion")
```

Afterwards, we `cbind` the resulting data using `po("featureunion")`, connecting the different 
operations using our **graph connector**: `%>>%`.

We can now also connect the newlvl imputation:

```{r}
gr <- gr %>>% po("imputenewlvl")
```

and see what happens when we now train the whole **Graph**:

```{r}
task_imputed <- gr$clone()$train(task)[[1]]
task_imputed$missings()
```

Awesome, now we do not have any missing values!

```{r}
autoplot(task_imputed)
```

We could now use `task_imputed` for resampling and see whether a **ranger** model does better.
But this is dangerous! 
If we preprocess all training data at once, data could leak through the different cross-validation folds.
In order to do this properly, we have to process the training data in every fold separately.
Luckily, this is automatically handled in our `Graph`, if we use it through a `GraphLearner`.

We can simple append a `ranger` learner to the Graph and create a `GraphLearner` from this.

```{r}
glrn <- GraphLearner$new(gr$clone() %>>% po(learner2))
```

```{r}
res <- resample(task, glrn, rdesc, store_models = TRUE)
agg <- res$aggregate(msr("classif.acc"))
agg
```

So our model has not improved heavily, currently it has an accuracy of  `r round(agg, 3)`.
In order to improve more, we might need to do some feature engineering.

## Feature Engineering

We will do this using `PipeOpMutate` in order to showcase the power of `mlr3pipelines`.
Additionally, we will make use of the `character` columns, and thus re-select them:

```{r}
task$col_roles$feature <- c(task$feature_names, c("cabin", "name", "ticket"))
```

```{r}
library("stringi")
po_ftextract <- po("mutate", param_vals = list(
  mutation = list(
    fare_per_person = ~ fare / (parch + sib_sp + 1),
    deck = ~ factor(stri_sub(cabin, 1, 1)),
    title = ~ factor(stri_match(name, regex = ", (.*)\\.")[, 2]),
    surname = ~ factor(stri_match(name, regex = "(.*),")[, 2]),
    ticket_prefix = ~ factor(stri_replace_all_fixed(stri_trim(stri_match(ticket, regex = "(.*) ")[, 2]), ".", ""))
  )
))
```

Quickly checking what happens:

```{r}
task_eng <- po_ftextract$clone()$train(list(task))[[1]]
task_eng$data()
```

```{r}
autoplot(task_eng$clone()$select(c("sex", "age")), type = "pairs")
```

Now we can put everything together again, we concatenate our new `PipeOp` with the `Graph` created above and use `PipeOpSelect` in order to de-select the `character` features we used for feature extraction.
Additionally, we collapse the 'surname', so only surnames that make up more than 0.6 \% of the data are kept.

In summary we do the following: 

* `mutate`: The `po_ftextract` we defined above extracts additional features from the data.
* `collapsefactors`: Removes factor levels that make up less then 3 \% of the data.
* `select`: Drops `character` columns.
* `gunion`: Puts two `PipeOp`s in parallel.
  * `missind`: `po_indicator` adds a column for each numeric with the info whether the value is NA or not.
  * `imputehist`: Imputes numeric and integer columns by sampling from the histogram.
* `featureunion`: Cbind's parallel data streams.
* `imputenewlvl`: Imputes factor and ordered columns.
* `fixfactors`: Removes empty factor levels and removes factor levels that do not exist during training.
* `imputesample`: In some cases, if missing factor levels do not occur during training but only while predicting, `imputenewlvl` does not create a new level. For those, we sample a random value.
* `learner`: Appends a learner to the `Graph`.

The full graph we created is the following:

```{r}
learner2 <- lrn("classif.ranger")
learner2$param_set$values <- list(num.trees = 500, min.node.size = 4)
```

```{r}
gr_final <- po_ftextract %>>%
  po("collapsefactors", param_vals = list(no_collapse_above_prevalence = 0.03)) %>>%
  po("select", param_vals = list(selector = selector_invert(selector_type("character")))) %>>%
  gunion(list(po_indicator, po("imputehist"))) %>>%
  po("featureunion") %>>%
  po("imputenewlvl") %>>%
  po("fixfactors") %>>%
  po("imputesample") %>>%
  po(learner2)
```

## Evaluation 

Let us see if things have improved:

```{r}
glrn <- GraphLearner$new(gr_final)
res <- resample(task, glrn, rdesc, store_models = TRUE)
agg_ranger <- res$aggregate(msr("classif.acc"))
agg_ranger
```


```{r}

```


We have improved even more!
But there are many more things to explore!
We could extract even more information from the different features and see what happens.

```{r}
task = task_eng
learner = lrn("classif.rpart", predict_type = "prob")
resampling = rsmp("cv")
object = resample(task, learner, resampling)
autoplot(object)
autoplot(object, type = "histogram", bins = 30)
autoplot(object, type = "roc")
autoplot(object, type = "prc")
```

## Future

But now you are left to yourself! There are many [kaggle kernels](https://www.kaggle.com/c/titanic) that treat the **Titanic Dataset**
available. This can be a great starter to find even better models.

## Appendix

### R Pro Tips

* What are the arguments of `lrn()`, `tsk()`, etc. again? -> Think about the corresponding dictionary.

```{r}
mlr_learners
mlr_tasks
mlr_measures
mlr_resamplings
```

* What are the arguments of a `$new()` constructor?

```{r}
formals(TaskClassif$public_methods$initialize)
```

* What are the possible slots and functions of an object?

```{r}
# Writing `pred_rf$`, and pressing <TAB> should work.
# Otherwise:
names(pred_rf)

# try names without `()` first
# and see if it is a function
```

* How do I see the help file of an object

```{r}
# The documentation is organized by object classes
class(pred_rf)

# use ?PredictionClassif, ?Prediction etc.
# Try all elements listed in the class
```
