---
title: Feature Engineering of Date-Time Variables
author: Lennart Schneider
date: '2020-02-20'
slug: date-features
categories: []
tags: ['feature-engineering', 'mlr3pipelines', 'random-forest', 'regression']
packages: ['mlr3', 'mlr3learners', 'mlr3pipelines', 'ranger']
---

```{r, echo=FALSE}
library(mlr3book)
```

In this tutorial, we demonstrate how `r mlr_pkg("mlr3pipelines")` can be used to easily engineer
features based on date-time variables. Relying on the [Bike Sharing
Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) and the
`r ref("mlr_learners_regr.ranger", "ranger learner")` we compare the RMSE of a random forest using
the original features (baseline), to the RMSE of a random forest using newly engineered features on
top of the original ones.

## Motivation
A single Date-Time variable (i.e., a `POSIXct` column) contains plenty of information ranging from
year, month, day, hour, minute and second to other features such as week of the year, or day of the
week. Moreover, most of these features are of cyclical nature, i.e., the eleventh and twelfth hour
of a day are one hour apart, but so are the 23rd hour and midnight of the other day (see also this
[blog post](http://blog.davidkaleko.com/feature-engineering-cyclical-features.html) and
[fastai](https://docs.fast.ai/tabular.transform.html#Treating-date-columns) for more information).

Not respecting this cyclical nature results in treating hours on a linear continuum. One way to
handle a cyclical feature, $\mathbf{x}$, is to compute the sines and cosines transformation of
$\frac{2 \pi \mathbf{x}}{max_{\mathbf{x}}}$. This results in a two-dimensional representation of the
feature:
```{r, echo=FALSE, fig.height=5.5, fig.width=10, fig.align = "center"}
hours = 0:23
hours_scaled = (2 * pi * hours) / 24
hours_sin = sin(hours_scaled)
hours_cos = cos(hours_scaled)
n = 24
cols = rainbow(n, s = 1, v = 1, start = 0, end = max(1, n - 1) / n, alpha = 1)
par(mfrow = c(1, 2))
plot(hours, type = "n", xlab = "Index", ylab = "Hours")
text(hours, col = cols, labels = as.character(hours))
plot(hours_sin, hours_cos, type = "n", xlab = "Sinus Transformation", ylab = "Cosines Transformation")
text(hours_sin, hours_cos, col = cols, labels = as.character(hours))
```

`r mlr_pkg("mlr3pipelines")` provides the `PipeOpDateFeatures` pipeline which can be used to
automatically engineer features based on `POSIXct` columns, including handling of cyclical features.

## Bike Sharing
The [Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) contains
the hourly count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the
corresponding weather and seasonal information:
```{r}
tmp <- tempfile()
download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip", tmp)
bikes = read.csv(unz(tmp, filename = "hour.csv"), as.is = TRUE)
bikes$season = factor(bikes$season, labels = c("winter", "spring", "summer", "fall"))
bikes$holiday = as.logical(bikes$holiday)
bikes$workingday = as.logical(bikes$workingday)
bikes$weathersit = as.factor(bikes$weathersit)
str(bikes)
```

The original dataset does not contain a `POSIXct` column, but we can easily generate one based on
the other variables available (note that as no information regarding minutes and seconds is
available, we set them to `:00:00`):
```{r}
bikes$date = as.POSIXct(strptime(paste0(bikes$dteday, " ", bikes$hr, ":00:00"), tz = "GMT",
  format = "%Y-%m-%d %H:%M:%S"))
summary(bikes$date)
```

Using a random forest, we want to predict the total count of rental bikes `cnt`.

## Baseline
We construct a new regression task and create a vector of train and test indices:
```{r}
library(mlr3)
library(mlr3learners)
set.seed(2906)
tsk = TaskRegr$new("bikes", backend = bikes, target = "cnt")
train.idx = sample(seq_len(tsk$nrow), size = 0.7 * tsk$nrow)
test.idx = setdiff(seq_len(tsk$nrow), train.idx)
```

This allows us to construct a train and test task:
```{r}
tsk_train = tsk$clone()$filter(train.idx)
tsk_test = tsk$clone()$filter(test.idx)
```

To estimate the performance on future data, we will use `5-fold cross-validation`:
```{r}
cv5 = rsmp("cv", folds = 5)
```

As our baseline model, we use a random forest, `r ref("mlr_learners_regr.ranger", "ranger learner")`.
For the baseline, we only use the original features that are sensible and drop `instant` (record
index), `dteday` (year-month-day as a `character`, not usable), `date` (our new `POSIXct` variable
which we will only use later), `casual` (count of casual users) and `registered` (count of
registered users). Note that `casual` and `registered` adds up to `cnt`.
```{r}
lrn_rf = lrn("regr.ranger")
tsk_train_rf = tsk_train$clone()$select(setdiff(tsk$feature_names,
  c("instant", "dteday", "date", "casual", "registered")))
```

We can then use `resample` with `5-fold cross-validation`:
```{r}
res_rf = resample(tsk_train_rf, learner = lrn_rf, resampling = cv5)
res_rf$score(msr("regr.mse"))
```

The average RMSE is given by:
```{r}
sprintf("RMSE ranger original features: %s", round(sqrt(res_rf$aggregate()), digits = 2))
```

We now want to improve our baseline model by using newly engineered features based on the `date`
`POSIXct` column.

## PipeOpDateFeatures
To engineer new features we use `PipeOpDateFeatures`. This pipeline automatically dispatches on
`POSIXct` columns of the data and by default adds plenty of new date-time related features (for
details, see `r help(PipeOpDateFeatures)`). Here, we want to add all expect for `minute` and
`second`, because this information is not available. As we additionally want to use cyclical
versions of the features we set `cyclic = TRUE`:
```{r}
library(mlr3pipelines)
pop = po("datefeatures", param_vals = list(cyclic = TRUE, minute = FALSE, second = FALSE))
```

Training this pipeline will result in simply adding the new features (and removing the original
`POSIXct` feature(s) used for the feature engineering, see also the `keep_date_var` parameter). In
our training task, we can now also drop the features, `yr`, `mnth`, `hr`, and `weekday`, because our
pipeline will generate these anyways:
```{r}
tsk_train_ex = tsk_train$clone()$select(setdiff(tsk$feature_names,
  c("instant", "dteday", "yr", "mnth", "hr", "weekday", "casual", "registered")))
pop$train(list(tsk_train_ex))
```

To combine this feature engineering step with a random forest,
`r ref("mlr_learners_regr.ranger", "ranger learner")`, we now construct a `GraphLearner`.

## Using the New Features in a GraphLearner
We create a `GraphLearner` consisting of the `PipeOpDateFeatures` pipeline and a
`r ref("mlr_learners_regr.ranger", "ranger learner")`. This `GraphLearner` then behaves like any
other `Learner`:
```{r}
grl = GraphLearner$new(
  po("datefeatures", param_vals = list(cyclic = TRUE, minute = FALSE, second = FALSE)) %>>%
  po("learner", lrn("regr.ranger"))
)
```

Using `resample` with `5-fold cross-validation` on the training task yields:
```{r}
tsk_train_grl = tsk_train$clone()$select(setdiff(tsk$feature_names,
  c("instant", "dteday", "yr", "mnth", "hr", "weekday", "casual", "registered")))
res_grl = resample(tsk_train_grl, learner = grl, resampling = cv5)
res_grl$score(msr("regr.mse"))
```

The average RMSE is given by
```{r}
sprintf("RMSE graph learner date features: %s", round(sqrt(res_grl$aggregate()), digits = 2))
```
and therefore improved by almost 30%.

Finally, we fit our `GraphLearner` on the complete training task and predict on the test task:
```{r}
tsk_train$select(setdiff(tsk$feature_names,
  c("instant", "dteday", "yr", "mnth", "hr", "weekday", "casual", "registered")))
grl$train(tsk_train)
```
```{r}
tsk_test$select(setdiff(tsk$feature_names,
  c("instant", "dteday", "yr", "mnth", "hr", "weekday", "casual", "registered")))
pred = grl$predict(tsk_test)
pred$score(msr("regr.mse"))
```
