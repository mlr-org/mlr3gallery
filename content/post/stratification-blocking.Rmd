---
title: "Resampling: stratified, blocked and predefined"
author: "Milan Dragicevic & Giuseppe Casalicchio"
date: '2020-03-30'
slug: stratified-blocked-predefined-cross-validation
categories: []
tags: ['stratified resampling', 'blocked resampling', 'predefined folds']
packages: ['mlr3', 'mlbench', 'tidyverse', 'caret']
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Intro

When evaluating machine learning algorithms through resampling, it is preferable that each train/test partition will be a representative subset of the whole data set.
This post covers three ways to achieve such reliable resampling procedures:
  
  1. Stratified resampling for classification problems where each train/test split maintains the target class distribution of the original data set.  
  2. Blocked resampling where a grouping factor determines which observations should be together in train/test splits.  
  3. Using predefined and manually created folds for the train/test splits.  

## Prerequisites

```{r packages}
library(mlr3)
library(mlbench)
library(tidyverse)
```

## Stratified resampling

In classification tasks, the ratio of the target class distribution should be similar in each train/test split, which is achieved by stratification.  

In `mlr3`, each [task](https://mlr3.mlr-org.com/reference/Task.html) has a slot [`col_roles`](https://mlr3.mlr-org.com/reference/Task.html#active-bindings). 
We can specify the column used for stratification in `task$col_roles$stratum`. 
This will be illustrated in the following example using the `german_credit` data:  

```{r cv4}
gc_tsk = tsk("german_credit")
```

By default, the `col_roles` slot shows the roles of the columns that will be used as `feature`s and the `target` feature:

```{r col_roles}
gc_tsk$col_roles
```

We use the target feature called `credit_risk` to specify the column used for stratification:

```{r col_roles_startum}
gc_tsk$col_roles$stratum = "credit_risk"
```

After specification of `task$col_roles$stratum` the [`task$strata`](https://mlr3.mlr-org.com/reference/Task.html#active-bindings) active binding will show the number of observations in each group and the corresponding row id's:

```{r cv5}
gc_tsk$strata
```

Specify 3-fold cross validation and instantiate the resampling on the task:

```{r cv1}
cv3 = rsmp("cv", folds = 3)

set.seed(123)
cv3$instantiate(gc_tsk)
cv3$instance
```

Check if the target class distribution is similar in each fold:

```{r cv2}
cv3$instance %>%
  left_join(gc_tsk$data() %>%
      mutate(row_id = 1:n()) %>%
      select(credit_risk, row_id)) %>%
  group_by(fold) %>%
  summarise(class_ratio = sum(.$credit_risk == "bad")/sum(.$credit_risk == "good"))
```

And compare it with the target class distribution from the whole data set:

```{r cv3}
gc_tsk$data() %>%
  summarise(class_ratio = sum(.$credit_risk == "bad")/sum(.$credit_risk == "good"))
```

Note that the column used for stratification does not necessarily have to be the target class.
In fact, multiple discrete variables can be used:

```{r cv4}
gc_tsk$col_roles$stratum = c("housing", "telephone")
gc_tsk$strata
```

To check if stratification works in this case, a 2-fold CV will be specified and instantiated:

```{r cv6}
cv2 = rsmp("cv", folds = 2)

set.seed(123)
cv2$instantiate(gc_tsk)
cv2$instance
```

Again, we check the ratio of observations in each group (combination of groups) will be checked:  

```{r cv7}
cv2$instance %>%
  left_join(gc_tsk$data() %>%
              mutate(row_id = 1:n()) %>%
              select(housing, telephone, row_id)) %>%
  group_by(fold, housing, telephone) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  mutate(ratio = n/n()) 
```

It is evident that each combination of "housing" and "telephone" have a similar ratio of observations in each fold.

## Blocked resampling

The following example is based on the [BreastCancer](https://www.rdocumentation.org/packages/mlbench/versions/2.1-1/topics/BreastCancer) data set from the `mlbench` package:

```{r create task}
data(BreastCancer, package = "mlbench")

bc_tsk = TaskClassif$new(id = "BreastCancer",
  backend = BreastCancer,
  target = "Class",
  positive = "malignant")

# remove Id column as feature
bc_tsk$col_roles$group = "Id"
```

An additional concern when specifying resampling is respecting the natural grouping of the data.
In the [BreastCancer](https://www.rdocumentation.org/packages/mlbench/versions/2.1-1/topics/BreastCancer) data set, for example, several observations have the same "Id" (Sample code number) which implies these are samples taken from the same patient at different times.

```{r Id}
# Let's count how many observation actually have the same Id more than once
sum(table(BreastCancer$Id) > 1)
```

There are 46 Id's with more than one observation (row).  

The model trained on this data set will be used to predict cancer status in new patients. 
Hence, we have to make sure that each `Id` occurs exactly in one fold, i.e., all observations with the same `Id` should be either used for training or for evaluating the model.
This way, we get less biased performance estimates via k-fold cross validation.
This is known as blocked cross validation. 
The following example will illustrate blocked cross validation which can be achieved by specifying a blocking factor in the [`task$col_roles$group`](https://mlr3.mlr-org.com/reference/Task.html#active-bindings) slot:  

```{r create task_ blocked}
cv5 = rsmp("cv", folds = 5)
set.seed(123)
cv5$instantiate(bc_tsk)
cv5$instance
```

In this case, the `row_id` column of the `cv5$instance` slot refers to values of the grouping variable "Id". 
Additionally, the number of rows of the `cv5$instance` is the same as the number of unique groups:

```{r instance_blocked}
all(cv5$instance$row_id %in% BreastCancer$Id)
nrow(cv5$instance) == length(unique(BreastCancer$Id))
```

To check if the specified groups are respected we can create a data frame with the specified folds:  

```{r grouped_folds1}
grouped_folds = map_df(1:5, function(x){
  data.frame(row_id = cv5$test_set(x),
    fold = x)
}) 
```

And check if any of the Ids is present in more than one folds:  

```{r grouped_folds2}
grouped_folds %>%
  left_join(bc_tsk$data() %>%
      mutate(row_id = as.character(1:n())) %>%
      select(Class, Id, row_id))  %>%
  group_by(Id) %>%
  summarise(unique_folds = length(unique(fold))) %>%
  filter(unique_folds > 1)
```

As expected there are no Id's present in more than one fold.

## Resampling with predefined folds

In some circumstances on specific tasks it may be desirable to specify predefined fold indexes. This can also be achieved using the [`task$col_roles$group`](https://mlr3.mlr-org.com/reference/Task.html#active-bindings) when using k-fold cross validation.

First create some predefined folds:  

```{r predefined_folds1}
set.seed(1)
folds = sample(rep(1:5, length.out = nrow(BreastCancer)),
               size = nrow(BreastCancer),
               replace = F)
head(folds, 20)
table(folds)
```

and specify them as a blocking factor:

```{r predefined_folds2}
bc_tsk = TaskClassif$new(id = "BreastCancer",
                         backend = data.frame(BreastCancer, folds = as.factor(folds)),
                         target = "Class",
                         positive = "malignant")
bc_tsk$col_roles$group = "folds"
bc_tsk$col_roles$feature = bc_tsk$col_roles$feature[!bc_tsk$col_roles$feature %in% c("folds", "Id")]
cv5 = rsmp("cv", folds = 5)
set.seed(123)
cv5$instantiate(bc_tsk)
cv5$instance
```

Since we have only five predetermined folds the `cv5$instance` data table has five rows.
To check if the predefined groups are respected:

```{r predefined_folds3}
grouped_folds = map_df(1:5, function(x){
  data.frame(row_id = cv5$test_set(x),
             fold = x)
}) 
grouped_folds %>%
  left_join(data.frame(row_id = 1:nrow(BreastCancer),
                       folds = folds)) %>%
  group_by(fold, folds) %>%
  n_groups
```

There are five groups so it can be concluded each instantiated fold corresponds to one of the predefined folds.

From the above example it is not clear how to perform repeated k-fold CV or time series CV with predefined indices. This is possible via the [`mlr_resamplings_custom()`](https://mlr3.mlr-org.com/reference/mlr_resamplings_custom.html) to which a list of predefined train and test indices are provided. In the following example custom indexes will be created using [`caret::createMultiFolds()`](https://www.rdocumentation.org/packages/caret/versions/6.0-85/topics/createDataPartition):  


```{r predefined_folds4}
gc_tsk = tsk("german_credit")
train_ind = caret::createMultiFolds(gc_tsk$truth(),
                                    k = 5,
                                    times = 10)
all_ind = seq_along(gc_tsk$truth())
test_ind = lapply(train_ind,
                  function(x) setdiff(all_ind, x))
rc = rsmp("custom")
rc$instantiate(gc_tsk, train_ind, test_ind)
```

To check if it performs as intended:

```{r predefined_folds5}
all.equal(
  rc$train_set(1),
  train_ind[[1]])
```

## Conclusions

This post shows how to control the resampling process when using [mlr3](https://mlr3.mlr-org.com/index.html) in order to account for data specificities.
