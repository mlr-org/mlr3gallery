---
title: "Tuning a stacked learner "
author: "Milan Dragicevic"
date: '2020-03-31'
slug: tune-ensemble
categories: []
tags: ['stacking', 'multilevel stacking', 'ensemble tuning']
packages: ['mlr3', 'mlr3learners', 'mlr3pipelines', 'mlr3filters', 'mlr3tuning', 'paradox', 'glmnet']
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(mlr3)
library(mlr3learners)
library(mlr3pipelines)
library(mlr3filters)
library(mlr3tuning)
library(paradox)
library(glmnet)
```

## Intro 

Multilevel stacking is an ensemble technique where several learners are trained on the data and their predictions are added as new features to extended the orginal data on different levels.
On each level, the extended data is then used to train a new level of learners.
This can be repeated for several iterations until a final learner is trained.
To avoid overfitting, it is advisable to use test set (out-of-bag) predictions in each level.   

In this post, a multilevel stacking example will be described using [mlr3pipelines](https://mlr3pipelines.mlr-org.com/articles/introduction.html). 
A slightly different example is available in the [mlr3book](https://mlr3book.mlr-org.com/pipe-nonlinear.html#multilevel-stacking). 
An important distinction to the book example is in explaining how to tune the hyperparameters of the whole ensamble and each underlying learner jointly.  

In the following pipeline, three learners (`rpart`, `glmnet` and `lda`) will be trained on a sparser feature space obtained using functions from [mlr3filters](https://mlr3filters.mlr-org.com/) on the input data in level 0. 
The test set predictions of these models will then be concatenated with the original data and used as input to the level 1 learners. 
In level 1, the data will first be transformed using PCA and additional three learners (`rpart`, `glmnet` and `lda`) will be trained. 
Finally, the test set predictions from level 1 models will be concatenated with the level 1 input data and a `ranger` learner will be trained. 
The number of features selected by the filters and the number of principal components retained will be tuned jointly with other learner hyperparameters.

## Prerequisites

```{r packages}
library(mlr3)
library(mlr3learners)
library(mlr3pipelines)
library(mlr3filters)
library(mlr3tuning)
library(paradox)
library(glmnet)
```

To illustrate stacking the sonar task will be used:

```{r task}
sonar_task = tsk("sonar")
sonar_task$col_roles$stratum = sonar_task$target_names #stratification
```


## Pipeline creation

### Level 0 

As mentioned the level 0 learners will be rpart, glmnet and lda:

```{r lrn1}
rprt_lrn  = lrn("classif.rpart", predict_type = "prob")
glmnet_lrn =  lrn("classif.glmnet", predict_type = "prob")
lda_lrn = lrn("classif.lda", predict_type = "prob")
```

To use learner out-of-bag predictions [PipeOpLearnerCV()](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_learner_cv.html) will be used:  

```{r lrn2}
rprt_cv1 = po("learner_cv", rprt_lrn , id = "rprt_1")
glmnet_cv1 = po("learner_cv", glmnet_lrn, id = "glmnet_1")
lda_cv1 = po("learner_cv", lda_lrn, id = "lda_1")
```

A sparser representation of the input data will be obtained using the following filters:  

```{r filt}
anova = po("filter", flt("anova"), id = "filt1")
mrmr = po("filter", flt("mrmr") , id = "filt2")
find_cor = po("filter", flt("find_correlation") , id = "filt3")
```

To join everything into level 0, the  [gunion](https://mlr3pipelines.mlr-org.com/reference/gunion.html) function will be used. 
The out-of-bag predictions of all Level 0 learners will be concatenated using [PipeOpFeatureUnion](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_featureunion.html) along with the original data passed via [PipeOpNOP()](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_nop.html):  

```{r lrn3}
level0 = gunion(list(
  anova %>>% rprt_cv1,
  mrmr %>>% glmnet_cv1,
  find_cor %>>% lda_cv1,
  po("nop", id = "nop1")))  %>>%
  po("featureunion", id = "union1")
```

Check how the pipeline looks like:  

```{r lrn4, fig.width=6, fig.height = 6}
level0$plot(html = FALSE)
```

### Level 1

Next to create the level 1 learners:

```{r lrn5}
rprt_cv2 = po("learner_cv", rprt_lrn , id = "rprt_2")
glmnet_cv2 = po("learner_cv", glmnet_lrn, id = "glmnet_2")
lda_cv2 = po("learner_cv", lda_lrn, id = "lda_2")
```

All level 1 learners will as input receive PCA transformed data:

```{r lrn6}
level1 = level0 %>>%
  po("copy", 4) %>>%
  gunion(list(
    po("pca", id = "pca2_1", param_vals = list(scale. = TRUE)) %>>% rprt_cv2,
    po("pca", id = "pca2_2", param_vals = list(scale. = TRUE)) %>>% glmnet_cv2,
    po("pca", id = "pca2_3", param_vals = list(scale. = TRUE)) %>>% lda_cv2,
    po("nop", id = "nop2"))
  )  %>>%
  po("featureunion", id = "union2")
```

Level 1 learner out-of-bag predictions will be concatenated with the input to level 1 and a final ranger learner will be trained:  

```{r lrn7, fig.width=6, fig.height = 7}
ranger_lrn = lrn("classif.ranger")
ranger_lrn$predict_type <- "prob"

ensemble = level1 %>>% ranger_lrn

ensemble$plot(html = FALSE)
```

### Defining the tuning space

In order to tune the ensemble a [paradox](https://paradox.mlr-org.com/) [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) will be created with the desired hyper parameter search space. 

```{r ps1}
ps_ens = ParamSet$new(
  list(
    ParamInt$new("filt1.filter.nfeat", 5, 50),
    ParamInt$new("filt2.filter.nfeat", 5, 50),
    ParamInt$new("filt3.filter.nfeat", 5, 50),
    ParamInt$new("pca2_1.rank.", 3, 50),
    ParamInt$new("pca2_2.rank.", 3, 50),
    ParamInt$new("pca2_3.rank.", 3, 20),
    ParamDbl$new("rprt_1.cp", 0.001, 0.1),
    ParamInt$new("rprt_1.minbucket", 1, 10),
    ParamDbl$new("glmnet_1.alpha", 0, 1),
    ParamDbl$new("rprt_2.cp", 0.001, 0.1),
    ParamInt$new("rprt_2.minbucket", 1, 10),
    ParamDbl$new("glmnet_2.alpha", 0, 1),
    ParamInt$new("classif.ranger.mtry", lower = 1L, upper = 10L),
    ParamDbl$new("classif.ranger.sample.fraction", lower = 0.5, upper = 1),
    ParamInt$new("classif.ranger.num.trees", lower = 5L, upper = 200L)
  ))
```

### Performance comparison

Even with a relatively simple ensemble there is quite a few things to setup. 
Lets check how it fairs versus just the final ranger learner.

To proceed the `ensemble` pipeline will be converted to a [GraphLearner](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html):  

```{r compare1}
ens_lrn =  GraphLearner$new(ensemble)
ens_lrn$predict_type = "prob"
```

The ranger learner search space will be created:

```{r compare2}
ps_ranger = ParamSet$new(
  list(
    ParamInt$new("mtry", lower = 1L, upper = 10L),
    ParamDbl$new("sample.fraction", lower = 0.5, upper = 1),
    ParamInt$new("num.trees", lower = 5L, upper = 200L)
  ))
```

For performance comparison [benchmark](https://mlr3.mlr-org.com/reference/benchmark.html) will be used which requires a design incorporating a list of learners and a list of tasks. 
Here there are two learners and one task. 
Since we would like to compare tuned learners, each of them needs to be converted to an [AutoTuner](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html). 
Resampling strategies for learner tuning (`cv3` in the following code chunk) and benchmarking (`outer_hold`) also need to be defined:  

```{r compare3}
cv3 = rsmp("cv", folds = 3)

learns = list(
  AutoTuner$new(
    learner = ens_lrn,
    resampling = cv3,
    measures = msr("classif.auc"),
    tune_ps = ps_ens,
    terminator = term("evals", n_evals = 2), #to limit running time, increase for sensible comparison 
    tuner = tnr("random_search")
  ),
  AutoTuner$new(
    learner = ranger_lrn,
    resampling = cv3,
    measures = msr("classif.auc"),
    tune_ps = ps_ranger,
    terminator = term("evals", n_evals = 2), #to limit running time
    tuner = tnr("random_search")
  )
)
  
outer_hold = rsmp("holdout", ratio = 0.7)

set.seed(321)
outer_hold$instantiate(sonar_task)

design = benchmark_grid(
  tasks = sonar_task,
  learners = learns,
  resamplings = outer_hold
)

set.seed(123)
bmr = benchmark(design, store_models = TRUE)

bmr$aggregate(msr("classif.auc"))
```

The toy example prefers the ensemble. 
For sensible comparison the number of evaluation of the random search should be increased as well as ranger number of trees. 

## Conclusion

This example shows the versatility of [mlr3pipelines](https://mlr3pipelines.mlr-org.com/articles/introduction.html).
By using more learners, varied representations of the data set as well as more levels, a powerful yet compute hungry pipeline can be created. 
It is important to note care should be taken to avoid name clashes of pipeline objects. 
