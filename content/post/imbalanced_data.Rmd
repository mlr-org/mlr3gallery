---
title: mlr3 imbalanced data
author: Giuseppe Casalicchio
date: '2020-03-30'
slug: mlr3-imbalanced
categories: []
tags: ['mlr3', 'mlr3pipelines', 'mlr3tuning', 'tuning', 'OpenML']
packages: ['OpenML', 'mlr3', 'mlr3learners', 'mlr3pipelines', 'mlr3tuning', 'mlr3viz']
---

# Intro 

This use case compares different approaches to handle class imbalancy for the [`optdigits`](https://www.openml.org/d/980) binary classification data set using the mlr3 package. 
We mainly focus on undersampling the majority class, oversampling the minority class, and the smote imbalancy correction that enriches the minority class with synthetic data. 
The use case requires prior knowledge in basic ML concepts (issues imbalanced data, hyperparameter tuning, nested cross-validation).
The R packages `mlr3`, `mlr3pipelines` and `mlr3tuning` will be used.
You can find most of the content here also in the [mlr3book](https://mlr3book.mlr-org.com/) explained in a more detailed way.

These steps are performed:

* Retrieve data sets from `OpenML`
* Define imbalance correction pipelines (undersampling, oversampling and smote) with `mlr3pipelines`
* Combine pipeline with a learner and autotune it with `mlr3tuning`
* Benchmark the autotuned pipelines and visualize the results using `mlr3viz`

# Prerequisites

## Loading required packages

```{r}
library("mlr3")          # mlr3 base package
library("mlr3misc")      # contains some helper functions
library("mlr3pipelines") # create ML pipelines
library("mlr3tuning")    # tuning ML algorithms
library("mlr3learners")  # additional ML algorithms
library("mlr3viz")       # autoplot for benchmarks
library("paradox")       # hyperparameter space
library("OpenML")        # to obtain data sets
library("smotefamily")   # smote algorithm for imbalancy correction
```

## Retrieve data sets from OpenML

OpenML.org is an open machine learning platform, which allows users to share data, code and machine learning experiments.
The OpenML R package can query available data sets using a filter-like approach by providing desired dataset characteristics like `number.of.classes` or `number.of.features`.

```{r}
# get list of curated binary classification data sets (see https://arxiv.org/abs/1708.03731v2) 
ds = listOMLDataSets(
  number.of.classes = 2, 
  number.of.features = c(1, 100), 
  number.of.instances = c(5000, 10000)
)
# select imbalanced data sets (without categorical features as smote cannot handle them)
ds = subset(ds, minority.class.size/number.of.instances < 0.2 & number.of.symbolic.features == 1)
ds

# pick one data set from list above 
d = getOMLDataSet(980)
d
```

After downloading the chosen data set, we create an `mlr3` classification task:

```{r}
# make sure target is a factor and create mlr3 tasks 
data = as.data.frame(d)
data[[d$target.features]] = as.factor(data[[d$target.features]])
task = TaskClassif$new(id = d$desc$name, backend = data, target = d$target.features)
task
```

Quick overview of the data:
```{r}
skimr::skim(data)
```

## Imbalancy correction

In `mlr3pipelines`, there is a [`classbalancing`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_classbalancing.html) and a [`smote`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_smote.html) pipe operator that can be combined with any learner. 
Below, we define the undersampling, oversampling and smote `PipeOp`s.
All three imbalancy correction methods have hyperparameters to control the degree of class imbalance.
We apply the `PipeOp`s to the current `task` with specific hyperparameter values to see how the class balance changes:

```{r}
# check original class balance
table(task$truth())

# undersample majority class (relative to majority class)
po_under = po("classbalancing", id = "undersample", adjust = "major", 
  reference = "major", shuffle = FALSE, ratio = 1/5)
table(po_under$train(list(task))$output$truth()) # reduce majority class by factor 5

# oversample majority class (relative to majority class)
po_over = po("classbalancing", id = "oversample", adjust = "minor", 
  reference = "minor", shuffle = FALSE, ratio = 5)
table(po_over$train(list(task))$output$truth()) # enrich minority class by factor 5

# smote enriches the minority class with synthetic data
po_smote = po("smote", dup_size = 5)
table(po_smote$train(list(task))$output$truth()) # enrich minority class by factor (dup_size + 1)
```

## Construct `AutoTuner`

We combine the `PipeOp`s with a learner (here `ranger`) to make each pipeline behave like a learner:

```{r}
# create random forest learner
lrn = lrn("classif.ranger", num.trees = 50)

# combine learner with pipeline
lrn_under = GraphLearner$new(po_under %>>% lrn)
lrn_over = GraphLearner$new(po_over %>>% lrn)
lrn_smote = GraphLearner$new(po_smote %>>% lrn)
```

We define the search space in order to tune the hyperparameters of the class imbalancy methods. 

```{r}
# define parameter search space for each method
ps_under = ParamSet$new(list(ParamDbl$new("undersample.ratio", lower = 1/5, upper = 1)))
ps_over = ParamSet$new(list(ParamDbl$new("oversample.ratio", lower = 1, upper = 5)))
ps_smote = ParamSet$new(list(
  ParamInt$new("smote.dup_size", lower = 1, upper = 5),
  ParamInt$new("smote.K", lower = 2, upper = 6)
))
```

We create an `AutoTuner` class from the learner to tune the graph (random forest learner + imbalancy correction method) based on a 3-fold CV using the `classif.fbeta` as performance measure. 
To keep computational runtime low, we define the search space only for the imbalacy correction method. However, one can also jointly tune the hyperparameter of the learner along with the imbalancy correction method by extending the search space with the learner's hyperparameters. 
Note that SMOTE has two hyperparameters `K` and `dup_size`. 
While `K` changes the behavior of the SMOTE algorithm, `dup_size` will affect oversampling rate. 
To focus on the effect of the oversampling rate on the performance, we will consider SMOTE with K = 1 as a different imbalacy correction method as SMOTE with K=2 (and so on).
Hence, we use grid search with 5 different hyperparameter configurations for the undersampling method, the oversampling method and each SMOTE variant for tuning:

```{r}
# define the 
inner_cv3 = rsmp("cv", folds = 3)
measure = msr("classif.fbeta")

learns = list(
  AutoTuner$new(
    learner = lrn_under,
    resampling = inner_cv3,
    measures = measure,
    tune_ps = ps_under,
    terminator = term("none"),
    tuner = tnr("grid_search", resolution = 5)
  ),
  AutoTuner$new(
    learner = lrn_over,
    resampling = inner_cv3,
    measures = measure,
    tune_ps = ps_over,
    terminator = term("none"),
    tuner = tnr("grid_search", resolution = 5)
  ),
  AutoTuner$new(
    learner = lrn_smote,
    resampling = inner_cv3,
    measures = measure,
    tune_ps = ps_smote,
    terminator = term("none"),
    tuner = tnr("grid_search", resolution = 5)
  )
)
names(learns) = mlr3misc::map(learns, "id")
names(learns) 
```

## Benchmark `AutoTuner`

The `AutoTuner` is a fully tuned pipeline that behaves like a usual learner. 
For the tuning a 3-fold CV is used.
Now, we use the `benchmark` function to compare the tuned class imbalancy pipelines based on a holdout for the outer evaluation:

```{r}
# NOTE: This code runs about 3 minutes
outer_resampling = rsmp("holdout")
design = benchmark_grid(
  tasks = task,
  learners = learns,
  resamplings = outer_resampling
)
print(design)

set.seed(1)
# future::plan("multiprocess")
bmr = benchmark(design, store_models = TRUE)
```

### Visualize benchmark results

```{r}
bmr$aggregate(measure)
autoplot(bmr, measure = measure) # one value per boxplot since we used holdout as outer resampling 
```

### Visualize the tuning path

With `store_models = TRUE` we allow the `benchmark` function to store each single model that was computed during tuning. 
Therefore, we can plot the tuning path of the best learner from the subsampling iterations:

```{r}
library(ggplot2)
utune_path = bmr$data$learner[[1]]$archive("params")
utune_gg = ggplot(utune_path, aes(x = undersample.ratio, y = classif.fbeta)) +
  geom_point(size = 3) + 
  geom_line() + ylim(0.9, 1)

otune_path = bmr$data$learner[[2]]$archive("params")
otune_gg = ggplot(otune_path, aes(x = oversample.ratio, y = classif.fbeta)) +
  geom_point(size = 3) + 
  geom_line() + ylim(0.9, 1)

stune_path = bmr$data$learner[[3]]$archive("params")
stune_gg = ggplot(stune_path, aes(x = smote.dup_size, y = classif.fbeta, col = factor(smote.K))) +
  geom_point(size = 3) + 
  geom_line() + ylim(0.9, 1)

library(ggpubr)
ggarrange(utune_gg, otune_gg, stune_gg, common.legend = TRUE, nrow = 1) 
```

## Conclusion

In this post, we tuned and compared 5 different settings of sampling ratios for the undersampling method, the oversampling method and different SMOTE variants (using different values of `K` nearest neighbors during the sampling process). 
If you want to know more, read the [mlr3book](https://mlr3book.mlr-org.com/) and the documentation of the mentioned packages.
