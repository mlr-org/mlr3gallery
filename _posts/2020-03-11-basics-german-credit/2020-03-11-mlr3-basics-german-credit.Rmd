---
title: mlr3 Basics - German Credit
categories:
  - visualization
  - classification
  - feature importance
author:
  - name: Martin Binder
  - name: Florian Pfisterer
date: 03-11-2020
description: |
  mlr3 is a machine learning framework for R. Together with other packages following the naming scheme "mlr3___", it offers functionality around developing, tuning, and evaluating machine learning workflows.
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---

```{r, include=FALSE, warning=FALSE, message=FALSE}
# Just some preparation
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80)
)
library(mlr3book)
if (require("data.table")) data.table::setDTthreads(1)
set.seed(8008135)
lgr::get_logger("mlr3")$set_threshold("warn")
```

## Intro

This is the first part in a serial of tutorials.
The other parts of this series can be found here:

- [Part II - Tuning](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3tuning-tutorial-german-credit/)
- [Part III - Pipelines](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3pipelines-tutorial-german-credit/)

We will walk through this tutorial interactively. The text is kept short to be followed in real time.

## Prerequisites

Ensure all packages used in this tutorial are installed.
This includes packages from the `mlr3` family, as well as other packages for data handling, cleaning and visualization which we are going to use (`r cran_pkg("DataExplorer")`, `r cran_pkg("data.table")`, `r cran_pkg("ggplot2")`, and `r cran_pkg("skimr")`).

Then, load the main packages we are going to use:

```{r, message=FALSE, warning=FALSE}
library("data.table")
library("mlr3")
library("mlr3learners")
library("ggplot2")
```

```{r, include=FALSE}
theme_set(theme_light())
```

## Machine Learning Use Case: German Credit Data

The German credit data was originally donated in 1994 by Prof. Dr. Hans Hoffman of the University of Hamburg.
A Description can be found at the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).
The Goal is to classify people by their credit risk (good or bad) using 20 personal, demographic and financial features.

| Feature Name            | Description                                            |
| ------------------------| ------------------------------------------------------ |
| age                     | age in years                                           |
| amount                  | amount asked by applicant                              |
| credit_history          | past credit history of applicant at this bank          |
| duration                | duration of the credit in months                       |
| employment_duration     | present employment since                               |
| foreign_worker          | is applicant foreign worker?                           |
| housing                 | type of apartment rented, owned, for free / no payment |
| installment_rate        | installment rate in percentage of disposable income    |
| job                     | current job information                                |
| number_credits          | number of existing credits at this bank                |
| other_debtors           | other debtors/guarantors present?                      |
| other_installment_plans | other installment plans the applicant is paying        |
| people_liable           | number of people being liable to provide maintenance   |
| personal_status_sex     | combination of sex and personal status of applicant    |
| present_residence       | present residence since                                |
| property                | properties that applicant has                          |
| purpose                 | reason customer is applying for a loan                 |
| savings                 | savings accounts/bonds at this bank                    |
| status                  | status/balance of checking account at this bank        |
| telephone               | is there any telephone registered for this customer?   |

### Importing the Data

The dataset we are going to use is a transformed version of this German credit dataset, as provided by the `r cran_pkg("rchallenge")` package (this transformed dataset was proposed by Ulrike Gr√∂mping, with factors instead of dummy variables and corrected features).

`r mlr_pkg("mlr3")` provides a predefined `r ref("Task")`, more precisely a `r ref("TaskClassif", "classification Task")` for this dataset.
We can load it using the `tsk()` function.

```{r, message=FALSE}
task = tsk("german_credit")
```

First, we'll do a thorough investigation of the dataset.

### Exploring the Data

FIXME: I'm not 100% sure whether we need all this data exploration

We can get a quick overview of our dataset using R's summary function:

```{r}
german_credit = task$data()
summary(german_credit)
dim(german_credit)
```

Our dataset has `r nrow(german_credit)` observations and `r ncol(german_credit)` columns.
The variable we want to predict is `credit_risk` (either good or bad).
That means we aim to classify people by their credit risk (`"good"` or `"bad"`).

We also recommend the `skimr` (`skimr::skim`) and `DataExplorer` (`DataExplorer::plot_bar`, `DataExplorer::plot_histogram`, `DataExplorer::plot_boxplot`) packages as they create very well readable and understandable overviews.

```{r, R.options=list(width = 120)}
skimr::skim(german_credit)
```

```{r, echo=FALSE}
library(mlr3misc)
german_credit_short = map_dtc(german_credit, function(x) {
  if (is.factor(x) || is.character(x)) {
    levels(x) = abbreviate(stringr::str_trunc(levels(x), 16), 12)
  }
  x
})
```

```{r, eval=FALSE}
DataExplorer::plot_bar(german_credit, nrow = 6, ncol = 3)
```

```{r, echo=FALSE, out.width="100%", fig.height=18}
DataExplorer::plot_bar(german_credit_short, nrow = 6, ncol = 3)
```

```{r, out.width="100%", fig.height=3}
DataExplorer::plot_histogram(german_credit, nrow = 1, ncol = 3)
```

```{r, out.width="100%", fig.height=3}
DataExplorer::plot_boxplot(german_credit, by = "credit_risk", nrow = 1, ncol = 3)
```

During this exploratory analysis meaningful aspects could be:

- Skewed distributions
- Missing values
- Empty / rare factor variables

An explanatory analysis is crucial to get a feeling for your data.
On the other hand the data can be validated this way.
Non plausible data can be investigated or outliers can be removed.

After feeling confident with the data, we want to do modeling now.

## Modeling

Considering how we are going to tackle the problem of classifying the credit risk relates closely to what `mlr3` entities we will use.

The typical questions that arise when building a machine learning workflow are:

- What is the problem we are trying to solve?
- What are appropriate learning algorithms?
- How do we evaluate "good" performance?

More systematically in `mlr3` they can be expressed via five components:

1) The `Task` definition

2) The `Learner` definition

3) The training

4) The prediction

5) The evaluation via `Measure`s

### Task Definition

First of all, we are interested in the target which we want to model.
Most supervised machine learning problems are **regression** or **classification** problems.
However, note that other problems include unsupervised learning or time-to-event data (covered in `r mlr_pkg("mlr3survival")`).

Within `mlr3`, to distinguish between these problems, we define `Task`s.
If we want to solve a classification problem, we define a classification task -- `r ref("TaskClassif")`.
For a regression problem, we define a regression task -- `r ref("TaskRegr")`.

In our case it is clearly our objective to model or predict the binary `factor` variable `credit_risk`.
Thus, we define a `TaskClassif`.

Suppose `mlr3` would not contain a predefined `Task` for the German credit dataset. We could then initialize a classification task using `TaskClassif$new()`.
We need to specify a name for the task (we choose `"GermanCredit"`), the data we want to use (`german_credit`) and name of the target (`"credit_risk"`):

```{r}
task = TaskClassif$new("GermanCredit", backend = german_credit, target = "credit_risk")
```

### Learner Definition

After having decided *what* should be modeled, we need to decide on *how*.
This means we need to decide which learning algorithms, or `Learner`s are appropriate.
Using prior knowledge (e.g. knowing that it is a classification task or assuming that the classes are linearly separable) one ends up with one or more suitable `Learner`s.

Many learners can be obtained via the `r mlr_pkg("mlr3learners")` package.
Additionally, many learners are provided via the `mlr3learners` organization ([github page](https://github.com/mlr3learners)).
These two resources combined account for a large fraction of standard learning algorithms.
As `mlr3` usually only wraps learners from packages, it is even easy to create a formal `Learner` by yourself.
You may find the section 'Extending mlr3' in the `r mlr_pkg("mlr3book")` very helpful.
If you happen to write your own `Learner` in `mlr3`, we would be happy if you share it with the `mlr3` community.

All available `Learner`s (i.e. all which you have installed from `mlr3`, `mlr3learners`, the mlr3learners github page, or self-written ones) can be listed in the following dictionary:

```{r}
mlr_learners
```

For our problem, a suitable learner could be one of the following:
Logistic regression, CART, Random Forest (or many more).

A learner can be initialized with the `lrn` function and the name of the learner, e.g., `lrn("classif.xxx")`.
Use `?mlr_learners_xxx` to open the help page of a learner named `xxx`, or use [the internet](https://mlr3learners.mlr-org.com/reference/index.html).

For example, a logistic regression can be initialized in the following manner (logistic regression uses R's `glm()` function and is provided by the `mlr3learners` package):

```{r}
learner_logreg = lrn("classif.log_reg")
```

Alternatively but equivalently, one could use:

```{r}
learner_logreg = LearnerClassifLogReg$new()
```

### Training

Training is the procedure, where a model is fitted on the (training) data.

#### Logistic Regression

We start with the example of the logistic regression.
However, you will immediately see that the procedure generalizes to any learner very easily.

An initialized learner can be trained on data using `$train()`.

```{r}
learner_logreg$train(task)
```

Typically, in machine learning, one does not use the full data which is available but a subset, the so-called training data.

To efficiently perform a split of the data one could do the following:

```{r}
train_set = sample(task$row_ids, 0.8 * task$nrow)
test_set = setdiff(task$row_ids, train_set)
```

80 percent of the data is used for training.
The remaining 20 percent are used for evaluation at a subsequent later point in time.
`train_set` is an integer vector displaying the selected rows of the original dataset:

```{r}
head(train_set)
```

In `mlr3` the training with a subset of the data can be declared by the additional argument `row_ids = train_set`:

```{r}
learner_logreg$train(task, row_ids = train_set)
```

The fitted model can be accessed via:

```{r, eval=FALSE}
learner_logreg$model
```

The stored object is a normal `glm` object and all its `S3` methods work as expected.

```{r eval=FALSE}
class(learner_logreg$model)
summary(learner_logreg$model)
```

#### Random Forest

Just like the logistic regression, we could train a random forest instead.
We use the fast implementation from the `r cran_pkg("ranger")` package.
To do so we first need to define the learner and then actually train it.

We now additionally supply the importance argument (`importance = "permutation"`).
Doing so, we override the default and let the learner do feature importance determination based on permutation feature importance.

```{r}
learner_rf = lrn("classif.ranger", importance = "permutation")
learner_rf$train(task, row_ids = train_set)
```

We can access the importance values using `$importance()`.

```{r, eval=FALSE}
learner_rf$importance()
```

In order to obtain a better plot, we convert the importance to a `data.table`.

```{r}
importance = as.data.table(learner_rf$importance(), keep.rownames = TRUE)
colnames(importance) = c("Feature", "Importance")
```

```{r}
ggplot(importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col() + coord_flip() + xlab("")
```

### Prediction

Let's see what the models predict.

After training a  model, the model can be used for prediction.
Usually, prediction is the main purpose of machine learning models.

In our case, the model can be used to classify new credit applicants w.r.t. their associated credit risk (good vs. bad) on the basis of the features.
Typically, machine learning models predict numeric values.
In the regression case this is very natural.
For classification, most models predict scores or probabilities.
Based on these values, one can derive class predictions.

#### Predict Classes

First, we directly predict classes.
We will clone the original task and filter it to only keep the rows of the previously defined `test_set`:

```{r}
task_predict = task$clone()
task_predict$filter(test_set)
pred_logreg = learner_logreg$predict(task_predict)
pred_rf = learner_rf$predict(task_predict)
```

```{r}
pred_logreg
```

```{r}
pred_rf
```

The `predict()` function returns  a `r ref("Prediction")` object.
It can be converted to a `data.table` if one wants to use it downstream.

We can also display the prediction results aggregated in a confusion matrix.

```{r}
pred_logreg$confusion
pred_rf$confusion
```

#### Predict Probabilities

Most learners may not only predict a class variable ("response"), but also their degree of "belief" / "uncertainty" in a given response.
Typically, we achieve this by setting the `$predict_type` slot of a `Learner` to `"prob"`.
Sometimes this needs to be done *before* the learner is trained.
Alternatively, we can directly create the learner with this option:
`lrn("classif.log_reg", predict_type = "prob")`.

```{r}
learner_logreg$predict_type = "prob"
```

```{r}
learner_logreg$predict(task_predict)
```

Note that sometimes one needs to be cautious when dealing with the probability interpretation of the predictions.

### Performance Evaluation

To measure the performance of a learner on new unseen data, we usually mimic the scenario of unseen data by splitting up the data into training and test set.
The training set is used for training the learner, and the test set is only used for predicting and evaluating the performance of the trained learner.
Numerous resampling methods (cross-validation, bootstrap) repeat the splitting process in different ways.

Within `mlr3`, we need to specify the resampling strategy using the `rsmp()` function:

```{r}
resampling = rsmp("holdout", ratio = 2/3)
print(resampling)
```

Here, we use "holdout", a simple train-test split (with just one iteration).
We use the `resample()` function to undertake the resampling calculation:

```{r}
res = resample(task, learner = learner_logreg, resampling = resampling)
res
```

The default score of the measure is included in the `$aggregate()` slot:

```{r}
res$aggregate()
```

The default measure in this scenario is `classif.ce`.
This refers to the classification error.
A low value close to zero is desired.

We can easily run different resampling strategies, e.g. repeated holdout (`"subsampling"`), or cross validation.
Most methods perform repeated train/predict cycles on different data subsets and aggregate the result (usually as the mean).
Doing this manually would require us to write loops.
`mlr3` does the job for us.

```{r}
res_sub = resample(task, learner = learner_logreg, resampling = rsmp("subsampling", repeats = 10))
res_sub$aggregate()
```

Instead, we could also apply cross-validation.

```{r}
res_cv = resample(task, learner = learner_logreg, resampling = rsmp("cv", folds = 10))
res_cv$aggregate()
```

`mlr3` features scores for many more measures.
Here, we apply `msr("classif.fpr")` for the false positive rate, or `msr("classif.fnr")` for the false negative rate.
Multiple measures are entered via a list:

```{r}
# false positive rate
res_cv$aggregate(msr("classif.fpr"))
# false positive rate and false negative
msr_list = list(
  msr("classif.fpr"),
  msr("classif.fnr")
)
res_cv$aggregate(msr_list)
```

There are a few more resampling methods, and quite a few more measures.
We list them in

```{r}
mlr_resamplings
```

and

```{r}
mlr_measures
```

To get help on a resampling method, use `?mlr_resamplings_xxx`, for a measure do `?mlr_measures_xxx`.
You can also use the [mlr3 reference](https://mlr3.mlr-org.com/reference/index.html) online.

Note that some measures, for example AUC, require a "probability" prediction, instead of a response prediction.

### Performance Comparison and Benchmarks

We could compare `Learners` by evaluating `resample()` for each of them manually.
However, `benchmark()` automatically performs resampling evaluations for multiple learners and tasks.
`benchmark_grid()` creates fully crossed designs:
Multiple `Learner`s for multiple `Task`s are compared w.r.t. multiple `Resampling`s.

```{r}
lrn_list = list(
  lrn("classif.log_reg", predict_type = "prob"),
  lrn("classif.ranger", predict_type = "prob")
)

bm_design = benchmark_grid(task, learners = lrn_list, resamplings = rsmp("cv", folds = 10))
bmr = benchmark(bm_design)
```

Careful, large benchmarks may take a long time! This one should take less than a minute, however.
In general, we want use parallelization to speed things up on multi-core machines.
For parallelization, `mlr3` relies on the `r cran_pkg("future")` package:

```{r, eval=FALSE}
#future::plan("multiprocess") # uncomment for parallelization
```

In the benchmark we can compare different measures.
We compare misclassification rate and AUC.

```{r}
msr_list = list(msr("classif.ce"), msr("classif.auc"))
performances = bmr$aggregate(msr_list)
performances[, c("learner_id", "classif.ce", "classif.auc")]
```

We see that the two models perform very similarly.

## Deviating from package defaults

The previously shown techniques build the backbone of a `mlr3`-featured machine learning workflow.
However, in most cases one would never proceed in the way we did.
While many R packages have carefully selected default settings, they will not perform optimally in any scenario.
Typically, we can select the values of such hyperparameters.
The (hyper)parameters of a `Learner` can be accessed via its `$param_set`:

```{r, eval=FALSE}
learner_rf$param_set
```

We can parameterize our learners in two distinct manners.
If we have prior knowledge on how the learner should be (hyper-)parameterized, the way to go would be manually entering the parameters in the parameter set.
In most cases, however, we would want to tune the learner so that it can search good model configurations itself.
For now, we only want to compare a small number of models.

To get an idea on which parameters can be manipulated, we can investigate the parameters of the original package version.

```{r, eval=FALSE}
?ranger::ranger
```

For the random forest two meaningful parameters which steer model complexity are `num.trees` and `mtry`.

`num.trees` defaults to `500` and `mtry` to `floor(sqrt(ncol(data) - 1))`, in our case `r floor(sqrt(ncol(german_credit) - 1))`.

In the following we aim to train three different learners:

1) The default random forest.
2) A random forest with low `num.trees` and low `mtry`.
3) A random forest with high `num.trees` and high `mtry`.

We will benchmark their performance on the German credit dataset.

We construct these three learners and set the parameters with a named list.

```{r}
learner_rf_medium = lrn("classif.ranger", predict_type = "prob")

learner_rf_low = lrn("classif.ranger", predict_type = "prob")
learner_rf_low$param_set$values = list(num.trees = 5, mtry = 2)

learner_rf_high = lrn("classif.ranger", predict_type = "prob")
learner_rf_high$param_set$values = list(num.trees = 1000, mtry = 11)
```

Once the learners are defined, we can benchmark them:

```{r}
lrn_list = list(
  learner_rf_low = learner_rf_low,
  learner_rf_medium = learner_rf_medium,
  learner_rf_high = learner_rf_high
)
bm_design = benchmark_grid(task = task, resamplings = rsmp("cv", folds = 10), learners = lrn_list)
```

```{r}
bmr = benchmark(bm_design)
```

We compare misclassification rate and AUC again:

```{r}
msr_list = list(msr("classif.ce"), msr("classif.auc"))
performances = bmr$aggregate(msr_list)
cbind(learner = names(lrn_list), performances[, c("classif.ce", "classif.auc")])
```

All learners seem to have similar performance while the `learner_rf_low` seems to underfit a bit.

## Outlook

This tutorial was a detailed introduction to machine learning workflows within `mlr3`.
Having followed this tutorial you should be able to run your first models yourself.
Next to that we spiked into performance evaluation and benchmarking.
Furthermore, we showed how to customize learners.

The next parts of the tutorial will go more into depth into additional `mlr3` topics:

- [Part II - Tuning](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3tuning-tutorial-german-credit/) introduces you to the `r mlr_pkg("mlr3tuning")` package

- [Part III - Pipelines](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3pipelines-tutorial-german-credit/) introduces you to the `r mlr_pkg("mlr3pipelines")` package

## Appendix

### Tips

- What are the arguments of `tsk()`, `lrn()`, `rsmp()`, `msr()` etc. again? $\rightarrow$ Think about the corresponding dictionary.

```{r}
mlr_tasks

mlr_learners

mlr_resamplings

mlr_measures
```

- What are the arguments of a `$new()` constructor?

```{r}
formals(TaskClassif$public_methods$initialize)
```

- What are the possible slots and functions of an object?
  - Writing `pred_rf$`, and pressing \<TAB\> should work.
    Otherwise:

```{r}
names(pred_rf)
```

- How do I see the help file of an object?
  - The documentation is organized by object classes

```{r}
class(pred_rf)
```

### mlr3 and its Extensions

| Package         | Functionality                                                                                |
| :-------------- | :--------------------------------------------------------------------------------------------|
| `mlr3`          | Framework for machine learning: `Task`, `Learner`, `Measure`, `resample()` and `benchmark()` |
| `mlr3learners`  | Concrete `Learner`s for many popular machine learning implementations                        |
| `mlr3pipelines` | Dataflow programming of machine learning workflows                                           |
| `mlr3tuning`    | Hyperparameter tuning for machine learning algorithms                                        |
| `mlr3filter`    | Feature filtering                                                                            |
| `mlr3viz`       | Visualisations and plots                                                                     |
| `paradox`       | Auxiliary package providing (hyper)parameter handling                                        |
| `mlr3misc`      | Auxiliary functions                                                                          |
