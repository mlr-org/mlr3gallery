---
title: mlr3 Basics - German Credit
categories:
  - visualization
  - classification
  - feature importance
author:
  - name: Martin Binder
  - name: Florian Pfisterer
date: 03-11-2020
description: |
  mlr3 is a machine learning framework for R. Together with other packages following the naming scheme "mlr3___", it offers functionality around developing, tuning, and evaluating machine learning workflows.
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---

```{r, include = FALSE, warning = FALSE, message = FALSE}
# Just some preparation
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80)
)
library(mlr3book)
if (require("data.table")) data.table::setDTthreads(1)
set.seed(8008135)
lgr::get_logger("mlr3")$set_threshold("warn")
```

## Intro

This is the first part in a serial of tutorials.
The other parts of this series can be found here:

- [Part II - Tuning](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3tuning-tutorial-german-credit/)
- [Part III - Pipelines](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3pipelines-tutorial-german-credit/)

`r mlr_pkg("mlr3")` is a machine learning framework for R.
Together with other packages from the same developers, mostly following the naming scheme "mlr3___", it offers functionality around developing, tuning, and evaluating machine learning workflows.

We will walk through this tutorial interactively. The text is kept short to be followed in real time.

## Prerequisites

Ensure all packages used in this tutorial are installed.
This includes packages from the `mlr3` family, as well as other packages for data handling, cleaning and visualization which we are going to use (`r cran_pkg("DataExplorer")`, `r cran_pkg("data.table")`, `r cran_pkg("ggplot2")`, and `r cran_pkg("skimr")`).

Then, load the main packages we are going to use:

```{r, message=FALSE, warning=FALSE}
library("data.table")
library("mlr3")
library("mlr3learners")
library("ggplot2")
```

```{r, include=FALSE}
theme_set(theme_light())
```

## Machine Learning Use Case: German Credit Data

- The German credit data was originally donated in 1994 by Prof. Dr. Hans Hoffman of the University of Hamburg
- A Description can be found at the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)
- The Goal is to classify people by their credit risk (good or bad) using 20 features:


| Feature Name            | Description                                            |
| ------------------------| ------------------------------------------------------ |
| age                     | age in years                                           |
| amount                  | amount asked by applicant                              |
| credit_history          | past credit history of applicant at this bank          |
| duration                | duration of the credit in months                       |
| employment_duration     | present employment since                               |
| foreign_worker          | is applicant foreign worker?                           |
| housing                 | type of apartment rented, owned, for free / no payment |
| installment_rate        | installment rate in percentage of disposable income    |
| job                     | current job information                                |
| number_credits          | number of existing credits at this bank                |
| other_debtors           | other debtors/guarantors present?                      |
| other_installment_plans | other installment plans the applicant is paying        |
| people_liable           | number of people being liable to provide maintenance   |
| personal_status_sex     | combination of sex and personal status of applicant    |
| present_residence       | present residence since                                |
| property                | properties that applicant has                          |
| purpose                 | reason customer is applying for a loan                 |
| savings                 | savings accounts/bonds at this bank                    |
| status                  | status/balance of checking account at this bank        |
| telephone               | is there any telephone registered for this customer?   |

### Importing the Data

The dataset we are going to use is a transformed version of this German credit dataset, as provided by the `r cran_pkg("rchallenge")` package (this transformed dataset was proposed by Ulrike Gr√∂mping, with factors instead of dummy variables and corrected features).

mlr3 provides a predefined `r ref("Task")`, more precisely a `r ref("TaskClassif", "classification Task")` for this dataset.
We can load it using the `tsk()` function.

```{r, message=FALSE}
task = tsk("german_credit")
```

First, we'll do a thorough investigation of the dataset.

```{r, message=FALSE}
credit = task$data()
```

### Exploring the Data

FIXME: I'm not 100% sure whether we need all this data exploration

- We have a look at the dataset before we start modeling
- The `str()` and `summary()` functions give an overview of features and their type
- The `r cran_pkg("skimr")` package gives more readable summaries
- The `r cran_pkg("DataExplorer")` package lets us visualize categorical (`plot_bar()`) and numeric (`plot_histogram()` and `plot_boxplot()`) data, as well as data relationships
- Basic things to watch out for are:
  - Skewed distributions
  - Missing values
  - Empty / rare factor variables

```{r, R.options=list(width = 120)}
skimr::skim(credit)
```

```{r, echo = FALSE}
library(mlr3misc)
credit_short = map_dtc(credit, function(x) {
  if (is.factor(x) || is.character(x)) {
    levels(x) = abbreviate(stringr::str_trunc(levels(x), 16), 12)
  }
  x
})
```

```{r, out.width="100%", fig.height=18}
DataExplorer::plot_bar(credit_short, nrow = 6, ncol = 3)
```

```{r, out.width="100%", fig.height=3}
DataExplorer::plot_histogram(credit, nrow = 1, ncol = 3)
```

```{r, out.width="100%", fig.height=3}
DataExplorer::plot_boxplot(credit, by = "credit_risk", nrow = 1, ncol = 3)
```

## Modeling
Considering how we are going to tackle the problem of classifying persons with respect to their credit risk relates closely to what mlr3 entities we will use.

- What kind of the problem are we trying to solve?
  - i.e., what `r ref("Task")` do we use?
  - Here, we are in a supervised setting and want to classify people by their credit risk (good or bad)
  - $\rightarrow$ We use `r ref("TaskClassif")`
- What are appropriate learning algorithms?
  - i.e., what `r ref("Learner")` do we use?
  - e.g., Logistic regression, CART, Random Forest
  - $\rightarrow$ `r ref("LearnerClassifLogReg")`, `r ref("LearnerClassifRpart")`, `r ref("LearnerClassifRanger")`
- How do we evaluate "good" performance? $\rightarrow$ Depends on many things! Cost of false positive vs. false negative, legal requirements, ...
  - i.e., what `r ref("Measure")` do we use?
  - Here, we start with misclassification error and will also false positive and false negative rate and AUC
  - $\rightarrow$ `msr("classif.ce)`, `msr("classif.fpr")`, `msr("classif.fnr")`, `msr("classif.auc")`

### Task Definition

For educational reasons, suppose mlr3 did not provide a predefined `Task` for the German credit dataset.
We could then create a new task from our data.

Using `TaskClassif$new()`, we can initialize a classification task.
We need to specify a name for the task, the data we want to use and the name of the target variable (here `"credit_risk"`):

```{r}
task = TaskClassif$new("GermanCredit", backend = credit, target = "credit_risk")
```

### Learner Definition

You need to install and load the `r mlr_pkg("mlr3learners")` package to get access to a wide range of learners.
All available `Learner`s can be inspected via the following dictionary:

```{r}
mlr_learners
```

A learner can be constructed with the `lrn` function and the name of the learner (e.g., `lrn("classif.xxx")`). Use `?mlr_learners_xxx` to open the help page of a learner named `xxx`.

### Model Fitting: Logistic Regression

- The `Learner` for logistic regression, `r ref("LearnerClassifLogReg")`, uses R's `glm()` function and is provided by the `mlr3learners` package.

```{r}
learner_logreg = lrn("classif.log_reg")
```

Training the learner is straightforward (here we use the whole task), with the resulting model being stored in the `$model` slot.

```{r}
learner_logreg$train(task)
```

We can then inspect the trained model (the output is too much to show here in the post - but you should definitely do it!)

```{r, eval=FALSE}
summary(learner_logreg$model)
```

### Model Fitting: Random Forest

Fitting a Ranger random forest, `r ref("LearnerClassifRanger")` is done analogously.
Here, we let the model store the permutation based variable importance (`importance = "permutation"`):

```{r}
learner_rf = lrn("classif.ranger", importance = "permutation")
learner_rf$train(task)
```

We can access the importance values using `$importance()`:

```{r, eval = FALSE}
learner_rf$importance()
```

We can convert the importance to a `data.table` so we can plot it

```{r}
importance = as.data.table(learner_rf$importance(), keep.rownames = TRUE)
colnames(importance) = c("Feature", "Importance")
importance
```

```{r}
ggplot(importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col() + coord_flip() + xlab("")
```

## Prediction

A model, once trained, can be used to predict outcomes from new data.
Here, we will simulate new data by sampling from the credit data.

```{r, R.options=list(width = 120)}
newdata = as.data.table(lapply(credit, FUN = sample, size = 5, replace = TRUE))
newdata$credit_risk = NULL
newdata
```

### Predict Classes

Let's see what the models predict:

```{r}
pred_logreg = learner_logreg$predict_newdata(newdata)
pred_rf = learner_rf$predict_newdata(newdata)

pred_logreg
pred_rf
```

The `predict()` function actually returns a `r ref("Prediction")` object. It can be converted to a `data.table`, as well as inspected for other things:

```{r}
pred_rf$confusion
```

The different predictions of the learners may disagree, but which do we trust most? We should do [Performance Evaluation](#performance-evaluation) and [benchmarks](#performance-comparison-and-benchmarks)!

### Predict Probabilities

Classification learners may not only predict a class variable (`"response"`), but also their degree of "belief" / uncertainty in a given response.
We achieve this by setting the `$predict_type` slot to `"prob"`.
Sometimes this needs to be done before the learner is trained.
Alternatively, we can directly create the learner with this option: `lrn("classif.log_reg", predict_type = "prob")`

```{r}
learner_logreg$predict_type = "prob"
```

```{r}
learner_logreg$predict_newdata(newdata)
```

Note that sometimes, you need to be careful when interpreting these values as probabilities!

# Performance Evaluation

To measure the performance of a learner on new unseen data, we usually mimic the scenario of unseen data by splitting up the data into training and test set.
The training set is used for training the learner, and the test set is only used for predicting and evaluating the performance of the trained learner.
Many resampling methods (cross-validation, bootstrap) repeat the splitting process in different ways.

We need to specify the resampling scheme using the `rsmp()` function:

```{r}
resampling = rsmp("holdout", ratio = 2 / 3)
print(resampling)
```

We use `resample()` to do the resampling calculation:

```{r}
res = resample(task, learner = learner_logreg, resampling = resampling)
res
```

The default score of the measure is included in the `$aggregate()` slot:

```{r}
res$aggregate()
```

We can easily do different resampling schemes, e.g. repeated holdout (`"subsampling"`), or cross validation.
Most methods do repeated train/predict cycles on different data subsets and aggregate the result (usually as the mean). Doing this manually would require us to write loops.

```{r}
res_sub = resample(task, learner = learner_logreg, resampling = rsmp("subsampling", repeats = 10))
res_sub$aggregate()
```

```{r}
res_cv = resample(task, learner = learner_logreg, resampling = rsmp("cv", folds = 10))
res_cv$aggregate()
```

We can also calculate scores for different measures (e.g., `msr("classif.fpr")` for the false positive rate).

```{r}
# false positive rate
res_cv$aggregate(msr("classif.fpr"))

# false positive rate and false negative
msr_list = list(
  msr("classif.fpr"),
  msr("classif.fnr")
)
res_cv$aggregate(msr_list)
```

There are a few more resampling methods, and quite a few more measures.
They can be inspected via the following dictionaries:

```{r}
mlr_resamplings
```

```{r}
mlr_measures
```

To get help on a resampling method, use `?mlr_resamplings_xxx`, for a measure do `?mlr_measures_xxx`.

Some measure, for example AUC, require a "probability" prediction, instead of a response prediction, see [Predict Probabilities](#predict-probabilities).

## Performance Comparison and Benchmarks

We could compare `Learner`s by evaluating `resample()` for each of them manually.
`benchmark()` automatically performs resampling evaluations for multiple learners and tasks.
To create fully crossed designs use `benchmark_grid()`: multiple `Tasks`s $\times$ multiple `Learner`s $\times$ multiple `Resampling`s.

```{r}
lrn_list = list(
  lrn("classif.log_reg", predict_type = "prob"),
  lrn("classif.ranger", predict_type = "prob")
)

bm_design = benchmark_grid(task, learners = lrn_list, resamplings = rsmp("cv", folds = 10))
bmr = benchmark(bm_design)
```

Careful, large benchmarks may take a long time! This one should take less than a minute, however.
In general, we want use parallelization to speed things up on multi-core machines.
For parallelization, mlr3 relies on the `r cran_pkg("future")` package:

```{r, eval=FALSE}
#future::plan("multiprocess") # uncomment for parallelization
```

We can compare different measures. Here, we compare the misclassification rate and AUC.

```{r}
msr_list = list(msr("classif.ce"), msr("classif.auc"))
performances = bmr$aggregate(msr_list)
performances[, c("learner_id", "classif.ce", "classif.auc")]
```

## Outlook

Things we have not done that should be considered:
- We have only worked with default hyperparameters, but we may want to see if tuning them helps (FIXME:)
- Some preprocessing and feature extraction steps may sometimes be helpful (FIXME:)

## Appendix

### R Pro Tips

- What are the arguments of `tsk()`, `lrn()`, `rsmp()`, `msr()` etc. again? $\rightarrow$ Think about the corresponding dictionary.

```{r}
mlr_learners

mlr_tasks

mlr_measures

mlr_resamplings
```

- What are the arguments of a `$new()` constructor?

```{r}
formals(TaskClassif$public_methods$initialize)
```

- What are the possible slots and functions of an object?
  - Writing `pred_rf$`, and pressing <TAB> should work.
    Otherwise:

```{r}
names(pred_rf)
```

- How do I see the help file of an object?
  - The documentation is organized by object classes

```{r}
class(pred_rf)
```

### mlr3 and its Extensions

| Package         | Functionality                                                                                |
| :-------------- | :--------------------------------------------------------------------------------------------|
| `mlr3`          | Framework for machine learning: `Task`, `Learner`, `Measure`, `resample()` and `benchmark()` |
| `mlr3learners`  | Concrete `Learner`s for many popular machine learning implementations                        |
| `mlr3pipelines` | Dataflow programming of machine learning workflows                                           |
| `mlr3tuning`    | Hyperparameter tuning for machine learning algorithms                                        |
| `mlr3filter`    | Feature filtering                                                                            |
| `mlr3viz`       | Visualisations and plots                                                                     |
| `paradox`       | Auxiliary package providing (hyper)parameter handling                                        |
| `mlr3misc`      | Auxiliary functions                                                                          |
