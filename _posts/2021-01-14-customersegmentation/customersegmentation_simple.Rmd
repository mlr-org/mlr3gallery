---
title: "customer_segmentation"
description: |
  Customer Segmentation based on RFM scores
author:
  - name: Giuseppe Casalicchio
  - name: Henri Funk
date: 01-14-2021
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
    toc: yes
    toc_depth: 4
---

```{r setup, include = FALSE}
library("mlr3book")
```


```{r, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
set.seed(1)
library(knitr)
library(ggplot2)
theme_set(theme_bw() + theme(legend.position = "bottom"))
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center", dev = "png", fig.retina = 1, R.options = list(width = 100))
gg_color_hue = function(n) {
  hues = seq(15, 375, length = n + 1)
  grDevices::hcl(h = hues, l = 65, c = 100)[1:n]
}
# load packages
library(gridExtra)
library(factoextra)
library(DataExplorer)
library(cluster)
library(dplyr)
#library(scales)
#library(R6)

library(mlr3verse)
```


# Steel Wheels Data

### Data Description

The provided data contains purchase information of a fictional store named Steel Wheels (available at https://www.kaggle.com/kyanyoga/sample-sales-data). 
Each line refers to a specific product that was sold to a customer in a specific order.
In our analysis, we will focus on the following columns:

- `ORDERNUMBER`: Unique ID for each order made by a customer
- `ORDERDATE`: Date of the order
- `PRODUCTLINE`: Type of product
- `QUANTITYORDERED`: Quantity of the product item included in the considered order specified by `ORDERNUMBER`
- `SALES`: Total price of the product item included in the considered order
- `CUSTOMERNAME`: Name of the customer of the considered order

```{r}
# import data
df = read.csv("archive/sales_data_sample.csv")

# select required columns
feats = c("ORDERNUMBER", "ORDERDATE", "PRODUCTLINE", "QUANTITYORDERED", "SALES", "CUSTOMERNAME")
df = df[, feats]

# encode date and category columns properly
df$ORDERDATE = as.Date(df$ORDERDATE, '%m/%d/%Y %H:%M')
df$ORDERNUMBER = as.factor(df$ORDERNUMBER)
df$PRODUCTLINE = as.factor(df$PRODUCTLINE)
df$CUSTOMERNAME = as.factor(df$CUSTOMERNAME)

# look at the data
head(df)
summary(df)
```

### Motivation: Customer Segmentation

Almost every company that sells products or services stores data of this form.
This type of data can be used to perform customer segmentation to plan efficient client-targeted marketing strategies.
To do so, the data is often aggregated on customer level to analyze the value of a customer.
The so-called RFM (Recency, Frequency, Monetary Value) approach offers a nice way to analyze the customer value on three dimensions:

- **R**ecency: How recently did the customer made an order?
- **F**requency: How often did the customer made an order (this value is often capped to avoid outliers)?
- **M**onetary Value: How much did the customer spend per order?

![](https://images.squarespace-cdn.com/content/v1/5ae8cb742714e518400c587e/1603372536920-HBSB3IU477Z38AUCPJBY/ke17ZwdGBToddI8pDm48kFWJ8WX-fAYJkDfe91aE9sV7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UVn9BDQI9BwT6RRRBkYuN-2TC5gedsUldwTK65d0sDSVZDqXZYzu2fuaodM4POSZ4w/rfm_segments.png)

### Aggregate Data at Customer Level

To perform customer segmentation, we need to aggregate the purchase data on customer level and calculate the aforementioned RFM scores for each customer:

```{r}
df_RFM = df %>% 
  group_by(CUSTOMERNAME) %>% 
  summarise(
    recency = as.numeric(max(ORDERDATE) - as.Date("2005-06-01")),
    frequency = min(c(n_distinct(ORDERNUMBER), 6)),
    monetary = sum(SALES)/n_distinct(ORDERNUMBER)
  )

# look at the aggregated data
head(df_RFM)
summary(df_RFM)

# visualize aggregated data
DataExplorer::plot_histogram(df_RFM, geom_histogram_args = list(bins = 10))
DataExplorer::plot_correlation(df_RFM)
```

### mlr3 Task

With the resulting dataframe, we first scale it and then create a `Clustering Task` for the scaled data which we later use for learning the cluster assignments. 
This is an mlr3 object that can be used to fit unsupervised learners on.

```{r}
df_scaled = as.data.frame(scale(df_RFM[,c("frequency", "monetary", "recency")]))
task_RFM = TaskClust$new(id = "customer_segment", df_scaled)
```

# Clustering with $k$-means

### The Elbow Method

- The idea behind the elbow method is to determine the value of $k$ where the within cluster variance begins to decrease most quickly.
- If $k$ increases, the within cluster variance will always decrease, as the points are getting closer to the centroids they are assigned to.
- However, at some point, the decrease in variability will usually slow down, which can be visualized by an elbow (a smaller angle). 
- The optimal number of clusters is chosen at exactly this point. Note, however, that the "elbow" is not always clearly identifiable.

We use the normalized data to perform $k$-means with different number of clusters $k$ and select the optimal number for $k$ according to the Elbow method:

```{r}
# specify a vector of values for k
k_clusters = 1:10
# loop for each value in k_clusters
km_wss = sapply(k_clusters, function(k) {
  lrn = lrn("clust.kmeans", centers = k, nstart = 100L)
  lrn$train(task_RFM)
  pred = lrn$predict(task_RFM)
  pred$score(msr("clust.wss"), task = task_RFM)
})
plot(x = k_clusters, y = km_wss, type = "b", main = "Elbow Plot")
```

### Cluster Insights

From the elbow method, we might chose $k=4$ as optimal number of clusters.
Hence, we perform $k$-means with $k=4$ and augment the original RFM data with a new column called `km_partition` containing the cluster assignments:

```{r, message=FALSE}
km_best = lrn("clust.kmeans", nstart = 100L,  centers = 4L)
km_best$train(task_RFM)
prediction = km_best$predict(task_RFM)
partition = as.factor(prediction$data$partition)
df_RFM$km_partition = partition
```

We can now use exploratory data analysis (EDA) to gain insights from the cluster assignment.

#### Conditional Plots

We start with looking at **conditional (or stratified) density plots** for the columns recency, frequency and monetary.
They illustrate how the **frequency distribution** of the clusters change with increasing values for the considered RFM scores.
As in this use case, the **frequency score** consists of only few distinct values, we will use a **stratified bar chart** to show the frequency distribution stratified by each cluster.

```{r, fig.height=4, fig.width=10, echo=FALSE}
# conditional density (and frequency) plots
p_rec = ggplot(df_RFM, aes(x = recency, fill = km_partition)) + 
  geom_density(position = "fill") + 
  ggtitle("Recency")
p_fre = ggplot(df_RFM, aes(x = as.factor(frequency), fill = km_partition)) + 
  geom_bar(position = "fill") +
  ggtitle("Frequency")
p_mon = ggplot(df_RFM, aes(x = monetary, fill = km_partition)) + 
  geom_density(position = "fill") + 
  ggtitle("Monetary Value")
gridExtra::grid.arrange(p_rec, p_fre, p_mon, ncol = 3)
```

From this, we can observe that clusters of customers refer to specific ranges of the scores, e.g., high recency scores belong to customers from **cluster 2**.

#### Scatter Plots

We can visualize the clusters in an interactive 3D scatter plot using different colors for the clusters:

```{r}
plotly::plot_ly(df_RFM, x = ~recency, y = ~monetary, z = ~frequency, color =  ~km_partition)
```

We can use the `fviz_cluster` function from the `factoextra` package, as it offers a unified interface also for other clustering algorithms.
By default, if more than two features are chosen, the function visualizes a scatter plot on the first two **principal components**:

```{r, fig.height=4, fig.width=10}
scatter_km1 = factoextra::fviz_cluster(km_best$model, df_RFM, 
  choose.vars = c("frequency", "monetary", "recency"))
# scatterplot for 'recency' vs. 'monetary' (on non-scaled data)
scatter_km2 = factoextra::fviz_cluster(km_best$model, df_RFM, 
  choose.vars = c("recency", "monetary"), stand = FALSE)
gridExtra::grid.arrange(scatter_km1, scatter_km2, nrow = 1)
```

#### Pairs Plot

A pairs plot gives even more insights at one glance if we use the cluster belongings as color aesthetics. 
It shows

- size of clusters in bar charts (first row, first column)
- box plots of the RFM scores, stratified by each cluster (first row, after first column)
- densities of the RFM scores, stratified by each cluster (diagonal)
- histograms of the RFM scores, stratified by each cluster (first column, after first row)
- pairwise scatter plots of the RFM scores with different colors for their cluster belongings (lower diagonal)
- pairwise correlations of the RFM scores, stratified by each cluster (upper diagonal)

```{r,fig.height=7, fig.width=7}
# pairs plot from the GGally package
GGally::ggpairs(df_RFM, mapping = aes(col = km_partition, alpha = 0.9), 
  columns = c("km_partition", "frequency", "monetary", "recency"))
```

#### Summarize Cluster Centers

Now, we look at the cluster centers of the original data to derive a description for the customer segments

```{r}
df_RFM %>% 
  group_by(km_partition) %>% # define column used for grouping
  summarize_if(is.numeric, mean) # aggregate all numeric columns by the mean
```

### Conclusion

After this analysis, we might be able to describe the clusters (e.g., customer segments) as follows:

- **Cluster 1**: Customers with highest recency score (last purchase long time ago), lowest frequency, and medium monetary value.\
$\Rightarrow$ Customers **at risk** or **lost** due to inactivity but maybe salvageable by offering big discounts or sending reminders to receive again their attention.

- **Cluster 2**: Customers with moderate recency and frequency and lowest monetary value.\
$\Rightarrow$ Customers show medium activity but do not spend much, e.g., marketing strategy should motivate these customers to spend more money, e.g., offer quantity discounts or time limited discounts.

- **Cluster 3**: Customers with moderate recency and frequency but highest monetary value.\
$\Rightarrow$ These are **champions** or **promising** customers that we must keep, e.g., we could directly contact these customers to ask if they are happy with our service.

- **Cluster 4**: Customers with lowest recency, highest frequency and medium monetary value.\
$\Rightarrow$ These are rather **new** customers that are pretty active. Let us keep an eye on these customers and make sure that they remain active.

# Hierarchical Clustering

```{r, echo = FALSE}
theme_set(theme_bw())
```

Another type of exploration is to try out different clustering algorithms (e.g., hierarchical clustering with different linkage methods) for customer segmentation.
A good customer segmentation process usually includes exploring different clustering strategies to arrive at optimal customer segments that provide valuable insights.

### Hierarchical Clustering with `hclust`

To compute a dendogram and visualize the cluster assignments, we need to do the following steps:

```{r mlr3implementationII}
hclust = lrn("clust.hclust", method = "complete", distmethod = "euclidean", k = 4L)
hclust$train(task_RFM)
factoextra::fviz_dend(hclust$model, k = hclust$param_set$values$k)
```

<!-- ### Compare Hierarchical Clustering and $k$-means -->

<!-- If we want to compare the performance of different cluster algorithms, we can look at the **total within sum of square** measure for a specific value of $k$. -->
<!-- To do so, we store all information from the elbow plot for optimal number of clusters of different linkage methods: -->

<!-- ```{r fun, warning=FALSE} -->
<!-- ellbow_data = function(method = "average", task = task_RFM, k_clusters = 1:10) { -->
<!--   sapply(k_clusters, function(k) { -->
<!--     lrn = lrn("clust.hclust", method = method, k = k) -->
<!--     lrn$train(task) -->
<!--     pred = lrn$predict(task) -->
<!--     pred$score(msr("clust.wss"), task = task) -->
<!--   }) -->
<!-- } -->

<!-- compare = rbind( -->
<!-- cbind.data.frame("y" = as.numeric(km_wss),"clusters" =  1:10, "method"= "kmeans"), -->
<!-- cbind.data.frame("y" = as.numeric(ellbow_data()), "clusters" = 1:10, "method"= "hclust average"), -->
<!-- cbind.data.frame("y" = as.numeric(ellbow_data("centroid")), "clusters" = 1:10, "method"= "hclust centroid"), -->
<!-- cbind.data.frame("y" = as.numeric(ellbow_data("single")), "clusters" = 1:10, "method"= "hclust single") -->
<!-- ) -->
<!-- ``` -->

<!-- To compare $k$-means and different hierarchical clustering algorithms, we combine all relevant elbow plot information in one `data.frame` and visualize the results: -->

<!-- ```{r} -->
<!-- ggplot(compare, aes(x = clusters, y = as.integer(y), col = method)) +  -->
<!--   geom_point() + -->
<!--   geom_line(aes(group = method)) +  -->
<!--   ylab("Total Within Sum of Square") -->
<!-- ``` -->

<!-- Obviously, $k$-means outperforms all other hierarchical clustering algorithms as it directly aims at minimizing the within sum of square measure `wss`. -->
<!-- Many other metrics and indices to evaluate the clusters exist. -->

# Next Steps

For an improved discovery and insights, we can further improve the cluster analysis by adding other customer-related features or other relevant order details, e.g.:

- Generate other features from the original data like the duration of the **customer relationship**, which can be obtained from the date of the first order of the customer.
- Aggregate the monetary value for specific type of products separately (information available in `PRODUCTLINE`) to target customers according to product interests and develop a product-specific marketing strategy.
