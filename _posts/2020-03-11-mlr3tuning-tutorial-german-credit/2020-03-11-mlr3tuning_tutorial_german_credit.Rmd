---
title: mlr3tuning Tutorial - German Credit
author:
  - name: Martin Binder
  - name: Florian Pfisterer
date: 03-11-2020
description: |
   In this use case, we continue working with the German credit dataset. We
   work on hyperparameter tuning and apply nested resampling.
categories:
  - mlr3tuning
  - tuning
  - german credit
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---

```{r, include=FALSE}
# Just some preparation
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80)
)
library(mlr3book)
if (require("data.table")) data.table::setDTthreads(1)
set.seed(8008135)
lgr::get_logger("mlr3")$set_threshold("warn")
# SET BBOTK TO 'info' IF YOU PLAY AROUND WITH THIS!
lgr::get_logger("bbotk")$set_threshold("warn")
```

## Intro

This is the second part of a serial of tutorials.
The other parts of this series can be found here:

- [Part I - Basics](https://mlr3gallery.mlr-org.com/posts/2020-03-11-basics-german-credit/)
- [Part III - Pipelines](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3pipelines-tutorial-german-credit/)

We will continue working with the German credit dataset. In Part I, we peeked into the dataset by using and comparing some learners with their default parameters. We will now see how to:

- Tune hyperparameters for a given problem
- Perform nested resampling

## Prerequisites

First, load the packages we are going to use:

```{r, message=FALSE, warning=FALSE}
library("data.table")
library("ggplot2")
library("mlr3")
library("mlr3learners")
library("mlr3tuning")
library("paradox")
```

```{r, include=FALSE}
theme_set(theme_light())
```

We use the same `Task` as in Part I:

```{r, message=FALSE}
task = tsk("german_credit")
```

We also might want to use multiple cores to reduce long run times of tuning runs.

```{r, warning=FALSE}
# future::plan("multiprocess") # uncomment for parallelization
```

### Evaluation

We will evaluate all  hyperparameter configurations using 10-fold CV.
We use a *fixed* train-test split, i.e. the same splits for each evaluation.
Otherwise, some evaluation could get unusually "hard" splits, which would make comparisons unfair.

```{r}
set.seed(8008135)
cv10_instance = rsmp("cv", folds = 10)

# fix the train-test splits using the $instantiate() method
cv10_instance$instantiate(task)

# have a look at the test set instances per fold
cv10_instance$instance
```

## Simple Parameter Tuning

Parameter tuning in `mlr3` needs two packages:

1) The `r mlr_pkg("paradox")` package is used for the search space definition of the hyperparameters
2) The `r mlr_pkg("mlr3tuning")` package is used for tuning the hyperparameters

### Search Space and Problem Definition

First, we need to decide what `r ref("Learner")` we want to optimize.
We will use `r ref("LearnerClassifKKNN")`, the "kernelized" k-nearest neighbor classifier.
We will use `kknn` as a normal kNN without weighting first (i.e., using the rectangular kernel):

```{r}
knn = lrn("classif.kknn", predict_type = "prob")
knn$param_set$values$kernel = "rectangular"
```

As a next step, we decide what parameters we optimize over.
Before that, though, we are interested in the parameter set on which we could tune:

```{r}
knn$param_set
```

We first tune the `k` parameter (i.e. the number of nearest neighbors), between 3 to 20.
Second, we tune the `distance` function, allowing L1 and L2 distances.
To do so, we use the `paradox` package to define a search space (see the [online vignette](https://mlr3book.mlr-org.com/paradox.html) for a more complete introduction.

```{r}
search_space = ParamSet$new(list(
  ParamInt$new("k", lower = 3, upper = 20),
  ParamInt$new("distance", lower = 1, upper = 2)
))
```

As a next step, we define a `r ref("TuningInstanceSingleCrit")` that represents the problem we are trying to optimize.

```{r}
instance_grid = TuningInstanceSingleCrit$new(
  task = task,
  learner = knn,
  resampling = cv10_instance,
  measure = msr("classif.ce"),
  search_space = search_space,
  terminator = trm("none")
)
```

### Grid Search

After having set up a tuning instance, we can start tuning.
Before that, we need a tuning strategy, though.
A simple tuning method is to try all possible combinations of parameters: **Grid Search**.
While it is very intuitive and simple, it is inefficient if the search space is large.
For this simple use case, it suffices, though.
We get the `r ref("TunerGridSearch", "grid_search tuner")` via:

```{r}
set.seed(1)
tuner_grid = tnr("grid_search", resolution = 18, batch_size = 36)
```


Tuning works by calling `$optimize()`.
Note that the tuning procedure *modifies* our tuning instance (as usual for `R6` class objects).
The result can be found in the `instance` object.
Before tuning it is empty:

```{r}
instance_grid$result
```

Now, we tune:

```{r}
tuner_grid$optimize(instance_grid)
```

The result is returned by `$optimize()` together with its performance. It can be also accessed with the  `$result` slot:

```{r}
instance_grid$result
```

We can also look at the `Archive` of evaluated configurations:

```{r}
instance_grid$archive$data()
```

We plot the performances depending on the sampled `k` and `distance`:

```{r}
ggplot(instance_grid$archive$data(), aes(x = k, y = classif.ce, color = as.factor(distance))) +
  geom_line() + geom_point(size = 3)
```

On average, the Euclidean distance (`distance` = 2) seems to work better.
However, there is much randomness introduced by the resampling instance.
So you, the reader, may see a different result, when you run the experiment yourself and set a different random seed.
For `k`, we find that values between 7 and 13 perform well.

### Random Search and Transformation

Let's have a look at a larger search space.
For example, we could tune *all* available parameters and limit `k` to large values (50).
We also now tune the distance param continuously from 1 to 3 as a double and tune
distance kernel and whether we scale the features.

We may find two problems when doing so:

First, the resulting difference in performance between `k` = 3 and `k` = 4 is probably larger than the difference between `k` = 49 and `k` = 50.
While 4 is 33% larger than 3, 50 is only 2 percent larger than 49.
To account for this we will use a **transformation function** for `k` and optimize in log-space.
We define the range for `k` from `log(3)` to `log(50)` and exponentiate in the transformation.
Now, as `k` has become a double instead of an int (in the search space, before transformation),
we a√∂sp round it in the trafo.

```{r}
large_searchspace = ParamSet$new(list(
  ParamDbl$new("k", lower = log(3), upper = log(50)),
  ParamDbl$new("distance", lower = 1, upper = 3),
  ParamFct$new("kernel", c("rectangular", "gaussian", "rank", "optimal")),
  ParamLgl$new("scale")
))

large_searchspace$trafo = function(x, param_set) {
  x$k = round(exp(x$k))
  x
}
```

The second problem is that grid search may (and often will) take a long time.
For instance, trying out three different values for `k`, `distance`, `kernel`, and the two values for `scale` will take 54 evaluations.
Because of this, we use a different search algorithm, namely the **Random Search**.
We need to specify in the *tuning instance* a *termination criterion*.
The criterion tells the search algorithm when to stop.
Here, we will terminate after 36 evaluations:

```{r}
tuner_random = tnr("random_search", batch_size = 36)

instance_random = TuningInstanceSingleCrit$new(
  task = task,
  learner = knn,
  resampling = cv10_instance,
  measure = msr("classif.ce"),
  search_space = large_searchspace,
  terminator = trm("evals", n_evals = 36)
)
```

```{r}
tuner_random$optimize(instance_random)
```

Like before, we can review the `Archive`.
It includes the points before and after the transformation.
The archive includes a column for each parameter the `Tuner` sampled on the search space (points before the transformation):

```{r}
instance_random$archive$data()
```

The parameters used by the learner (points after the transformation) are stored in in the `x_domain` column as lists.
By using `unnest = x_domain`, the list elements are expanded to separate columns:

```{r}
instance_random$archive$data(unnest = "x_domain")
```

Let's now investigate the performance by parameters.
This is especially easy using visualization:

```{r}
ggplot(instance_random$archive$data(unnest = "x_domain"),
  aes(x = x_domain_k, y = classif.ce, color = x_domain_scale)) +
  geom_point(size = 3) + geom_line()
```

The previous plot suggests that `scale` has a strong influence on performance.
For the kernel, there does not seem to be a strong influence:

```{r}
ggplot(instance_random$archive$data(unnest = "x_domain"),
  aes(x = x_domain_k, y = classif.ce, color = x_domain_kernel)) +
  geom_point(size = 3) + geom_line()
```

## Nested Resampling

Having determined tuned configurations that seem to work well, we want to find out which performance we can expect from them.
However, this may require more than this naive approach:

```{r}
instance_random$result_y
instance_grid$result_y
```

The problem associated with evaluating tuned models is *overtuning*.
The more we search, the more optimistically biased the associated performance metrics from tuning become.

There is a solution to this problem, namely **Nested Resampling**.

The `mlr3tuning` package provides an `r ref("AutoTuner")` that acts like our tuning method but is actually a `Learner`.
The `$train()` method facilitates tuning of hyperparameters on the training data, using a resampling strategy (below we use 5-fold cross-validation).
Then, we actually train a model with optimal hyperparameters on the whole training data.

The `AutoTuner` finds the best parameters and uses them for training:

```{r, eval=FALSE}
grid_auto = AutoTuner$new(
  learner = knn,
  resampling = rsmp("cv", folds = 5), # we can NOT use fixed resampling here
  measure = msr("classif.ce"),
  search_space = search_space,
  terminator = trm("none"),
  tuner = tnr("grid_search", resolution = 18),
)
```

The `AutoTuner` behaves just like a regular `Learner`.
It can be used to combine the steps of hyperparameter tuning and model fitting but is especially useful for resampling and *fair* comparison of performance through benchmarking:

```{r, eval=FALSE}
rr = resample(task, grid_auto, cv10_instance, store_models = TRUE)
```

We aggregate the performances of all resampling iterations:

```{r, echo=FALSE}
rr = readRDS("resample_result.rds")
```

```{r}
rr$aggregate()
```

Essentially, this is the performance of a "knn with optimal hyperparameters found by grid search". Note that `grid_auto` is not changed since `resample()` creates a clone for each
resampling iteration. The trained `AutoTuner` objects can be accessed by using

```{r}
rr$learners[[1]]
rr$learners[[1]]$tuning_result
```


## Appendix

### Example: Tuning With A Larger Budget

It is always interesting to look at what could have been. The following dataset contains an optimization run result with 3600 evaluations -- more than above by a factor of 100:

```{r, echo=FALSE, eval=FALSE}
set.seed(2409)
instance_random = TuningInstanceSingleCrit$new(
  task = task,
  learner = knn,
  resampling = cv10_instance,
  measure = msr("classif.ce"),
  search_space = large_searchspace,
  terminator = trm("evals", n_evals = 3600)
)
tuner_random$optimize(instance_random)
instance_random$result
```

```{r, echo=FALSE}
perfdata = readRDS("randomsearch_3600.rds")
```

```{r}
perfdata
```

The scale effect is just as visible as before with fewer data:

```{r}
ggplot(perfdata, aes(x = x_domain_k, y = classif.ce, color = scale)) +
  geom_point(size = 2, alpha = 0.3)
```

Now, there seems to be a visible pattern by kernel as well:

```{r}
ggplot(perfdata, aes(x = x_domain_k, y = classif.ce, color = kernel)) +
  geom_point(size = 2, alpha = 0.3)
```

In fact, if we zoom in to `(5, 35)` $\times$ `(0.23, 0.28)` and do decrease smoothing we see that different kernels have their optimum at different values of `k`:

```{r, warning=FALSE}
ggplot(perfdata, aes(x = x_domain_k, y = classif.ce, color = kernel,
  group = interaction(kernel, scale))) +
  geom_point(size = 2, alpha = 0.3) + geom_smooth() +
  xlim(5, 35) + ylim(0.23, 0.28)
```

What about the `distance` parameter?
If we select all results with `k` between 10 and 20 and plot distance and kernel we see an approximate relationship:

```{r, warning=FALSE}
ggplot(perfdata[x_domain_k > 10 & x_domain_k < 20 & scale == TRUE],
  aes(x = distance, y = classif.ce, color = kernel)) +
  geom_point(size = 2) + geom_smooth()
```

In sum our observations are:
The `scale` parameter is very influential, and scaling is beneficial.
The `distance` type seems to be the least influential.
Their seems to be an interaction between 'k' and 'kernel'.

