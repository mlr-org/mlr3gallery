---
title: mlr3tuning tutorial - german credit
author: 
  - name: Martin Binder
  - name: Florian Pfisterer
date: 03-11-2020
description: |
  We evaluate all algorithms using 10-fold cross-validation. We use a fixed train-test split, i.e. the same splits for each evaluation. Otherwise, some evaluation could get unusually "hard" splits, which would make comparisons unfair.
categories:
  - 'tuning'
  - 'imputation'
  - 'filter'
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---

```{r, include = FALSE}
## Just some preparation
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80)
)
data.table::setDTthreads(1)
set.seed(8008135)
lgr::get_logger("mlr3")$set_threshold("warn")
```

## Intro

This is the second part of a serial of tutorials.
The other parts of this series can be found here:

* [Part I - Basics](/basics_german_credit/)
* [Part III - Pipelines](/basics_pipelines_german_credit/)

In this case, we will continue working with the **German Credit Dataset**. Yesterday we peeked into the data set by using and comparing some learners with their default parameters. We will now see how to:

- Tune hyperparameters for a given problem
- Perform nested resampling
- Adjust decision thresholds

## Prerequisites

We expect you have installed all packages from day 1. 
If not, load the day 1 (basics) script and run the **Prerequisites** install chunk.

Load the packages we are going to use:

```{r, message=FALSE, warning=FALSE}
library("data.table")
library("mlr3")
library("mlr3learners")
library("mlr3tuning")
library("ggplot2")
```

```{r, include = FALSE}
theme_set(theme_light())
```

We use the same data as in the earlier blog posts.

```{r, message = FALSE}
task = tsk("german_credit")
```

Also, because tuning often takes a long time, we want to make more efficient use of our multicore CPUs. 
Don't do this on rstudio cloud, however.

```{r, warning = FALSE}
# future::plan("multiprocess")
```

### Evaluation

We evaluate all algorithms using 10-fold cross-validation. 
We use a *fixed* train-test split, i.e. the same splits for each evaluation. 
Otherwise, some evaluation could get unusually "hard" splits, which would make comparisons unfair.

```{r}
set.seed(8008135)
cv10_instance = rsmp("cv", folds = 10)
## fix the train-test splits using the $instantiate() method
cv10_instance$instantiate(task)
## have a look at the test set instances per fold
cv10_instance$instance
```

## Simple Parameter Tuning

Parameter tuning in `mlr3` needs two packages:
The `paradox` package is used for the search space definition of the hyperparameters.
The `mlr3tuning` package is used for tuning the hyperparameters.

```{r}
library("mlr3tuning")
library("paradox")
```

### Search Space and Problem Definition

First, we need to decide what `Learner` we want to optimize. 
We will use `"classif.kknn"`, the "kernelized" k-nearest neighbor classifier.
We will use `kknn` as a normal kNN without weighting first (i.e., use the `rectangular` kernel):

```{r}
knn = lrn("classif.kknn", predict_type = "prob")
## use rectangular kernel for normal kNN
knn$param_set$values$kernel = "rectangular"
```

As a next step, we decide what parameters we optimize over. 
Before that, though, we are interested in the parameter set on which we could tune.

```{r}
knn$param_set
```

We first tune the `k` parameter (i.e. the number of neighbors to be accounted for).
A range from 3 to 20 seems reasonable for this.
Second, we aim to tune the `distance` function.
We aim to keep it simple and only tune over the L1 and L2 distance.
To do so, we use the `paradox` package to define a search space (see [Appendix](#very-quick-paradox-primer) for a short list of possible parameter types).

```{r}
searchspace = ParamSet$new(list(
  ParamInt$new("k", lower = 3, upper = 20),
  ParamInt$new("distance", lower = 1, upper = 2)
))
```

As a next step, we define a "tuning instance" that represents the problem we are trying to optimize.

Typically a tuning instance needs to be supplied with:
  - The task we are optimizing for.
  - The learner are we using.
  - The resampling we use.
  - The performance measure to be optimized.
  - The search space ("parameter set").
  - The end of tuning ("terminator") (ignore this for now).

```{r}
instance_grid = TuningInstance$new(
  task = task,
  learner = knn,
  resampling = cv10_instance,
  measures = msr("classif.ce"),
  param_set = searchspace,
  terminator = term("none")
)
```

### Grid Search

After having set up a tuning instance, we can start tuning.
Before that, we need a tuning strategy, though.
A simple tuning method is to try all possible combinations of parameters: **Grid Search**
While it is very intuitive and simple, it is inefficient if the search space is large.
For this simple use case, it suffices, though.
We get the `"grid_search"` tuner by:

```{r}
set.seed(1)
tuner_grid = tnr("grid_search", resolution = 18, batch_size = 36)
```

Tuning works by calling `$tune()`. 
Note that the tuning procedure *modifies* our "tuning instance" (As usual for `R6` class objects). 
The result can be found in the `instance` object.
Before tuning it is empty:

```{r}
## empty instance object:
instance_grid$result
## now run tuning:
tuner_grid$tune(instance_grid)
```

The result of tuning can now be found in the `$result` slot -- together with its performance.

```{r}
instance_grid$result
```

We can also look at the "archive" of evaluated configurations.
We do so by expanding the "params" (the parameters that the `Learner` actually saw).

```{r}
perfdata = instance_grid$archive("params")
perfdata[, c("nr", "k", "distance", "classif.ce")]
```

```{r}
ggplot(perfdata, aes(x = k, y = classif.ce, color = as.factor(distance))) +
  geom_line() + geom_point(size = 3)
```

On average, the Euclidean distance (`distance` = 2) seems to work better.
However, there is much randomness introduced by the resampling instance.
So you, the reader, may see a different result, when you run the experiment yourself.
For `k`, we find that values between 10 and 15 perform well.

### Random Search and Transformation

Let's have a look at a larger search space.
For example, we could tune *all* available parameters and limit `k` to large values (50).

We may find 2 problems when doing so:

First, the resulting difference in performance between `k` = 3 and `k` = 4 is probably larger than the difference between `k` = 49 and `k` = 50.
While 4 is 33% larger than 3, 50 is only 2 percent larger than 49.
To account for this we will use a **transformation function** for `k` and sample values on the log-space. 
We define the range for `k` from `log(3)` to `log(50)` and exponentiate in the transformation. 
Now, `k` is not necessarily an integer anymore.
So, we must use `ParamDbl` instead of `ParamInt`.

```{r}
large_searchspace = ParamSet$new(list(
  ParamDbl$new("k", lower = log(3), upper = log(50)),
  ParamDbl$new("distance", lower = 1, upper = 3),
  ParamFct$new("kernel", c("rectangular", "gaussian", "rank", "optimal")),
  ParamLgl$new("scale")
))

large_searchspace$trafo = function(x, param_set) {
  x$k = round(exp(x$k))
  x
}
```

The second problem is that grid search may (and often will) take a long time.
For instance, trying out three different values for `k`, `distance`, `kernel`, and the two values for `scale` will take 54 evaluations.
Because of this, we use a different search algorithm, namely the **Random Search**.
We need to specify in the *tuning instance* a *termination criterion*.
The criterion tells the search algorithm when to stop.

```{r}
tuner_random = tnr("random_search", batch_size = 36)

instance_random = TuningInstance$new(
  task = task,
  learner = knn,
  resampling = cv10_instance,
  measures = msr("classif.ce"),
  param_set = large_searchspace,
  terminator = term("evals", n_evals = 36)
)
```

```{r}
tuner_random$tune(instance_random)
instance_random$result
```

Like before, we can review the "archive".
We can do so in two ways-
First, we can expand the `"tune_x"` parameters (the points we sampled on the search space, before transformation):

```{r}
perfdata = instance_random$archive("tune_x")
perfdata[, c("k", "distance", "kernel", "scale", "classif.ce")]
```

Second, we can look at the `"params"` parameters (the points the `Learner` was used with---these are the `exp()`'d parameters, after transformation of the sampled points):

```{r}
perfdata = instance_random$archive("params")
perfdata[, c("k", "distance", "kernel", "scale", "classif.ce")]
```

Let's now look to investigate the performance by parameters.
This is especially easy using visualization.

```{r}
ggplot(perfdata, aes(x = k, y = classif.ce, color = scale)) +
  geom_point(size = 3) + geom_line()
```

The previous plot suggests that `scale` has a strong influence on performance.
For the kernel, there does not seem to be a strong influence:

```{r}
ggplot(perfdata, aes(x = k, y = classif.ce, color = kernel)) +
  geom_point(size = 3) + geom_line()
```

## Nested Resampling

Having determined tuned configurations that seem to work well, we want to find out which performance we can expect from them.
However, this may require more than this naive approach:

```{r}
instance_random$result$perf
instance_grid$result$perf
```

The problem associated with evaluating tuned models is *overtuning*.
The more we search, the more our result is likely to just be "lucky" on your training data.
An extreme example would be predicting random values and “tuning” the seed value:
If we try enough seeds we will get good (tuning set) performance!
Also, different search spaces or search methods may introduce different amounts of randomness, so even the comparison is flawed.

There is a solution to this problem, namely Nested Resampling.

The `mlr3tuning` package provides an `AutoTuner` that acts like our tuning method is actually a `Learner`.
In this fashion the `AutoTuner` features:

- a `$train()` method:
- a `$predict()` method: use model trained on the whole training data as model.

The `$train()` method facilitates tuning of hyperparameters on the training data, using a resampling strategy (below we use 5-fold CV).
Then, we actually train a model with optimal hyperparameters on whole training data.

The `AutoTuner` finds the best parameters and uses them for training.

```{r, eval = FALSE}
grid_auto = AutoTuner$new(
  learner = knn,
  resampling = rsmp("cv", folds = 5), # we can NOT use fixed resampling here
  measures = msr("classif.ce"),
  tune_ps = searchspace,
  terminator = term("none"),
  tuner = tnr("grid_search", resolution = 18)
)
```

The autotuner behaves just like a `Learner`. 
It can be used to combine the steps of hyperparameter tuning and model fitting but is especially useful for resampling and _fair_ comparison of performance through benchmarking.

```{r, eval = FALSE}
resample(task, grid_auto, cv10_instance)$aggregate()
```

Essentially, this is the performance of a "knn with optimal hyperparameters found by grid search"

## Appendix

### Example: Tuning With Larger Budget

It is always interesting to look at what could have been. The following dataset contains an optimization run result with 3600 evaluations--more than above by a factor of 100.

```{r, echo = FALSE}
perfdata = readRDS(gzcon(url("https://github.com/mlr-org/mlr-outreach/raw/master/2019_madrid/mlr3tuning/randomsearch_3600.rds")))
```

```{r}
perfdata # obtained from mlr-outreach github repository
```

The scale effect is just as visible as before with fewer data.

```{r}
ggplot(perfdata, aes(x = k, y = classif.ce, color = scale)) +
  geom_point(size = 2, alpha = 0.3)
```

Now, there seems to be a visible pattern by kernel as well:

```{r}
ggplot(perfdata, aes(x = k, y = classif.ce, color = kernel)) +
  geom_point(size = 2, alpha = 0.3)
```

In fact, if we zoom in to `(5, 30)` x `(0.2, 0.3)` and do decrease smoothing we see that different kernels have their optimum at different `k`.

```{r, warning = FALSE}
ggplot(perfdata, aes(x = k, y = classif.ce, color = kernel,
  group = interaction(kernel, scale))) +
  geom_point(size = 2, alpha = 0.3) + geom_smooth() +
  xlim(5, 30) + ylim(0.2, 0.3)
```

What about the `distance` parameter? 
If we select all results with `k` between 10 and 20 and plot distance and kernel we see an approximate relationship:

```{r, warning = FALSE}
ggplot(perfdata[k > 10 & k < 20 & scale == TRUE],
  aes(x = distance, y = classif.ce, color = kernel)) +
  geom_point(size = 2) + geom_smooth()
```

In sum our observations are:
The `scale` parameter is very influential.
The `distance` type seems to be the least influential.
If we had performed a grid search, we would have wasted a lot of evaluations on trying different `distance` values that usually give similar results. 
This is why random search tends to work well.
One could also observe that `scale = FALSE` performs badly.
Then, we should not try out so many points with this option.

### Very quick `paradox` primer

We use `paradox` in this use case.
It may be helpful to give you a very short introduction to it.

Initialization:

```{r, eval = FALSE}
ParamSet$new(list( <PARAMETERS> ))
```

Possible parameter types:

```{r, eval = FALSE}
## - logical (values TRUE, FALSE)
ParamLgl$new("parameter_id")
## - factorial (discrete values from a list of 'levels')
ParamFct$new("parameter_id", c("value1", "value2", "value3"))
## - integer (from 'lower' to 'upper' bound)
ParamInt$new("parameter_id", lower = 0, upper = 10)
## - numeric (from 'lower' to 'upper' bound)
## - unfortunately named after the storage type, "double precision floating point"
ParamDbl$new("parameter_id", lower = 0, upper = 10)

## Also possible: "untyped", but we can not tune with this!
ParamUty$new("parameter_id")
```

So an example parameter set with one logical parameter `"flag"` and one integer parameter `"count"`:

```{r}
ParamSet$new(list(
  ParamLgl$new("flag"),
  ParamInt$new("count", lower = 0, upper = 10)
))
```

See the [online vignette](https://mlr3book.mlr-org.com/paradox.html) of `paradox` for a more complete introduction.
