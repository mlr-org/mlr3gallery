---
title: "Boston Housing Regression Task on mlr3keras"
slug: mlr3-keras-basics
categories:
  - mlr3
  - mlr3keras
  - deeplearning
  - regression
  - basics
  - reproducability
  - callbacks
description: |
  This use case shows how to use mlr3 keras on a simple Regression Task.
author:
  - name: Florian Pfisterer
  - name: Henri Funk
date: 05-05-2020
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.csss
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80),
  cache.extra = knitr::rand_seed
)
library(mlr3book)
```

# Intro

This use case shows how to use `gh_pkg("mlr3keras")` on simple
[Boston Housing Regression Task](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html).
Therefore, the Code from
[Keras Basic Regression Tutorial](https://keras.rstudio.com/articles/tutorial_basic_regression.html)
is translated to `gh_pkg("mlr-org/mlr3keras")` respectively `gh_pkg("mlr-org/mlr3keras")` syntax.
Moreover, this use case extends the tutorial mentioned above by
 
 * giving an introduction to callbacks and how they can be used
 * applying reproducability to `gh_pkg("mlr-org/mlr3keras")` 

As `gh_pkg("mlr-org/mlr3keras")` is still under heavy development, this place might be 
a good way to look for currently working aspects of the package.

## Prerequisites

This tutorial assumes familiarity with the basics of
`r mlr_pkg("mlr3pipelines")`.
Consult the [mlr3book](https://mlr3book.mlr-org.com/pipelines.html) if some 
aspects are not  fully understandable.

```{r lib}
library(mlr3)
library(mlr3keras)
library(mlr3pipelines)
library(keras)
library(reticulate)
library(ggplot2)
```

# Basic Regression Task - Boston Housing Prices

In this use case we build a model that predicts the Median value of
owner-occupied homes in Boston in $1000's (`medv`).

The `r ref("mlr_tasks_boston_housing", "Boston Housing Task")` is accessible
directly from `mlr3tasks`.

```{r train}
boston_task = tsk("boston_housing")
```

This data set is much smaller than the others we’ve worked with so far: it has 506 total examples that are split between 404 training examples and 102 test examples:

```{r paste}
paste(
  "This data set contains", boston_task$nrow, "observations and", 
  boston_task$ncol - 1, "features."
  )
```

## Feature slection

Keras regression learner only supports numeric features.
Consequentely we drop all non-numeric features from `boston_task`.

```{r featureselection}
boston_task$select(subset(boston_task$feature_types, type == "numeric")$id)
boston_task$head(n = 3)
```


Now the dataset contains 13 different numeric features:

* `crim` - per capita crime rate by town
* `zn` - proportion of residential land zoned for lots over 25,000 sq.ft.
* `indus` - proportion of non-retail business acres per town.
* `nox` - nitric oxides concentration (parts per 10 million)
* `rm` - average number of rooms per dwelling
* `age` - proportion of owner-occupied units built prior to 1940
* `dis` - weighted distances to five Boston employment centres
* `ptratio` - pupil-teacher ratio by town
* `b` - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
* `lstat` - % lower status of the population
* `cmedv` - a numeric vector of corrected median values of owner-occupied 
housing in USD 1000
* `lon` - a numeric vector of tract point longitudes in decimal degrees
* `lat` - a numeric vector of tract point latitudes in decimal degrees

Notice that each one of these input data features is stored using a different scale.
In this case it’s recommended to normalize features.
Although the model might converge without feature normalization, it makes
training more difficult, and it makes the resulting model more dependant on the
choice of units used in the input.
We create a `r ref("mlr_pipeops_scale", "PipeOpScale")` that is going to be
connected to ther learner later on when it comes to training.

```{r normalize}
po_scale = PipeOpScale$new()
```

# Create the model

For this use case we create a sequential modell, a linear stack of layers.
In the input layer the shape of our input data is defined.
Input is analized by two densely connected hidden layers.
A final denselayer returns a single continous estimation value for the
observations target.

As creation of models is very convenient and intuitive in
[keras](https://keras.rstudio.com/), `gh_pkg("mlr-org/mlr3keras")` relies on
this syntax for model creation.
Wrapping our model in a function `build_modell` has several advantages:

* Each new learner needs a seperately build modell, if you aim to train a whole
new modell!
Cloning the learner is not sufficient in this case and will lead to continue 
training ione and the same model in a new learner.
You might notice a flat history curve in such cases.
* In case of more complex models, a function makes a model a tunable
hyperparameter.
* You can easily build models with reproducable and comparable results by
assigning a seed to the `build_model`. Note, that seed setting is not trivial in
this case. If you are intrested in more details, see chapter \@ref(reproducability)[Reproducability].

```{r seed, echo=FALSE}
reset_random_seeds <- function(seed = 1L) {
  checkmate::assert_integerish(seed, len = 1L, lower = 1L, all.missing = FALSE)
  if(!is.integer(seed)) seed <- as.integer(seed)
  
  # set seed in...
  set.seed(seed) # R
  random <- import("random")
  random$seed(seed) # Random
  numpy <- import("numpy")
  numpy$random$seed(seed) # NumPy
  # disable hash randomization before importing tensorflow
  os <- import("os")
  os$environ$setdefault("PYTHONHASHSEED", value = "str(1)")
  # set tensorflow seed
  tensorflow <- import("tensorflow")
  tensorflow$random$set_seed(seed)
}
```

```{r kerasmodel}
build_model = function(seed = TRUE) {
  
  checkmate::assert_logical(seed, len = 1, all.missing = FALSE)
  if (seed)  reset_random_seeds()
  
  model = keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
                input_shape = boston_task$ncol - 1) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1)
  
  model %>% compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_absolute_error")
  )
  
  model
}

model = build_model()
model %>% summary()
```

# Learner

## Define hyperparameters

Heading back to `mlr_pkg("mlr3")` syntax  we set up the keras regression learner
like any other and define hyperparameters.
Note that the compiled `model` defined in the previous chapter is such a
hyperparameter.
That makes `regr.keras` learner easily tunable over different models.
For more information about tuning see
[mlr3 manual](https://mlr3book.mlr-org.com/optimization.html).

```{r hyperparams}
learner = po("learner", lrn("regr.keras"))

learner$param_set$values$epochs = 500L
learner$param_set$values$model = model
learner$param_set$values$validation_split = 0.2
```

### Callbacks

Another hypereparameter is `callback`.
A `callback` is a set of functions, bundeled in a list that can be used to
explore specified progress stages during training.
Accordingly a defined `callback` function is called at its' specified stage of
training to monitor or display specified model parameters at this stage.
The history callback for example is applied to each training progress of a
modell by default.
It stores the training history in a `history` slot of its' concerning modell
object.
For a list of all implemented callbacks see
[Keras Callbacks](https://keras.io/callbacks/).

In our first learner we want to reduce information about training progress to a
single dot printed out at the end of each applied epoch.
Therefore we create a `LambdaCallback` that apllied at each eochs end.

```{r callbacks}
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
) 

learner$param_set$values$callbacks = list(print_dot_callback)
```

Callback can also be called through 
[mlr3keras](https://github.com/mlr-org/mlr3keras/blob/master/R/keras_callbacks.R)
syntax.
As not all callbacks are implemented yet
[mlr3keras callbacks](https://github.com/mlr-org/mlr3keras/blob/master/R/keras_callbacks.R)
is still in progress.
Currently working callbacks are:

* `cb_es`: Early stopping callback
* `cb_lrs`: Learning rate scheduler callback
* `cb_tb`: Tensorboard callback
* `cb_lr_log`: Learning rate logger callback
* `LogMetrics`: Batch-wise Metrics Logger Callback
* `SetLogLR`: Batch-wise Metrics Setter Callback

Initialize and early stopping for instance like this:

```{r mlr3cb}
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20) # keras syntax
# early_stop <- cb_es(monitor = "val_loss", patience = 20) # mlr3 syntax
```


## Set Up the Learner

We initialize our learner.
Note, that we clone the `R6` object `keras_500` learner to a second 
object `keras_es` that we will use later on.

```{r po}
keras500 = GraphLearner$new(po_scale %>>% learner)
keras_es = keras500$clone(deep = TRUE)
```

# Training

Before we train the learner we devide observations randomly into train and test
set.
The training set contains 404 observations and the test set 102.
Now we can train our learner, using the `training_id`.
Reminder: We set in `callback`, each `epoch` is represented by a dot.

```{r trainingI}
train_id <- sample(boston_task$row_ids, 404L)
test_id <- setdiff(boston_task$row_ids, train_id)
keras500$train(boston_task, row_ids = train_id)
```

### Vizualize training progress

We plot the saved history from each `epoch`.
The plot shows the `MAE` on training and validation set for each epoch.

```{r plotI}
history = keras500$model$regr.keras$model$history
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
  coord_cartesian(ylim = c(0, 5))
```

The validation curve noticably flattens after about 200 epochs.
So `epochs = 500L` seems to be set to high.
In this case we can iplement our Early stopping callback.
`early_stop` will stop the modell if it shows no improvment in `val_loss` within
20 `epochs`.
Accordingly it should stop after about 200 `epochs`.

### Train with early stopping

```{r earlystopping}
keras_es$param_set$values$regr.keras.callbacks = list(early_stop, print_dot_callback)
keras_es$param_set$values$regr.keras.model = build_model()
keras_es$train(boston_task, row_ids = train_id)
history = keras_es$model$regr.keras$model$history
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
  coord_cartesian(xlim = c(0, 500), ylim = c(0, 5))
```

And the graph stopps after arounf 200 `epochs`.

## Predict and evaluate

In the last step we predict on the test set using `keras_es`.
Prediction is as easy as with any other learner.

```{r predict}
predict_boston = keras_es$predict(boston_task, row_ids = test_id)
head(predict_boston$data$tab, n = 3)
predict_boston$score(msr("regr.mae"))
```

```{r pastemae, echo=FALSE}
paste0(
  "The models average error is at about ",
  round(predict_boston$score(msr("regr.mae")) * 1000, 2),
  "$")
```


## Disgression: Reproducability

As `Keras R Studio` is a system running code on multiple platforms, obtaining
reproducibal results is not trivial.
This chapter provides ways to easily obtain reproducible results anyways.

### Tensorflow versions

This chapter will teach you a way to obtain reproducible results using `keras`.
More precisely we will stick to $tensorflow version \ge 2.0$.
You can check the current version like this:

```{r checkversion, eval=FALSE}
tensorflow::tf_version()
```

If you use $tensorflow version < 2.0$ you have two options.

1. `use_session_with_seed()`  
[RKerasFAQ](https://keras.rstudio.com/articles/faq.html#how-can-i-obtain-reproducible-results-using-keras-during-development)
shows a nice way how to set seeds by the function `use_session_with_seed()`.
In this case you can just replace `reset_random_seeds` in the `build_model`
function by applying `use_session_with_seed()`.

2. Use `reset_random_seeds` (see below)

### Reset Random seeds

As you might have noticed we have already been using `reset_random_seeds` in
this chapter to obtain reproducible results.
We introduced this function because in some incidinces you might run into an
error calling `use_session_with_seed()` with $tensorflow version < 2.0$.

```{r useerror, error=TRUE}
tensorflow::use_session_with_seed(1L)
```

`reset_random_seeds` establishes a common random seed for `R`, `Python`,
`NumPy`, and `TensorFlow` and disables hash randomization.
Note, that you should not have been running `TensorFlow` before running the
function.
If you did you might have to restart the session before proceeding.

```{r printfun}
print(reset_random_seeds)
```

For more information see
[Stackoverflow Reset Seeds](https://stackoverflow.com/questions/59075244/if-keras-results-are-not-reproducible-whats-the-best-practice-for-comparing-mo).

### Nondeterministic Executions

GPU computations and CPU parallelization can be another source of  
non-reproducibility, since both can result in non-deterministic execution
patterns.
The only way to provide results from this error is to disable GPU and CPU
paralelism.
This can mean a hard Tade-of between time and reproducability.
`use_session_with_seed()` diables both GPU and CPU by default. For more detailed
description read code on
[Github](https://github.com/rstudio/tensorflow/blob/master/R/seed.R).

## Conclusion

sdak bna

