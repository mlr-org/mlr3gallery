---
title: Encode factor levels for xgboost
categories:
  - classification
  - mlr3pipelines
  - factor encoding
author:
  - name: Michel Lang
date: 01-31-2020
description: |
  The package "xgboost" unfortunately does not support handling of categorical features. Therefore it is required to manually convert factor columns to numerical dummy features. We show how to use "mlr3pipelines" to augment the "mlr_learners_classif.xgboost" learner with an automatic factor encoding.
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80)
)
library(mlr3book)
library(mlr3)
lgr::get_logger("mlr3")$set_threshold("warn")
```

The package `r cran_pkg("xgboost")` unfortunately does not support handling of categorical features.
Therefore it is required to manually convert factor columns to numerical dummy features.
We show how to use `r mlr_pkg("mlr3pipelines")` to augment the `r ref("mlr_learners_classif.xgboost", "xgboost learner")` with an automatic factor encoding.

## Construct the Base Objects

First, we take an example task with factors (`r ref("mlr_tasks_german_credit", "german_credit")`) and create the `r ref("mlr_learners_classif.xgboost", "xgboost learner")`:

```{r}
library(mlr3)
library(mlr3learners)

task = tsk("german_credit")
print(task)

learner = lrn("classif.xgboost", nrounds = 100)
print(learner)
```

We now compare the feature types of the task and the supported feature types:

```{r}
unique(task$feature_types$type)
learner$feature_types
setdiff(task$feature_types$type, learner$feature_types)
```

In this example, we have to convert factors and ordered factors to numeric columns to apply the xgboost learner.
Because xgboost is based on decision trees (at least in its default settings), it is perfectly fine to convert the ordered factors to integer.
Unordered factors must still be encoded though.

# Construct Operators

The factor encoder's man page can be found under `r ref("mlr_pipeops_encode")`.
Here, we decide to use "treatment" encoding (first factor level serves as baseline, and there will be a new binary column for each additional level).
We restrict the operator to factor columns using the respective `r ref("Selector")` `selector_type()`:

```{r}
library(mlr3pipelines)
fencoder = po("encode", method = "treatment",
  affect_columns = selector_type("factor"))
```

We can manually trigger the `r ref("PipeOp")` to test the operator on our task:

```{r}
fencoder$train(list(task))
```

The ordered factor remained untouched, all other factors have been converted to numeric columns.
To also convert the ordered variable `employment_duration`, we construct the `r ref("mlr_pipeops_colapply", "colapply")` operator with the converter `as.integer()`:

```{r}
ord_to_int = po("colapply", applicator = as.integer,
  affect_columns = selector_type("ordered"))
```

Applied on the original task, it changes the factor column to `integer`:

```{r}
ord_to_int$train(list(task))
```

## Construct Pipeline

Finally, we construct a linear pipeline consisting of

1. the factor encoder `fencoder`,
2. the ordered factor converter `ord_to_int`, and
3. the xgboost base learner.

```{r}
pipe = fencoder %>>% ord_to_int %>>% learner
print(pipe)
```

The pipeline is wrapped in a `r ref("GraphLearner")` so that it behaves like a regular learner:

```{r}
glearner = GraphLearner$new(pipe)
```

We can now apply the new learner on the task, here with a 3-fold cross validation:

```{r}
rr = resample(task, glearner, rsmp("cv", folds = 3))
rr$aggregate()
```

Success! We augmented xgboost with handling of factors and ordered factors.
If we combine this learner with a tuner from `r mlr_pkg("mlr3tuning")`, we get a universal and competitive learner.
