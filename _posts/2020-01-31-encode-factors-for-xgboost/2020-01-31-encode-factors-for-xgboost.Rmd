---
title: Encode Factor Levels for xgboost
categories:
  - classification
  - mlr3pipelines
  - factor encoding
  - german credit data set
  - classification
author:
  - name: Michel Lang
date: 01-31-2020
description: |
  We show how to encode factor levels for the xgboost learner with mlr3pipelines.
output:
  distill::distill_article:
    self_contained: false
    highlight_downlit: true
    css: ../../custom.css
---

```{r 2020-01-31-encode-factors-for-xgboost-001, include=FALSE}
 knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 120)
)

library(mlr3book)
library(kableExtra)

ggplot2::theme_set(ggplot2::theme_light())
```

The package `r cran_pkg("xgboost")` unfortunately does not support handling of categorical features.
Therefore, it is required to manually convert factor columns to numerical dummy features.
We show how to use `r mlr_pkg("mlr3pipelines")` to augment the `r ref("mlr_learners_classif.xgboost", "xgboost learner")` with an automatic factor encoding.

We load the `r mlr_pkg("mlr3verse")` package which pulls in the most important packages for this example. 

```{r 2020-01-31-encode-factors-for-xgboost-002}
library(mlr3verse)
```

We initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.

```{r 2020-01-31-encode-factors-for-xgboost-003}
set.seed(7832)
lgr::get_logger("mlr3")$set_threshold("warn")
```

## Construct the Base Objects

First, we take an example task with factors (`r ref("mlr_tasks_german_credit", "german_credit")`) and create the `r ref("mlr_learners_classif.xgboost", "xgboost learner")`:

```{r 2020-01-31-encode-factors-for-xgboost-004}
library(mlr3learners)

task = tsk("german_credit")
print(task)

learner = lrn("classif.xgboost", nrounds = 100)
print(learner)
```

We now compare the feature types of the task and the supported feature types:

```{r 2020-01-31-encode-factors-for-xgboost-005}
unique(task$feature_types$type)
learner$feature_types
setdiff(task$feature_types$type, learner$feature_types)
```

In this example, we have to convert factors and ordered factors to numeric columns to apply the xgboost learner.
Because xgboost is based on decision trees (at least in its default settings), it is perfectly fine to convert the ordered factors to integer.
Unordered factors must still be encoded though.

# Construct Operators

The factor encoder's man page can be found under `r ref("mlr_pipeops_encode")`.
Here, we decide to use "treatment" encoding (first factor level serves as baseline, and there will be a new binary column for each additional level).
We restrict the operator to factor columns using the respective `r ref("Selector")` `selector_type()`:

```{r 2020-01-31-encode-factors-for-xgboost-006}
fencoder = po("encode", method = "treatment", affect_columns = selector_type("factor"))
```

We can manually trigger the `r ref("PipeOp")` to test the operator on our task:

```{r 2020-01-31-encode-factors-for-xgboost-007}
fencoder$train(list(task))
```

The ordered factor remained untouched, all other factors have been converted to numeric columns.
To also convert the ordered variables `installment_rate`, `number_credits`, and `present_residence`, we construct the `r ref("mlr_pipeops_colapply", "colapply")` operator with the converter `as.integer()`:

```{r 2020-01-31-encode-factors-for-xgboost-008}
ord_to_int = po("colapply", applicator = as.integer, affect_columns = selector_type("ordered"))
```

Applied on the original task, it changes factor columns to `integer`:

```{r 2020-01-31-encode-factors-for-xgboost-009}
ord_to_int$train(list(task))
```

## Construct Pipeline

Finally, we construct a linear pipeline consisting of

1. the factor encoder `fencoder`,
2. the ordered factor converter `ord_to_int`, and
3. the xgboost base learner.

```{r 2020-01-31-encode-factors-for-xgboost-010}
graph = fencoder %>>% ord_to_int %>>% learner
print(graph)
```

The pipeline is wrapped in a `r ref("GraphLearner")` so that it behaves like a regular learner:

```{r 2020-01-31-encode-factors-for-xgboost-011}
graph_learner = as_learner(graph)
```

We can now apply the new learner on the task, here with a 3-fold cross validation:

```{r 2020-01-31-encode-factors-for-xgboost-012}
rr = resample(task, graph_learner, rsmp("cv", folds = 3))
rr$aggregate()
```

Success! We augmented xgboost with handling of factors and ordered factors.
If we combine this learner with a tuner from `r mlr_pkg("mlr3tuning")`, we get a universal and competitive learner.
