---
title: "Pipelines, selectors, branches"
categories:
  - mlr3pipelines
author:
  - name: Milan Dragicevic
  - name: Giuseppe Casalicchio
date: 04-23-2020
description: |
  This tutorial explains how applying different preprocessing steps on different features and branching of preprocessing steps can be achieved using the mlr3pipelines package.
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---

## Intro

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(R.options = list(width = 100),
               echo = TRUE)
```

[mlr3pipelines](https://mlr3pipelines.mlr-org.com/) offers a very flexible way to create data preprocessing steps.
This is achieved by a modular approach using [`PipeOps`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html). 
For detailed overview check the [mlr3book](https://mlr3book.mlr-org.com/pipelines.html).  

Recommended prior readings:

- [mlr3pipelines tutorial - german credit](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3pipelines-tutorial-german-credit/)
- [Impute missing variables](https://mlr3gallery.mlr-org.com/posts/2020-01-30-impute-missing-levels/) .  

This post covers:  

1. How to apply different preprocessing steps on different features
2. How to branch different preprocessing steps, which allows to select the best performing path
3. How to tune the whole pipeline

## Prerequisites

```{r}
library(mlr3)
library(mlr3pipelines)
library(mlr3tuning)
library(paradox)
```

The [Pima Indian Diabetes classification task](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) will be used.

```{r}
pima_tsk = tsk("pima")
pima_tsk$data()
skimr::skim(pima_tsk$data())
```

## Selection of features for preprocessing steps

Several features of the `pima` task have missing values:  

```{r}
pima_tsk$missings()
```

A common approach in such situations is to impute the missing values and to add a missing indicator column as explained in the [Impute missing variables](https://mlr3gallery.mlr-org.com/posts/2020-01-30-impute-missing-levels/) post.
Suppose we want to use

- [`imputehist`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html)  on features "glucose", "mass" and "pressure" which have only few missing values and
- [`imputemedian`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputemedian.html) on features "insulin" and "triceps" which have much more missing values.  

In the following subsections, we show two approaches to implement this.

### 1. Consider all features and apply the preprocessing step only to certain features

Using the `affect_columns` argument of a `PipeOp` to define the variables on which a `PipeOp` will operate with an appropriate [`selector`](https://mlr3pipelines.mlr-org.com/reference/Selector.html) function:

```{r}
# imputes values based on histogram
hist_imp = po("imputehist",
              param_vals = list(affect_columns = selector_name(c("glucose", "mass", "pressure"))))
# imputes values using the median
median_imp = po("imputemedian",
                param_vals = list(affect_columns = selector_name(c("insulin", "triceps"))))
# adds an indicator column for each feature with missing values
miss_ind = po("missind")
```

When `PipeOp`s are constructed this way, they will perform the specified preprocessing step on the appropriate features and pass all the input features to the subsequent steps:

```{r}
# no missings in "glucose", "mass" and "pressure"
hist_imp$train(list(pima_tsk))[[1]]$missings()
# no missings in "insulin" and "triceps"
median_imp$train(list(pima_tsk))[[1]]$missings()
```

We construct a pipeline that combines `hist_imp` and `median_imp`.
Here, `hist_imp` will impute the features "glucose", "mass" and "pressure", and `median_imp` will impute "insulin" and "triceps".
In each preprocessing step, all the input features are passed to the next step. 
In the end, we obtain a data set without missing values:  

```{r, fig.height = 6, fig.width = 6}
# combine the two impuation methods
impute_graph = hist_imp %>>% median_imp
impute_graph$plot(html = FALSE)
impute_graph$train(pima_tsk)[[1]]$missings()
```

The [`missind`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_missind.html) operator replaces features with missing values with a missing value indicator:

```{r}
miss_ind$train(list(pima_tsk))[[1]]$data()
```

Obviously, this step can not be applied to the already imputed data as there are no missing values.
If we want to combine the previous two imputation steps with a third step that adds missing value indicators, we would need to [`copy`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_copy.html) the data two times and supply the first copy to `impute_graph` and the second copy to `miss_ind` using [`gunion`](https://mlr3pipelines.mlr-org.com/reference/gunion.html).
Finally, the two outputs can be combined with [`featureunion`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_featureunion.html):  

```{r, fig.height = 6, fig.width = 6}
impute_missind = po("copy", 2) %>>%
  gunion(list(impute_graph, miss_ind)) %>>%
  po("featureunion")
impute_missind$plot(html = FALSE)
impute_missind$train(pima_tsk)[[1]]$data()
```

### 2. Select the features for each preprocessing step and apply the preprocessing steps to this subset

We can use the [`select`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_select.html) `PipeOp` to select the appropriate features and then apply the desired impute `PipeOp` on them:

```{r, fig.height = 6, fig.width = 6}
hist_imp2 = po("select",
  selector = selector_name(c("glucose", "mass", "pressure")),
  id = "slct1") %>>% # unique id so we can combine it in a pipeline with other select PipeOps
  po("imputehist")

hist_imp2$plot(html = FALSE)  

hist_imp2$train(pima_tsk)[[1]]$data()

median_imp2 = po("select", selector = selector_name(c("insulin", "triceps")), id = "slct2") %>>%
  po("imputemedian")

median_imp2$train(pima_tsk)[[1]]$data()
```

To reproduce the result of the fist example (1.), we need to copy the data four times and apply `hist_imp2`, `median_imp2` and `miss_ind` on each of the three copies.
The fourth copy is required to select the features without missing values and to append it to the final result.
We can do this as follows:

```{r, fig.height = 6, fig.width = 6}
other_features = pima_tsk$feature_names[pima_tsk$missings()[-1] == 0]

impute_missind2 = po("copy", 4) %>>%
  gunion(list(hist_imp2,
    median_imp2,
    miss_ind,
    po("select", selector = selector_name(other_features), id = "slct3"))) %>>%
  po("featureunion")

impute_missind2$plot(html = FALSE)

impute_missind2$train(pima_tsk)[[1]]$data()
```

Note that when there is one input channel, it is automatically copied as many times as needed for the downstream `PipeOp`s. 
In other words, the code above works also without `po("copy", 4)`:  

```{r}
impute_missind3 = gunion(list(hist_imp2,
  median_imp2,
  miss_ind,
  po("select", selector = selector_name(other_features), id = "slct3"))) %>>%
  po("featureunion")

impute_missind3$train(pima_tsk)[[1]]$data()
```

Usually, `po("copy")` is required when there are more than one input channels and multiple output channels, and their numbers do not match.

## Branching

We can not know if the combination of a learner with this preprocessing graph will benefit from the imputation steps and the added missing value indicators.
Maybe it would have been better to just use [`imputemedian`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputemedian.html) on all the variables.
We could investigate this assumption by adding an alternative path to the graph with the mentioned `imputemedian`.
This is possible using the ["branch"](https://mlr3pipelines.mlr-org.com/reference/branch.html) `PipeOp`:  

```{r, fig.height = 7, fig.width = 7}
median_imp3 = po("imputemedian", id = "simple_median") # add the id so it does not clash with `median_imp` 

branches = c("impute_missind", "simple_median") # names of the branches

graph_branch = po("branch", branches) %>>%
  gunion(list(impute_missind, median_imp3)) %>>%
  po("unbranch")

graph_branch$plot(html = FALSE)
```

## Tuning the pipeline

To finalize the graph, we combine it with a rpart learner:  

```{r, fig.height = 7, fig.width = 7}
rpart_lrn = lrn("classif.rpart")

grph = graph_branch %>>%
  rpart_lrn

grph$plot(html = FALSE)
```

To define the parameters to be tuned, we first check the available ones in the graph:  

```{r}
grph$param_set
```

We decide to jointly tune the "branch.selection", "classif.rpart.cp" and "classif.rpart.minbucket" hyperparameters:  

```{r}
ps = ParamSet$new(
  list(
    ParamFct$new("branch.selection", levels = c("impute_missind", "simple_median")),
    ParamDbl$new("classif.rpart.cp", 0.001, 0.1),
    ParamInt$new("classif.rpart.minbucket", 1, 10)
  ))
```

In order to tune the graph, it needs to be converted to a learner:  

```{r}
grph_lrn =  GraphLearner$new(grph)

cv3 = rsmp("cv", folds = 3)

set.seed(123) # for reproducibility of the folds
cv3$instantiate(pima_tsk) # to generate folds for cross validation

instance = TuningInstance$new(
  task = pima_tsk,
  learner = grph_lrn,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  param_set = ps,
  terminator = term("evals", n_evals = 5)
)

tuner = TunerRandomSearch$new()
set.seed(321)
tuner$tune(instance)

instance$archive()
```

The best performance in this short tuned experiment was achieved with `r dQuote(instance$result$params$branch.selection)`.  

```{r}
instance$result$params
```

## Conclusion

This post shows ways on how to specify features on which preprocessing steps are to be performed. In addition it shows how to create alternative paths in the learner graph. The preprocessing steps that can be used are not limited to imputation. Check the list of available [`PipeOp`s](https://mlr3pipelines.mlr-org.com/reference/index.html).  
