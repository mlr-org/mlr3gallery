---
title: Impute missing variables
categories:
  - classification
  - imputation
  - mlr3pipelines
author:
  - name: Florian Pfisterer
date: 01-31-2020
description: |
  We show how to use mlr3pipelines to augment the "mlr_learners_classif.ranger" learner with automatic imputation.
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80)
)
library(mlr3book)
library(mlr3)
lgr::get_logger("mlr3")$set_threshold("warn")
```

## Prerequisites

This tutorial assumes familiarity with the basics of `r mlr_pkg("mlr3pipelines")`.
Consult the [mlr3book](https://mlr3book.mlr-org.com/pipelines.html) if some aspects are not  fully understandable.
It deals with the problem of missing data.

The random forest implementation in the package `r cran_pkg("ranger")` unfortunately does not support missing values.
Therefore, it is required to impute missing features before passing the data to the learner.

We show how to use `r mlr_pkg("mlr3pipelines")` to augment the `r ref("mlr_learners_classif.ranger", "ranger random forest learner")` with automatic imputation.

## Construct the Base Objects

First, we take an example task with missing values (`r ref("mlr_tasks_pima", "pima")`) and create the `r ref("mlr_learners_classif.ranger", "ranger learner")`:

```{r}
library(mlr3)
library(mlr3learners)

task = tsk("pima")
print(task)

learner = lrn("classif.ranger")
print(learner)
```

We can now inspect the task for missing values.
`task$missings()` returns the count of missing values for each variable.

```{r}
task$missings()
```

Additionally, we can see that the `r ref("mlr_learners_classif.ranger", "ranger learner")` can not handle missing values:

```{r}
learner$properties
```

For comparison, other learners, e.g. the `r ref("mlr_learners_classif.rpart", "rpart learner")` can handle missing values internally.

```{r}
lrn("classif.rpart")$properties
```

Before we dive deeper, we quickly try to visualize the columns with many missing values:

```{r, message = FALSE, warning = FALSE}
library(mlr3viz)
autoplot(task$clone()$select(c("insulin", "triceps")), type = "pairs")
```

## Operators overview

An overview over implemented `r ref("PipeOp")`s for imputation can be obtained like so:

```{r}
library(mlr3pipelines)
dt = as.data.table(mlr_pipeops)
dt[grepl("impute", dt$key) | grepl("miss", dt$key), ]
```
<!-- Replace with something using tags at some point-->

## Construct Operators

`r mlr_pkg("mlr3pipelines")` contains several imputation methods.
We focus on rather simple ones, and show how to impute missing values for `factor` features and
`numeric` features respectively.

Since our task only has numeric features, we do not need to deal with imputing factor levels,
and can instead concentrate on imputing numeric values:

We do this in a two-step process:
* We create new indicator columns, that tells us whether the value of a feature is "missing" or "present".
  We achieve this using the  `r ref("mlr_pipeops_missind", "missind")` `r ref("PipeOp")`.

* Afterwards, we impute every missing value by sampling from the histogram of the respective column.
  We achieve this using the  `r ref("mlr_pipeops_imputehist", "imputehist")` `r ref("PipeOp")`.

We also have to make sure to apply the pipe operators in the correct order!

```{r}
imp_missind = po("missind")
imp_num = po("imputehist", param_vals = list(affect_columns = selector_type("numeric")))
```

In order to better understand we can look at the results of every `r ref("PipeOp")` separately.

We can manually trigger the `r ref("PipeOp")` to test the operator on our task:

```{r}
ext_task = imp_missind$train(list(task))[[1]]
ext_task$data()
```

For `r ref("mlr_pipeops_imputehist", "imputehist")`, we can do the same:

```{r}
ext_task = imp_num$train(list(task))[[1]]
ext_task$data()
```

This time we obtain the imputed data set without `missing` values.

```{r}
ext_task$missings()
```

## Putting everything together

Now we have to put all `r ref("PipeOp")`s  together in order to form a graph that handles imputation automatically.

We do this by creating a `r ref("Graph")` that copies the data twice, processes each copy using the respective imputation method and afterwards unions the features.
For this we need the following two `r ref("PipeOp")`s :
* `r ref("mlr_pipeops_copy", "copy")`: Creates copies of the data.
* `r ref("mlr_pipeops_featureunion", "featureunion")` Merges the two tasks together.


```{r}
graph = po("copy", 2) %>>% gunion(list(imp_missind, imp_num)) %>>% po("featureunion")
```

as a last step we append the learner we planned on using:

```{r}
graph = graph %>>% po(learner)
```

We can now visualize the resulting graph:

```{r}
graph$plot()
```

## Resampling

Correct imputation is especially important when applying imputation to held-out data during the `predict` step.
If applied incorrectly, imputation could leak info from the test set, which potentially skews our performance estimates.
`r mlr_pkg("mlr3pipelines")` takes this complexity away from the user and handles correct imputation internally.

By wrapping this graph into a  `r ref("GraphLearner")`, we can now train resample the full graph, here with a 3-fold cross validation:

```{r}
glearner = GraphLearner$new(graph)
rr = resample(task, glearner, rsmp("cv", folds = 3))
rr$aggregate()
```

## Missing values during prediction

In some cases, we have missing values only in the data we want to predict on.
In order to showcase this, we create a copy of the task with several more missing columns.

```{r}
dt = task$data()
dt[1:10, "age"] = NA
dt[30:70, "pedigree"] = NA
task2 = TaskClassif$new("pima2", dt, target = "diabetes")
```

And now we learn on `task`, while trying to predict on `task2`.

```{r}
glearner$train(task)
glearner$predict(task2)
```

## Missing factor features

For `factor` features, the process works analogously.
Instead of using `r ref("mlr_pipeops_imputehist", "imputehist")`, we can for example use `r ref("mlr_pipeops_imputeoor", "imputeoor")`.
This will simply replace every `NA` in each factor variable with a new value `missing`.

A full graph might the look like this:

```{r}
imp_missind = po("missind", param_vals = list(affect_columns = NULL, which = "all"))
imp_fct = po("imputeoor",
  param_vals = list(affect_columns = selector_type("factor")))
graph = po("copy", 2) %>>%
  gunion(list(imp_missind, imp_num %>>% imp_fct)) %>>%
  po("featureunion")
```

Note that we specify the parameter `affect_columns = NULL` when initializing `r ref("mlr_pipeops_missind", "missind")`, because we also want indicator columns for our `factor` features.
By default, `affect_columns` would be set to `selector_invert(selector_type(c("factor", "ordered", "character")))`.
We also set the parameter `which` to `"all"` to add indicator columns for all features, regardless whether values were missing during training or not.

In order to test out our new graph, we again create a situation where our task has missing factor levels.
As the (`r ref("mlr_tasks_pima", "pima")`) task does not have any factor levels, we use the
famous (`r ref("mlr_tasks_boston_housing", "boston_housing")`) task.

```{r}
# t1 is the training data without missings
t1 = tsk("boston_housing")

# t2 is the prediction data with missings
dt = t1$data()
dt[1:10, chas := NA][20:30, rm := NA]
t2 = TaskRegr$new("bh", dt, target = "medv")
```

Now we train on `t1` and predict on `t2`:

```{r}
gl = GraphLearner$new(graph %>>% po(lrn("regr.ranger")))
gl$train(t1)
gl$predict(t2)
```

Success! We learned how to deal with missing values in less than 10 minutes.
