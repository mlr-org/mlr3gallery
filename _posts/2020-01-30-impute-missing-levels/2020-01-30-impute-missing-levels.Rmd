---
title: Impute Missing Variables
categories:
  - classification
  - imputation
  - mlr3pipelines
  - pima data set
  - boston housing data set
  - classification
author:
  - name: Florian Pfisterer
date: 01-31-2020
description: |
  We show how to use mlr3pipelines to augment the ranger learner with automatic imputation.
output:
  distill::distill_article:
    toc: true
    self_contained: false
    highlight_downlit: true
    css: ../../custom.css
---

```{r 2020-01-30-impute-missing-levels-001, include=FALSE}
 knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 120)
)

library(mlr3book)
library(kableExtra)

ggplot2::theme_set(ggplot2::theme_light())
```

## Prerequisites

This tutorial assumes familiarity with the basics of `r mlr_pkg("mlr3pipelines")`.
Consult the [mlr3book](https://mlr3book.mlr-org.com/pipelines.html) if some aspects are not  fully understandable.
It deals with the problem of missing data.

The random forest implementation in the package `r cran_pkg("ranger")` unfortunately does not support missing values.
Therefore, it is required to impute missing features before passing the data to the learner.

We show how to use `r mlr_pkg("mlr3pipelines")` to augment the `r ref("mlr_learners_classif.ranger", "ranger learner ")` with automatic imputation.

We load the `r mlr_pkg("mlr3verse")` package which pulls in the most important packages for this example.

```{r 2020-01-30-impute-missing-levels-002}
library(mlr3verse)
```

We initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.

```{r 2020-01-30-impute-missing-levels-003}
set.seed(7832)
lgr::get_logger("mlr3")$set_threshold("warn")
```

## Construct the Base Objects

First, we take an example task with missing values (`r ref("mlr_tasks_pima", "pima")`) and create the `r ref("mlr_learners_classif.ranger", "ranger learner")`:

```{r 2020-01-30-impute-missing-levels-004}
library(mlr3learners)

task = tsk("pima")
print(task)

learner = lrn("classif.ranger")
print(learner)
```

We can now inspect the task for missing values.
`task$missings()` returns the count of missing values for each variable.

```{r 2020-01-30-impute-missing-levels-005}
task$missings()
```

Additionally, we can see that the `r ref("mlr_learners_classif.ranger", "ranger learner")` can not handle missing values:

```{r 2020-01-30-impute-missing-levels-006}
learner$properties
```

For comparison, other learners, e.g. the `r ref("mlr_learners_classif.rpart", "rpart learner")` can handle missing values internally.

```{r 2020-01-30-impute-missing-levels-007}
lrn("classif.rpart")$properties
```

Before we dive deeper, we quickly try to visualize the columns with many missing values:

```{r 2020-01-30-impute-missing-levels-008, message = FALSE, warning = FALSE, fig.width=10, fig.height=8}
autoplot(task$clone()$select(c("insulin", "triceps")), type = "pairs")
```

## Operators overview

An overview over implemented `r ref("PipeOp")`s for imputation can be obtained like so:

```{r 2020-01-30-impute-missing-levels-009}
as.data.table(mlr_pipeops)[tags %in% "missings", list(key)]
```

## Construct Operators

`r mlr_pkg("mlr3pipelines")` contains several imputation methods.
We focus on rather simple ones, and show how to impute missing values for `factor` features and
`numeric` features respectively.

Since our task only has numeric features, we do not need to deal with imputing factor levels,
and can instead concentrate on imputing numeric values:

We do this in a two-step process:
* We create new indicator columns, that tells us whether the value of a feature is "missing" or "present".
  We achieve this using the  `r ref("mlr_pipeops_missind", "missind")` `r ref("PipeOp")`.

* Afterwards, we impute every missing value by sampling from the histogram of the respective column.
  We achieve this using the  `r ref("mlr_pipeops_imputehist", "imputehist")` `r ref("PipeOp")`.

We also have to make sure to apply the pipe operators in the correct order!

```{r 2020-01-30-impute-missing-levels-010}
imp_missind = po("missind")
imp_num = po("imputehist", affect_columns = selector_type("numeric"))
```

In order to better understand we can look at the results of every `r ref("PipeOp")` separately.

We can manually trigger the `r ref("PipeOp")` to test the operator on our task:

```{r 2020-01-30-impute-missing-levels-011}
task_ext = imp_missind$train(list(task))[[1]]
task_ext$data()
```

For `r ref("mlr_pipeops_imputehist", "imputehist")`, we can do the same:

```{r 2020-01-30-impute-missing-levels-012}
task_ext = imp_num$train(list(task))[[1]]
task_ext$data()
```

This time we obtain the imputed data set without `missing` values.

```{r 2020-01-30-impute-missing-levels-013}
task_ext$missings()
```

## Putting everything together

Now we have to put all `r ref("PipeOp")`s  together in order to form a graph that handles imputation automatically.

We do this by creating a `r ref("Graph")` that copies the data twice, processes each copy using the respective imputation method and afterwards unions the features.
For this we need the following two `r ref("PipeOp")`s :
* `r ref("mlr_pipeops_copy", "copy")`: Creates copies of the data.
* `r ref("mlr_pipeops_featureunion", "featureunion")` Merges the two tasks together.


```{r 2020-01-30-impute-missing-levels-014}
graph = po("copy", 2) %>>%
  gunion(list(imp_missind, imp_num)) %>>%
  po("featureunion")
```

as a last step we append the learner we planned on using:

```{r 2020-01-30-impute-missing-levels-015}
graph = graph %>>% po(learner)
```

We can now visualize the resulting graph:

```{r 2020-01-30-impute-missing-levels-016, fig.width=6, fig.height=8}
# graph$plot()
```

## Resampling

Correct imputation is especially important when applying imputation to held-out data during the `predict` step.
If applied incorrectly, imputation could leak info from the test set, which potentially skews our performance estimates.
`r mlr_pkg("mlr3pipelines")` takes this complexity away from the user and handles correct imputation internally.

By wrapping this graph into a  `r ref("GraphLearner")`, we can now train resample the full graph, here with a 3-fold cross validation:

```{r 2020-01-30-impute-missing-levels-017}
graph_learner = as_learner(graph)
rr = resample(task, graph_learner, rsmp("cv", folds = 3))
rr$aggregate()
```

## Missing values during prediction

In some cases, we have missing values only in the data we want to predict on.
In order to showcase this, we create a copy of the task with several more missing columns.

```{r 2020-01-30-impute-missing-levels-018}
dt = task$data()
dt[1:10, "age"] = NA
dt[30:70, "pedigree"] = NA
task_2 = as_task_classif(dt, id = "pima2", target = "diabetes")
```

And now we learn on `task`, while trying to predict on `task_2`.

```{r 2020-01-30-impute-missing-levels-019}
graph_learner$train(task)
graph_learner$predict(task_2)
```

## Missing factor features

For `factor` features, the process works analogously.
Instead of using `r ref("mlr_pipeops_imputehist", "imputehist")`, we can for example use `r ref("mlr_pipeops_imputeoor", "imputeoor")`.
This will simply replace every `NA` in each factor variable with a new value `missing`.

A full graph might the look like this:

```{r 2020-01-30-impute-missing-levels-020}
imp_missind = po("missind", affect_columns = NULL, which = "all")
imp_fct = po("imputeoor", affect_columns = selector_type("factor"))
graph = po("copy", 2) %>>%
  gunion(list(imp_missind, imp_num %>>% imp_fct)) %>>%
  po("featureunion")
```

Note that we specify the parameter `affect_columns = NULL` when initializing `r ref("mlr_pipeops_missind", "missind")`, because we also want indicator columns for our `factor` features.
By default, `affect_columns` would be set to `selector_invert(selector_type(c("factor", "ordered", "character")))`.
We also set the parameter `which` to `"all"` to add indicator columns for all features, regardless whether values were missing during training or not.

In order to test out our new graph, we again create a situation where our task has missing factor levels.
As the (`r ref("mlr_tasks_pima", "pima")`) task does not have any factor levels, we use the
famous (`r ref("mlr_tasks_boston_housing", "boston_housing")`) task.

```{r 2020-01-30-impute-missing-levels-021}
# task_bh_1 is the training data without missings
task_bh_1 = tsk("boston_housing")

# task_bh_2 is the prediction data with missings
dt = task_bh_1$data()
dt[1:10, chas := NA][20:30, rm := NA]
task_bh_2 = as_task_regr(dt, id = "bh", target = "medv")
```

Now we train on `task_bh_1` and predict on `task_bh_2`:

```{r 2020-01-30-impute-missing-levels-022}
graph_learner = as_learner(graph %>>% po(lrn("regr.ranger")))
graph_learner$train(task_bh_1)
graph_learner$predict(task_bh_2)
```

Success! We learned how to deal with missing values in less than 10 minutes.
