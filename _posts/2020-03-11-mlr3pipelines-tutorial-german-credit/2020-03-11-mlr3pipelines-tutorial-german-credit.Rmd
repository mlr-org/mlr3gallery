---
title: mlr3pipelines tutorial - german credit
categories:
  - tuning
  - imputation
  - filtering
author:
  - name: Martin Binder
  - name: Florian Pfisterer
date: 03-11-2020
description: |
  In this use case, we will continue working with the German Credit Dataset. We already used different Learners on it in previous posts and tried to optimize their hyperparameters. To make things interesting, we artificially introduce missing values into the dataset.
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80)
)
data.table::setDTthreads(1)
set.seed(20191101)
lgr::get_logger("mlr3")$set_threshold("warn")
```

## Outline

This is the third part of a serial of use cases with the German Credit data.
The other parts of this series can be found here:

* [Part I - Basics](/basics_german_credit/)
* [Part II - Tuning](/basics_tuning_german_credit/)

In this tutorial, we will continue working with the **German Credit Dataset**. 
We already used different `Learner`s on it and tried to optimize their hyperparameters. 
Now we will do four additional things.
First, we preprocess the data as an integrated step of the model fitting process.
The integration may be important because...
Then, we tune the associated preprocessing parameters.
Third, we combine multiple `Learners` in an *ensemble* model.
Last, we discuss some techniques that make `Learner`s able to tackle _challenging_ datasets that they could not handle otherwise.
(We are going to outline what challenging means in particular later on.)

## Prerequisites

```{r, message=FALSE, warning=FALSE}
library("data.table")
library("mlr3")
library("mlr3learners")
library("mlr3pipelines")
library("mlr3tuning")
library("ggplot2")
```

```{r, include=FALSE}
theme_set(theme_light())
```

We use the same data as in the mlr3_basics_german_credit example, but will restrict ourselves to the *numerical features*. 
To make things interesting or to make it a bit harder for our learners, we introduce *missing values* in the dataset.

```{r, message=FALSE}
task = tsk("german_credit")
credit_full = task$data()
credit = credit_full[, sapply(credit_full, is.numeric), with = FALSE]

set.seed(20191101)
## turn 10% of values into an NA
credit = credit[, lapply(.SD, function(x) {
  x[sample(c(TRUE, NA), length(x), replace = TRUE, prob = c(.9, .1))]
})]
credit$credit_risk = credit_full$credit_risk
task = TaskClassif$new("GermanCredit", credit, "credit_risk")
```

We instantiate a resampling instance for this task to be able to compare resampling performance.

```{r}
set.seed(20191101)
cv10_instance = rsmp("cv")$instantiate(task)
```

You can uncomment the following line if you are running this locally (i.e. __not__ on RStudio Cloud).

```{r, warning=FALSE}
# future::plan("multiprocess")
```

## Intro

In this use case, we will take a look at composite machine learning algorithms that may incorporate data preprocessing or the combination of multiple `Learner`s ("ensemble methods").

We use the **mlr3pipelines** package that enables us to chain "`PipeOp`" objects into data flow graphs. 
Load the package using

```{r}
library("mlr3pipelines")
```

Available `PipeOp`s are enumareted in the `mlr_pipeops` dictionary:

```{r}
mlr_pipeops
```

## Missing Value Imputation

We introduced missing values into our data.
While some machine learning learners can deal with missing value, many cannot.
Trying to train a Random Forest fails because of this.

```{r, error = TRUE}
ranger = lrn("classif.ranger")
ranger$train(task)
```

We can perform imputation using a `PipeOp`. 
To find out what are the imputation `PipeOp`s that are available we do the following:

```{r}
mlr_pipeops$keys("^impute")
```

We choose to impute numeric features by their mean. 
If there were missing values of factorial features, we could also (or actually we would need to) add a separate imputer for them, e.g. `po("imputenewlvl")`.
Let's use the `PipeOp` itself to create an imputed task. 
This shows us how the `PipeOp` works.

```{r}
imputer = po("imputemean")

task_imputed = imputer$train(list(task))[[1]]

task_imputed$head() # no missing values
```

The `$state$model` slot contains the medians of all columns. 
The `PipeOp` needs to remember these for the `$predict()` phase.
We do not only need complete data for modeling but also prediction.
Using the same imputation heuristic for both is the most consistent strategy.
This way the imputation strategy can, in fact, be seen as a part of the complete learner (which could be tuned).

```{r}
imputer$state$model
```

If we used the imputed task for resampling, we would _leak_ information from the test set into the training set.
Therefore, it is mandatory to attach the imputation operator to the `Learner` itself, creating a `GraphLearner`.

```{r}
imp_ranger = GraphLearner$new(po("imputemean") %>>% ranger)

imp_ranger$train(task) # runs without error: training succeeds
```

This graph learner can be used for resampling -- like an ordinary learner.

```{r}
rr = resample(task, imp_ranger, cv10_instance)
rr$aggregate()
```

## Feature Filtering

Typically, sparse models, i.e. having models with few(er) features, are desirable.
This is due to a variety of reasons.
Some of them are enhanced interpretability or decreased costs of acquiring data.
Furthermore, sparse models may actually be associated with increased performance (especially if overfitting is anticipated).
We can use *feature filter* to only keep these features with the most _information_.

```{r}
library("mlr3filters")
mlr_filters
```

We apply the `"anova"` filter. 
It makes use of an F-test for values in different target classes (equivalent to a t-test in the binary classification case).

```{r}
filter = flt("anova")
filter$calculate(task_imputed)$scores
```

Making use of these filters, you may wonder at which costs the reduction of the feature space comes.
We can investigate the trade-off between features and performance by tuning.
We incorporate our filtering strategy into the pipeline using the `"filter"` `PipeOp`.
Like before, we need to perform imputation as the filter also relies on complete data.

```{r}
fpipe = po("imputemean") %>>% po("filter", filter, filter.nfeat = 3)

fpipe$train(task)[[1]]$head()
```

We tune over the `anova.filter.nfeat` parameter.
It steers how many features are kept by the filter and eventually used by the learner.

```{r}
library("paradox")
searchspace = ParamSet$new(list(
  ParamInt$new("anova.filter.nfeat", lower = 1, upper = length(task$feature_names))
))
```

The problem is one-dimensional (i.e. only one parameter is tuned).
Thus, we make use of a grid search. 
For higher dimensions, strategies like random search are more appropriate.

```{r}
inst = TuningInstance$new(
  task, fpipe %>>% lrn("classif.ranger"), cv10_instance, msr("classif.ce"),
  searchspace, term("none")
)
tuner = tnr("grid_search")
```

The tuning procedure may take some time.

```{r, warning = FALSE}
tuner$tune(inst)
```

We can plot the performance against the number of features.
If we do so, we see the possible trade-off between sparsity and predictive performance.
While in this setting sparsity looks desirable, there should be at least two features which are used.

```{r}
arx = inst$archive("params")
ggplot(arx, aes(x = anova.filter.nfeat, y = classif.ce)) + geom_line()
```

## Stacking

We want to build a model that is based on the predictions of other learners.
This means that we are in the state that we need predictions already during training.
This is a very specific case that is luckily handled by the `"learner_cv"` PipeOp.
The `"learner_cv"` PipeOp performs cross-validation during the training phase and returns the cross-validated predictions.
We use `"prob"` predictions because they carry more information than response predictions.

```{r}
stackgraph = po("imputemean") %>>%
  list(
    po("learner_cv", lrn("classif.ranger", predict_type = "prob")),
    po("learner_cv", lrn("classif.kknn", predict_type = "prob"))) %>>%
  po("featureunion") %>>% lrn("classif.log_reg")
```

We build a pretty complex graph `Graph` already.
In order not to lose the overview, we suggest plotting complex graphs from time to time.

```{r}
stackgraph$plot(html = TRUE)
```

```{r, warning = FALSE}
rr = resample(task, stackgraph, cv10_instance, store_model = TRUE)
rr$aggregate()
```

We compare the performance of the stacked learner to the performance of the individual `Learner`s. 
The stacked learner, the random forest, and the logistic regression seem to have negligible difference in their cross-validated `mmce` only.

```{r, warning = FALSE}
bmr = benchmark(data.table(
  task = list(task),
  learner = list(
    GraphLearner$new(po("imputemean") %>>% lrn("classif.ranger")),
    GraphLearner$new(po("imputemean") %>>% lrn("classif.kknn")),
    GraphLearner$new(po("imputemean") %>>% lrn("classif.log_reg"))),
  resampling = list(cv10_instance)))
bmr$aggregate()[, c("learner_id", "classif.ce")]
```

The stacked `Learner` is a logistic regression.
Hence, the model output already tells us a lot on the importance of the features, or in this case the parts of the stacked learner.
If we train the stacked `Learner` and look into the model (the logistic regression), we can see how "important" each part of the stacked models is.

```{r}
stackgraph$train(task)

summary(stackgraph$pipeops$classif.log_reg$state$model)
```

The Random Forest (`ranger`) contributes more to the outcome.
This is not surprising because it is generally a _stronger_ model.

## Robustify: Preventing new Prediction Factor Levels and other Problems

Now we shift the context: 
We take the complete German Credit dataset.

```{r, message=FALSE}
task = tsk("german_credit")

task$head()
```

There is a potential practical problem for both, small data sets and data sets with covariates having many factor levels.
It may occur that not all possible factor levels have been used by the `Learner` during training. 
This happens because these rare instances are simply not sampled.
The prediction then fails because the `Learner` does not know how to handle unseen factor levels.

```{r, error = TRUE, warning = FALSE}
logreg = lrn("classif.log_reg")
logreg$train(task$clone()$filter(1:30))
logreg$predict(task)
```

Not only logistic regression but also many other `Learner`s cannot handle new levels during prediction.
Thus, we use the `"fixfactors"` `PipeOp` to prevent that.
`"fixfactors"` introduces `NA` values.
This means that we may need to impute afterwards.
To solve this issue we use `"imputesample"`, but with `affect_cols` set to only *factorial* features.

```{r, error = TRUE, warning = FALSE}
logreg = lrn("classif.log_reg")
logreg$train(task$clone()$filter(1:2))
```

We observet that columns that are all-constant may also be a problem.
This can be fixed using `"removeconstants"`
We get the following robustification pipeline:

```{r}
robustify = po("fixfactors") %>>%
  po("removeconstants") %>>%
  po("imputesample", affect_columns = selector_type(c("ordered", "factor")))

robustify$plot()
```

This robust learner works even in very pathological conditions.
You may need to combine it with imputation if the data may have missing values.

```{r, warning = FALSE}
roblogreg = GraphLearner$new(robustify %>>% logreg)

roblogreg$train(task$clone()$filter(1:2))
roblogreg$predict(task)
```

## Encoding Categorical Features

There are many more technical issues that may result in errors when training (or predicting) which we have not presented yet.
Most of these, however, can be easily handled by `PipeOp`s.
For example, some `Learner`s, even frequently used ones like `xgboost`, cannot deal with categorical features.

```{r, error = TRUE}
xgb = lrn("classif.xgboost")
xgb$train(task)
```

To overcome this problem we can use, for instance, `po("encode")`, or `po("encodeimpact")` to perform factor encoding.
`"encode"` does one-hot encoding (or similar).
`"encodeimpact"` does impact-encoding, which may work better for features with many factor levels.

```{r}
xgb_all = GraphLearner$new(po("encode") %>>% xgb)

xgb_all$train(task) # runs without error
```

## Your Ideas

There are various possibilities for preprocessing with `PipeOp`s.
You can try different methods for preprocessing and training.
Feel free to discover this variety by yourself!
There are only a few hints that help when working with `PipeOp`s.

It is not allowed to have two `PipeOp`s with the same `ID` in a `Graph`. 
Initialize a `PipeOp` with `po("...", id = "xyz")` to change its ID on construction.

If you build large `Graph`s involving complicated optimizations, like many `"learner_cv"`, they may need a long time to train.

Use the `affect_columns` parameter if you want a `PipeOp` to only operate on part of the data. 
Use `po("select")` if you want to remove certain columns (possibly only along a single branch of multiple parallel branches). Both take `selector_XXX()` arguments, e.g. `selector_type("integer")`

You may get the best performance if you actually inspect the features and see what kind of transformations work best for them (know your data!).

See what `PipeOp`s are available by inspecting `mlr_pipeops$keys()`, and get help about them using `?mlr_pipeops_XXX`.
