---
title: mlr3pipelines Tutorial - German Credit
categories:
  - mlr3pipelines
  - imputation
  - filtering
  - stacking
  - german credit
author:
  - name: Martin Binder
  - name: Florian Pfisterer
date: 03-11-2020
description: |
  In this use case, we continue working with the German credit dataset. We already used different Learners on it in previous posts and tried to optimize their hyperparameters. To make things interesting, we artificially introduce missing values into the dataset, perform imputation and filtering and stack Learners.
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
---


```{r, include=FALSE}
# Just some preparation
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 80)
)
library(mlr3book)
if (require("data.table")) data.table::setDTthreads(1)
set.seed(20191101)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
```

## Outline

This is the third part of a serial of use cases with the German credit dataset.
The other parts of this series can be found here:

- [Part I - Basics](https://mlr3gallery.mlr-org.com/posts/2020-03-11-basics-german-credit/)
- [Part II - Tuning](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3tuning-tutorial-german-credit/)

In this tutorial, we continue working with the German credit dataset.
We already used different `Learner`s on it and tried to optimize their hyperparameters.
Now we will do four additional things:

1) We preprocess the data as an integrated step of the model fitting process
2) We tune the associated preprocessing parameters
3) We stack multiple `Learner`s in an *ensemble* model
4) We discuss some techniques that make `Learner`s able to tackle *challenging* datasets that they could not handle otherwise (we are going to outline what challenging means in particular later on)

## Prerequisites

```{r, message=FALSE, warning=FALSE}
library("data.table")
library("ggplot2")
library("mlr3")
library("mlr3learners")
library("mlr3filters")
library("mlr3pipelines")
library("mlr3tuning")
library("paradox")
```

```{r, include=FALSE}
theme_set(theme_light())
```

We again use the German credit dataset, but will restrict ourselves to the *factorial features*.
To make things interesting or to make it a bit harder for our `r ref("Learner", "Learners")`, we introduce *missing values* in the dataset:

```{r, message=FALSE}
task = tsk("german_credit")
credit_full = task$data()
credit = credit_full[, sapply(credit_full, FUN = is.factor), with = FALSE]

set.seed(20191101)
# sample values to NA
credit = credit[, lapply(.SD, function(x) {
  x[sample(c(TRUE, NA), length(x), replace = TRUE, prob = c(.9, .1))]
})]
credit$credit_risk = credit_full$credit_risk
task = TaskClassif$new("GermanCredit", credit, "credit_risk")
```

We instantiate a `Resampling` instance for this `Task` to be able to compare resampling performance:

```{r}
set.seed(20191101)
cv10_instance = rsmp("cv")$instantiate(task)
```

You can uncomment the following line if you are running this locally:

```{r, warning=FALSE}
# future::plan("multiprocess") # uncomment for parallelization
```

## Intro

In this use case, we will take a look at composite machine learning algorithms that may incorporate data preprocessing or the combination of multiple `Learner`s ("ensemble methods").

We use the `r mlr_pkg("mlr3pipelines")` package that enables us to chain `r ref("PipeOp", "PipeOps")` into data flow `r ref("Graph", "Graphs")`.

Available `PipeOp`s are listed in the `mlr_pipeops` dictionary:

```{r}
mlr_pipeops
```

## Missing Value Imputation

We have just introduced missing values into our data.
While some `Learner`s can deal with missing value, many cannot.
Trying to train a random forest fails because of this:

```{r, error=TRUE}
ranger = lrn("classif.ranger")
ranger$train(task)
```

We can perform imputation of missing values using a `PipeOp`.
To find out which imputation `PipeOp`s are available, we do the following:

```{r}
mlr_pipeops$keys("^impute")
```

We choose to impute factorial features using a new level (via `r ref("PipeOpImputeOOR")`).
Let's use the `PipeOp` itself to create an imputed `Task`.
This shows us how the `PipeOp` actually works:

```{r}
imputer = po("imputeoor")
task_imputed = imputer$train(list(task))[[1]]
task_imputed$missings()
head(task_imputed$data())
```

We do not only need complete data during training but also during prediction.
Using the same imputation heuristic for both is the most consistent strategy.
This way the imputation strategy can, in fact, be seen as a part of the complete learner (which could be tuned).

If we used the imputed `Task` for Resampling, we would *leak* information from the test set into the training set.
Therefore, it is mandatory to attach the imputation operator to the `Learner` itself, creating a `r ref("GraphLearner")`:

```{r}
imp_ranger = GraphLearner$new(po("imputeoor") %>>% ranger)

imp_ranger$train(task)
```

This `GraphLearner` can be used for resampling -- like an ordinary `Learner`:

```{r}
rr = resample(task, learner = imp_ranger, resampling = cv10_instance)
rr$aggregate()
```

## Feature Filtering

Typically, sparse models, i.e. having models with few(er) features, are desirable.
This is due to a variety of reasons, e.g., enhanced interpretability or decreased costs of acquiring data.
Furthermore, sparse models may actually be associated with increased performance (especially if overfitting is anticipated).
We can use *feature filter* to only keep features with the highest *information*. Filters are implemented in the `r mlr_pkg("mlr3filters")` package and listed in the following dictionary:

```{r}
mlr_filters
```

We apply the `r ref("FilterMIM")` (mutual information maximization) `Filter` as implemented in the `r cran_pkg("praznik")` package. This `Filter` allows for the selection of the top-`k` features of best mutual information.

```{r}
filter = flt("mim")
filter$calculate(task_imputed)$scores
```

Making use of this `Filter`, you may wonder at which costs the reduction of the feature space comes.
We can investigate the trade-off between features and performance by tuning.
We incorporate our filtering strategy into the pipeline using `r ref("PipeOpFilter")`.
Like before, we need to perform imputation as the `Filter` also relies on complete data:

```{r}
fpipe = po("imputeoor") %>>% po("filter", flt("mim"), filter.nfeat = 3)
fpipe$train(task)[[1]]$head()
```

We can now tune over the `mim.filter.nfeat` parameter.
It steers how many features are kept by the `Filter` and eventually used by the learner:

```{r}
searchspace = ParamSet$new(list(
  ParamInt$new("mim.filter.nfeat", lower = 1, upper = length(task$feature_names))
))
```

The problem is one-dimensional (i.e. only one parameter is tuned).
Thus, we make use of a grid search.
For higher dimensions, strategies like random search are more appropriate:

```{r}
inst = TuningInstanceSingleCrit$new(
  task, fpipe %>>% lrn("classif.ranger"), cv10_instance, msr("classif.ce"),
  searchspace, trm("none")
)
tuner = tnr("grid_search")
```

The tuning procedure may take some time:

```{r, warning=FALSE}
tuner$optimize(inst)
```

We can plot the performance against the number of features.
If we do so, we see the possible trade-off between sparsity and predictive performance:

```{r}
arx = inst$archive$data()
ggplot(arx, aes(x = mim.filter.nfeat, y = classif.ce)) + geom_line()
```

## Stacking

We want to build a model that is based on the predictions of other `Learner`s.
This means that we are in the state that we need predictions already during training.
This is a very specific case that is luckily handled by `r ref("PipeOpLearnerCV")`.
`PipeOpLearnerCV` performs cross-validation during the training phase and returns the cross-validated predictions.
We use `"prob"` predictions because they carry more information than response prediction:

```{r}
stackgraph = po("imputeoor") %>>%
  gunion(list(
    po("learner_cv", lrn("classif.ranger", predict_type = "prob")),
    po("learner_cv", lrn("classif.kknn", predict_type = "prob")))) %>>%
  po("featureunion") %>>% lrn("classif.log_reg")
```

We built a pretty complex `r ref("Graph")` already.
Therefore, we plot it:

```{r}
stackgraph$plot()
```

We now compare the performance of the stacked learner to the performance of the individual `Learner`s:

```{r, warning=FALSE}
bmr = benchmark(data.table(
  task = list(task),
  learner = list(
    stackgraph,
    GraphLearner$new(po("imputeoor") %>>% lrn("classif.ranger")),
    GraphLearner$new(po("imputeoor") %>>% lrn("classif.kknn")),
    GraphLearner$new(po("imputeoor") %>>% lrn("classif.log_reg"))),
  resampling = list(cv10_instance)))
bmr$aggregate()[, c("learner_id", "classif.ce")]
```

If we train the stacked learner and look into the final `Learner` (the logistic regression), we can see how "important" each `Learner` of the stacked learner is:

```{r}
stackgraph$train(task)

summary(stackgraph$pipeops$classif.log_reg$state$model)
```

The random forest has a higher contribution.

## Robustify: Preventing new Prediction Factor Levels and other Problems

We now shift the context, using the complete German credit dataset:

```{r, message=FALSE}
task = tsk("german_credit")
```

There is a potential practical problem for both, small data sets and data sets with covariates having many factor levels:
It may occur that not all possible factor levels have been used by the `Learner` during training.
This happens because these rare instances are simply not sampled.
The prediction then may fail because the `Learner` does not know how to handle unseen factor levels:

```{r, error=TRUE, warning=FALSE}
task_unseen = task$clone()$filter(1:30)
logreg = lrn("classif.log_reg")
logreg$train(task_unseen)
logreg$predict(task)
```

Not only logistic regression but also many other `Learner`s cannot handle new levels during prediction.
Thus, we use `r ref("PipeOpFixFactors")` to prevent that.
`PipeOpFixFactors` introduces `NA` values for unseen levels.
This means that we may need to impute afterwards.
To solve this issue we can use `r ref("PipeOpImputeSample")`, but with `affect_columns` set to only *factorial* features.

Another observation is that all-constant features may also be a problem:

```{r, error=TRUE, warning=FALSE}
task_constant = task$clone()$filter(1:2)
logreg = lrn("classif.log_reg")
logreg$train(task_constant)
```

This can be fixed using `r ref("PipeOpRemoveConstants")`.

Both, handling unseen levels and all-constant features can be handled simultaneously using the following `Graph`:

```{r}
robustify = po("fixfactors") %>>%
  po("removeconstants") %>>%
  po("imputesample", affect_columns = selector_type(c("ordered", "factor")))

robustify$plot()
```

This robust learner works even in very pathological conditions:

```{r, warning=FALSE}
roblogreg = GraphLearner$new(robustify %>>% logreg)

roblogreg$train(task_constant)
roblogreg$predict(task)
```

## Your Ideas

There are various possibilities for preprocessing with `PipeOp`s.
You can try different methods for preprocessing and training.
Feel free to discover this variety by yourself!
Here are only a few hints that help when working with `PipeOp`s:

- It is not allowed to have two `PipeOp`s with the same `ID` in a `Graph`
  - Initialize a `PipeOp` with `po("...", id = "xyz")` to change its ID on construction
- If you build large `Graph`s involving complicated optimizations, like many `"learner_cv"`, they may need a long time to train
- Use the `affect_columns` parameter if you want a `PipeOp` to only operate on part of the data
- Use `po("select")` if you want to remove certain columns (possibly only along a single branch of multiple parallel branches). Both take `selector_xxx()` arguments, e.g. `selector_type("integer")`
- You may get the best performance if you actually inspect the features and see what kind of transformations work best for them (know your data!)
- See what `PipeOp`s are available by inspecting `mlr_pipeops$keys()`, and get help about them using `?mlr_pipeops_xxx`
