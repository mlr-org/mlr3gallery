---
title: "mlr3fselect-basic"
categories:
  - mlr3fselect
  - nested resampling
  - feature importance
  - optimization
description: |
  Short introduction to mlr3fselect
author:
  - name: Marc Becker
date: 09-14-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (require("data.table")) data.table::setDTthreads(1)
set.seed(7832)
lgr::get_logger("mlr3")$set_threshold("warn")
```

## Introduction

In this tutorial, we introduce the **mlr3fselect** package by comparing feature selection methods on the titanic disaster data set. 
The objective of feature selection is to enhance the interpretability of models, speed up the learning process and increase the predictive performance.

## Titanic data set

The [Titanic data set](https://www.kaggle.com/c/titanic/data) contains data for 887 Titanic passengers, including whether they survived when the titanic sank. 
Our goal will be to predict the survival of the titanic passengers. 

After loading the data set from the `mlr3data` package, we impute the missing age values with the median age of the passengers, set missing embarked values to `s` and remove `character` features.
We could use feature engineering to create new features from the `character` features, however we want to focus on feature selection in this tutorial.

In addition to the `survived` column, the reduced data set contains for each passenger the following attributes:

* `age` Age 
* `sex` Sex 
* `sib_sp` Number of siblings/ spouses aboard
* `parch` Number of parents/ children aboard
* `fare` Fair paid 
* `pc_class` Passenger class
* `embarked` Port of embarkation

```{r data}
library(mlr3data)

data("titanic", package = "mlr3data")
titanic$age[is.na(titanic$age)] = median(titanic$age, na.rm = TRUE)
titanic$embarked[is.na(titanic$embarked)] = "S"
titanic$ticket = NULL
titanic$name = NULL
titanic$cabin = NULL
titanic = titanic[!is.na(titanic$survived),]
```

We construct a binary classification task.

```{r task}
library(mlr3)

task = TaskClassif$new(id = "titanic", backend = titanic, target = "survived", positive = "yes")
```

## Model

We use the logistic regression learner provided by the `mlr3learners` package.

```{r learner}
library(mlr3learners)

learner = lrn("classif.log_reg")
```

To evaluate the predictive performance, we choose a `3-fold-cross-validation` and the classification error as the measure.

```{r resample and measure}
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")

resampling$instantiate(task)
```

## Classes

The `FSelectInstanceSingleCrit` class specifies a general feature selection scenario. 
It includes the  `ObjectiveFSelect` object that encodes the black box objective function which is optimized by a feature selection algorithm. 
The evaluated feature sets are stored in an `ArchiveFSelect` object which provides a method for  querying the best performing feature set.

The `Terminator` subclasses determine when to stop the feature selection. 
In this example we choose a terminator that stops the feature selection after 10 seconds. 
The sugar functions `trm()` and `trms()` can be used to retrieve terminators from the `mlr_terminators` dictionary.

```{r terminator and instance}
library(mlr3fselect)

terminator = trm("run_time", secs = 10)
FSelectInstanceSingleCrit$new(
  task = task, 
  learner = learner, 
  resampling = resampling, 
  measure = measure, 
  terminator = terminator)
```

The `FSelector` subclasses describe the feature selection strategy. The sugar function `fs()` can be used to retrieve feature selection algorithms from the `mlr_fselectors` dictionary.

```{r dictionary}
mlr_fselectors
```
## Random search

Random search randomly draws feature sets and evaluates them in batches. 
We retrieve the `FSelectRandomSearch` class with the `fs()` sugar function and choose the `evals` terminator. 
We set the `n_evals` parameter to `10` which means that 10 feature sets are evaluated.

```{r random search}
set.seed(7832)

terminator = trm("evals", n_evals = 10)
instance = FSelectInstanceSingleCrit$new(
  task = task, 
  learner = learner, 
  resampling = resampling, 
  measure = measure, 
  terminator = terminator)
fselector = fs("random_search", batch_size = 5)
```

The feature selection is started by passing the `FSelectInstanceSingleCrit` object to the `$optimize()` method of `FSelectorRandomSearch` which generates the feature sets. 
These features set are internally passed to the `$eval_batch()` method of `FSelectInstanceSingleCrit` which evaluates them with the objective fuction and stores the results in the archive. 
This general interaction between the objects of **mlr3fselect** stays the same for the different feature selection methods.
However, the way how new feature sets are generated differs depending on the chosen `FSelector` subclass.

```{r random search optimize}
fselector$optimize(instance)
```

The `ArchiveFSelect` stores a `data.table` which conists of the evaluated feature sets and the corresponding estimated predictive performances. 

```{r random search archive}
instance$archive$data()
```
The associated resampling iterations can be accessed in the `BenchmarkResult` by calling

```{r}
instance$archive$benchmark_result
```

We retrieve the best performing feature set with 

```{r random search result}
instance$result
```
## Sequential forward selection 

We try sequential forward selection. We chose the `stagnation` terminator that stops the feature selection if the predictive performance does not increase anymore.

```{r sfs}
set.seed(7832)

terminator = trm("stagnation", iters = 5)
instance = FSelectInstanceSingleCrit$new(
  task = task, 
  learner = learner, 
  resampling = resampling, 
  measure = measure, 
  terminator = terminator)

fselector = fs("sequential")
fselector$optimize(instance)
```

## Nested resampling

The feature selection should not be performed on the same resampling sets which are used for evaluating the model itself since this would result in biased performance estimates.
Nested resampling uses an outer and inner resampling to separate the feature selection from the performance estimation of the model.

We choose `holdout` validation for the outer resampling loop to get a fixed test set. 
The outer training set is passed to the `AutoFSelect` objects to execute the feature selection. Then the learner is fitted on the outer training set using the selected feature sets and the performance is evaluated on the outer test set.

```{r outer resampling}
resampling_outer = rsmp("holdout")
```

We use  a `3-fold-cross-validation` to evaluate the predictive performance during the feature selection.

```{r inner resampling}
resampling_inner = rsmp("cv", folds = 5)
measure = msr("classif.ce")
terminator = trm("evals", n_evals = 60)
```

The `AutoFSelect` class inherits from the `Learner` class and starts an automatic feature selection when a task is passed to the `$train()` method.
 
```{r AutoFSelect}
fs_sequential = fs("sequential")
at_sequential = AutoFSelect$new(learner, resampling_inner, measure = measure, terminator, fselect = fs_sequential)
at_sequential$store_fselect_instance = TRUE

fs_random = fs("random_search")
at_random = AutoFSelect$new(learner, resampling_inner, measure = measure, terminator, fselect = fs_random)
at_random$store_fselect_instance = TRUE
```

We combine each `AutoFSelect` learner with the task and resampling to a benchmark grid design.

```{r design}
design = benchmark_grid(task, list(at_sequential, at_random), resampling_outer)
design
```

Now we construct a benchmark call to execute the feature selection and performance evaluation at once.

```{r benchmark}
set.seed(7832)
bmr = benchmark(design, store_models = TRUE)
```

We access the performance of the models estimated on the outer resampling test set.

```{r score}
bmr$score()
```

In order to access the corresponding feature sets, we use the `BenchmarkResult` object.

```{r feature set 1}
bmr$resample_result(1)$learners[[1]]$fselect_result
```

```{r feature set 2}
bmr$resample_result(2)$learners[[1]]$fselect_result
```
We may see a different result if we replace the `3-fold-cross-validation` with a cross validation with many repetitions since the used resampling instance introduces much randomness.

# Recursive feature elimination

Recrusive feature elemiation utilizes the `importance()` method of learners.
In each iteration the feature with the lowest importance score is droped. 
We choose the non-recruvie algorithm (`recursive = FALSE`) which calculates the feature importance once on the complete feature set. 
The recrusive version (`recursive = TRUE`) recomputes the feature importance on the reduced feature set in every iteration. 

```{r rfe}
set.seed(7832)

learner = lrn("classif.ranger", importance = "impurity")
terminator = trm("none")
instance = FSelectInstanceSingleCrit$new(
  task = task, 
  learner = learner, 
  resampling = resampling, 
  measure = measure, 
  terminator = terminator,
  store_models = TRUE)

fselector = fs("rfe", recursive = FALSE)
fselector$optimize(instance)
```

We access the results.

```{r rfe archive}
instance$archive$data()
```


