---
title: Practical Tuning Series - Build an Automated Machine Learning System
description: |
  We implement a simple automated machine learning (AutoML) system which includes preprocessing, a switch between multiple learners and hyperparameter tuning.
categories:
  - mlr3tuning
  - tuning
  - optimization
  - nested resampling
  - mlr3pipelines
  - automl
  - pima data set
  - classification
  - practical tuning series
author:
  - name: Marc Becker
  - name: Theresa Ullmann
  - name: Michel Lang
  - name: Bernd Bischl
  - name: Jakob Richter
  - name: Martin Binder
date: 03-11-2021
output:
  distill::distill_article:
    self_contained: false
params:
  eval_all: FALSE
---

```{r practical-tuning-series-build-an-automated-machine-learning-system-001, include=FALSE}
 knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 120)
)
library(mlr3book)
```

# Scope

This is the third part of the practical tuning series.
The other parts can be found here:

* [Part I - Tune a Support Vector Machine](https://mlr3gallery.mlr-org.com/posts/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/)
* [Part II - Tune a Preprocessing Pipeline](https://mlr3gallery.mlr-org.com/posts/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline/)
* [Part IV - Tuning and Parallel Processing](https://mlr3gallery.mlr-org.com/posts/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/)

In this post, we implement a simple automated machine learning (AutoML) system which includes preprocessing, a switch between multiple learners and hyperparameter tuning.
For this, we build a pipeline with the `r mlr_pkg("mlr3pipelines")` extension package.
Additionally, we use nested resampling to get an unbiased performance estimate of our AutoML system.

# Prerequisites

We load the `r mlr_pkg("mlr3verse")`  package which pulls in the most important packages for this example.

```{r practical-tuning-series-build-an-automated-machine-learning-system-002, message = FALSE}
library(mlr3verse)
```

We initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.
The [`lgr`](https://mlr3book.mlr-org.com/logging.html) package is used for logging in all `r mlr_pkg("mlr3")` packages.
The `r mlr_pkg("mlr3")` logger prints the logging messages from the base package, whereas the `r mlr_pkg("bbotk")`  logger is responsible for logging messages from the optimization packages (e.g. `r mlr_pkg("mlr3tuning")` ).

```{r practical-tuning-series-build-an-automated-machine-learning-system-003}
set.seed(7832)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
```

In this example, we use the [Pima Indians Diabetes data set](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) which is used to to predict whether or not a patient has diabetes.
The patients are characterized by 8 numeric features and some have missing values.

```{r practical-tuning-series-build-an-automated-machine-learning-system-004}
task = tsk("pima")
```

# Branching

We use three popular machine learning algorithms: k-nearest-neighbors, support vector machines and random forests.

```{r practical-tuning-series-build-an-automated-machine-learning-system-005}
learners = list(
  lrn("classif.kknn", id = "kknn"),
  lrn("classif.svm", id = "svm", type = "C-classification"),
  lrn("classif.ranger", id = "ranger")
)
```

The `r ref("PipeOpBranch")` allows us to specify multiple alternatives paths.
In this graph, the paths lead to the different learner models.
The `selection` hyperparameter controls which path is executed i.e., which learner is used to fit a model.
It is important to use the `r ref("PipeOpBranch")` after the branching so that the outputs are merged into one result object.
We visualize the graph with branching below.

```{r practical-tuning-series-build-an-automated-machine-learning-system-006}
graph =
  po("branch", options = c("kknn", "svm", "ranger")) %>>%
  gunion(lapply(learners, po)) %>>%
  po("unbranch")
graph$plot()
```

Alternatively, we can use the `r ref("ppl()")`-shortcut to load a predefined graph from the `r ref("mlr_graphs")` dictionary.
For this, the learner list must be named.

```{r practical-tuning-series-build-an-automated-machine-learning-system-007}
learners = list(
  kknn = lrn("classif.kknn", id = "kknn"),
  svm = lrn("classif.svm", id = "svm", type = "C-classification"),
  ranger = lrn("classif.ranger", id = "ranger")
)

graph = ppl("branch", lapply(learners, po))
```

# Preprocessing

The task has missing data in five columns.

```{r practical-tuning-series-build-an-automated-machine-learning-system-008}
round(task$missings() / task$nrow, 2)
```

The pipeline [`"robustify"`](https://mlr3pipelines.mlr-org.com/reference/mlr_graphs_robustify.html) function creates a preprocessing pipeline based on our task.
The resulting pipeline imputes missing values with `r ref("PipeOpImputeHist")` and creates a dummy column (`r ref("PipeOpMissInd")`) which indicates the imputed missing values.
Internally, this creates two paths and the results are combined with `r ref("PipeOpFeatureUnion")`.
In contrast to `r ref("PipeOpBranch")`, both paths are executed.
Additionally, `"robustify"` adds `r ref("PipeOpEncode")` to encode factor columns and `r ref("PipeOpRemoveConstants")` to remove features with a constant value.

```{r practical-tuning-series-build-an-automated-machine-learning-system-009}
graph = ppl("robustify", task = task, factors_to_numeric = TRUE) %>>%
  graph
plot(graph)
```

We could also create the preprocessing pipeline manually.

```{r practical-tuning-series-build-an-automated-machine-learning-system-010}
gunion(list(po("imputehist"), po("missind", affect_columns = selector_type(c("numeric", "integer"))))) %>>%
  po("featureunion") %>>%
  po("encode") %>>%
  po("removeconstants")
```

# Graph Learner

We create a `r ref("GraphLearner")` which encapsulates the pipeline and can be used like a learner.

```{r practical-tuning-series-build-an-automated-machine-learning-system-011}
graph_learner = GraphLearner$new(graph)
```

The parameter set of the graph learner includes all hyperparameters from all contained learners.
The hyperparameter ids are prefixed with the corresponding learner ids.
The first hyperparameter `branch.selection` controls which learner is used.

```{r practical-tuning-series-build-an-automated-machine-learning-system-012}
print(graph_learner$param_set)
```

# Tune the pipeline

We will only tune one hyperparameter for each learner in this example.
Additionally, we tune the branching parameter which selects one of the three learners.
We have to specify that a hyperparameter is only valid for a certain learner by using `depends = branch.selection == <learner_id>`.

```{r practical-tuning-series-build-an-automated-machine-learning-system-013}
# branch
graph_learner$param_set$values$branch.selection = to_tune(c("kknn", "svm", "ranger"))

# kknn
graph_learner$param_set$values$kknn.k = to_tune(p_int(3, 50, logscale = TRUE, depends = branch.selection == "kknn"))

# svm
graph_learner$param_set$values$svm.cost = to_tune(p_dbl(-1, 1, trafo = function(x) 10^x, depends = branch.selection == "svm"))

# ranger
graph_learner$param_set$values$ranger.mtry = to_tune(p_int(1, 8, depends = branch.selection == "ranger"))

# short learner id for printing
graph_learner$id = "graph_learner"
```

We define a tuning instance and select a random search which is stopped after 20 evaluated configurations.

```{r practical-tuning-series-build-an-automated-machine-learning-system-014, eval = params$eval_all}
instance = tune(
  method = "random_search",
  task = task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 20
)
```

```{r practical-tuning-series-build-an-automated-machine-learning-system-015, echo = FALSE, eval = params$eval_all}
saveRDS(instance, "data/instance_1.rda")
```

```{r practical-tuning-series-build-an-automated-machine-learning-system-016, echo = FALSE}
instance = readRDS("data/instance_1.rda")
```

The following shows a quick way to visualize the tuning results.

```{r practical-tuning-series-build-an-automated-machine-learning-system-017, fig.width=10}
autoplot(instance, type = "marginal", cols_x = c("x_domain_kknn.k","x_domain_svm.cost", "ranger.mtry"))
```

# Final Model

We add the optimized hyperparameters to the graph learner and train the learner on the full dataset.

```{r practical-tuning-series-build-an-automated-machine-learning-system-018}
learner = GraphLearner$new(graph)
learner$param_set$values = instance$result_learner_param_vals
learner$train(task)
```

The trained model can now be used to make predictions on new data.
A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result_y`) as the model's performance.
Instead we have to use nested resampling to get an unbiased performance estimate.

# Nested Resampling

We use nested resampling to get an unbiased estimate of the predictive performance of our graph learner.

```{r practical-tuning-series-build-an-automated-machine-learning-system-019, eval = params$eval_all}
graph_learner = GraphLearner$new(graph)
graph_learner$param_set$values$branch.selection = to_tune(c("kknn", "svm", "ranger"))
graph_learner$param_set$values$kknn.k = to_tune(p_int(3, 50, logscale = TRUE, depends = branch.selection == "kknn"))
graph_learner$param_set$values$svm.cost = to_tune(p_dbl(-1, 1, trafo = function(x) 10^x, depends = branch.selection == "svm"))
graph_learner$param_set$values$ranger.mtry = to_tune(p_int(1, 8, depends = branch.selection == "ranger"))
graph_learner$id = "graph_learner"

inner_resampling = rsmp("cv", folds = 3)
at = AutoTuner$new(
  learner = graph_learner,
  resampling = inner_resampling,
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 10),
  tuner = tnr("random_search")
)

outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)
```

```{r practical-tuning-series-build-an-automated-machine-learning-system-020, echo = FALSE, eval = params$eval_all}
saveRDS(rr, "data/rr_1.rda")
```


```{r practical-tuning-series-build-an-automated-machine-learning-system-021, echo = FALSE}
rr = readRDS("data/rr_1.rda")
```

We check the inner tuning results for stable hyperparameters.
This means that the selected hyperparameters should not vary too much.
We might observe unstable models in this example because the small data set and the low number of resampling iterations might introduce too much randomness.
Usually, we aim for the selection of stable hyperparameters for all outer training sets.

```{r practical-tuning-series-build-an-automated-machine-learning-system-022}
extract_inner_tuning_results(rr)
```

Next, we want to compare the predictive performances estimated on the outer resampling to the inner resampling.
Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.

```{r practical-tuning-series-build-an-automated-machine-learning-system-023}
rr$score()
```

The aggregated performance of all outer resampling iterations is essentially the unbiased performance of the graph learner with optimal hyperparameter found by random search.

```{r practical-tuning-series-build-an-automated-machine-learning-system-024}
rr$aggregate()
```

Applying nested resampling can be shortened by using the `r ref("tune_nested()")`-shortcut.

```{r practical-tuning-series-build-an-automated-machine-learning-system-025, eval = FALSE}
graph_learner = GraphLearner$new(graph)
graph_learner$param_set$values$branch.selection = to_tune(c("kknn", "svm", "ranger"))
graph_learner$param_set$values$kknn.k = to_tune(p_int(3, 50, logscale = TRUE, depends = branch.selection == "kknn"))
graph_learner$param_set$values$svm.cost = to_tune(p_dbl(-1, 1, trafo = function(x) 10^x, depends = branch.selection == "svm"))
graph_learner$param_set$values$ranger.mtry = to_tune(p_int(1, 8, depends = branch.selection == "ranger"))
graph_learner$id = "graph_learner"

rr = tune_nested(
  method = "random_search",
  task = task,
  learner = graph_learner,
  inner_resampling = rsmp ("cv", folds = 3),
  outer_resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 10,
)
```

# Resources

The [mlr3book](https://mlr3book.mlr-org.com/) includes chapters on [pipelines](https://mlr3book.mlr-org.com/pipelines.html) and [hyperparameter tuning](https://mlr3book.mlr-org.com/tuning.html).
The [mlr3cheatsheets](https://cheatsheets.mlr-org.com/) contain frequently used commands and workflows of mlr3.


