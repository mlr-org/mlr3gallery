---
title: Regression Chains
categories:
  - regression
  - mlr3pipelines
author:
  - name: Lennart Schneider
date: 04-18-2020
description: |
  We show how to use mlr3pipelines to do regression chains.
output:
  distill::distill_article:
    self_contained: false
    css: ../../custom.css
bibliography: biblio.bib
---

```{r regression-chains-001, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 120)
)
library(mlr3book)
lgr::get_logger("mlr3")$set_threshold("warn")
```

In this tutorial we demonstrate how to use `r mlr_pkg("mlr3pipelines")` to handle multi-target regression by arranging regression models as a chain, i.e., creating a linear sequence of regression models.

## Regression Chains

In a simple regression chain, regression models are arranged in a linear sequence.
Here, the first model will use the input to predict a single output and the second model will use the input and the prediction output of the first model to make its own prediction and so on.
For more details, see e.g. @spyromitros2016.

## Before you start

The following sections describe an approach towards working with `r ref("Task", "Tasks")` that have multiple targets.
E.g., in the example below, we have three target variables $y_{1}$ to $y_{3}$.
This type of `Task` can be created via the [mlr3multioutput](https://github.com/mlr-org/mlr3multioutput/) package (currently under development) in the future.
`mlr3multioutput` will also offer simple chaining approaches as pre-built pipelines (so called `r ref("ppl", "ppls")`).
The current goal of this post is to show how such modeling steps can be written as a relatively small amount of pipeline steps and how such steps can be put together.
Writing pipelines with such steps allows for great flexibility in modeling more complicated scenarios such as the ones described below.

## Data

In the following, we rely on some toy data.
We simulate 100 responses to three target variables, $y_{1}$, $y_{2}$, and $y_{3}$ following a multivariate normal distribution with a mean and covariance matrix of:

```{r regression-chains-002}
library(data.table)
library(mvtnorm)
set.seed(2409)
n = 100
(mean = c(y1 = 1, y2 = 2, y3 = 3))
(sigma = matrix(c(1, -0.5, 0.25, -0.5, 1, -0.25, 0.25, -0.25, 1),
  nrow = 3, ncol = 3, byrow = TRUE))
Y = rmvnorm(n, mean = mean, sigma = sigma)
```

The feature variables $x_{1}$, and $x_{2}$ are simulated as follows: $x_{1}$ is simply given by $y_{1}$ and an independent normally distributed error term and $x_{2}$ is given by $y_{2}$ and an independent normally distributed error term.

```{r regression-chains-003}
x1 = Y[, 1] + rnorm(n, sd = 0.1)
x2 = Y[, 2] + rnorm(n, sd = 0.1)
```

The final data is given as:

```{r regression-chains-004}
dat = as.data.table(cbind(Y, x1, x2))
str(dat)
```

This simulates a situation where we have multiple target variables that are correlated with each other, such that predicting them along with each other can improve the resulting prediction model.
As a real-world example for such a situation, consider e.g. hospital data, where time spent in the ICU (not known a priori) heavily influences the cost incurred by a patient's treatment.

## 3D Visualization of the Data

If you feel confident to already have a good feeling of the data, feel free to skip this section.
If not, you can use the `r cran_pkg("rgl")` package to play around with the following four 3D plots with either the feature variables or $y_{1}$ and $y_{2}$ on the x- and y-axis and the target variables on the respective z-axes:

```{r regression-chains-005, eval = FALSE}
library(rgl)
colfun = colorRampPalette(c("#161B1D", "#ADD8E6"))
```

```{r regression-chains-006, eval = FALSE}
setorder(dat, y1)
plot3d(dat$x1, dat$x2, dat$y1,
  xlab = "x1", ylab = "x2", zlab = "y1",
  type = "s", radius = 0.1, col = colfun(n))
```

```{r regression-chains-007, eval = FALSE}
setorder(dat, y2)
plot3d(dat$x1, dat$x2, dat$y2,
  xlab = "x1", ylab = "x2", zlab = "y2",
  type = "s", radius = 0.1, col = colfun(n))
```

```{r regression-chains-008, eval = FALSE}
setorder(dat, y3)
plot3d(dat$x1, dat$x2, dat$y3,
  xlab = "x1", ylab = "x2", zlab = "y3",
  type = "s", radius = 0.1, col = colfun(n))
```

```{r regression-chains-009, eval = FALSE}
setorder(dat, y3)
plot3d(dat$y1, dat$y2, dat$y3,
  xlab = "y1", ylab = "y2", zlab = "y3",
  type = "s", radius = 0.1, col = colfun(n))
```

## Building the Pipeline

In our regression chain, the first model will predict $y_{1}$.
Therefore, we initialize our `Task` with respect to this target:

```{r regression-chains-010}
library(mlr3)
library(mlr3learners)
library(mlr3pipelines)
task = TaskRegr$new("multiregression", backend = dat, target = "y1")
```

As `r ref("Learner", "Learners")` we will use simple `r ref("mlr_learners_regr.lm", "linear regression models")`.
Our pipeline building the regression chain then has to do the following:

* Use the input to predict $y_{1}$ within the first learner (i.e., $y_{1} \sim x_{1} + x_{2}$).
* Combine the input with the prediction of $y_{1}$, $\hat{y_{1}}$ and use this to predict $y_{2}$ within the second learner (i.e., $y_{2} \sim x_{1} + x_{2} + \hat{y_{1}}$).
* Combine the input with the prediction of $y_{2}$ and use this to predict $y_{3}$ within the final third learner (i.e., $y_{3} \sim x_{1} + x_{2} + \hat{y_{1}} + \hat{y_{2}}$).

To combine predictions of a `Learner` with the previous input, we rely on `r ref("PipeOpLearnerCV")` and `r ref("PipeOpNOP")` arranged in parallel via `r ref("gunion")` combined via `r ref("PipeOpFeatureUnion")`.
To drop the respective remaining target variables as features, we rely on `r ref("PipeOpColRoles")`.
The first step of predicting $y_{1}$ looks like the following:

```{r regression-chains-011}
step1 = po("copy", outnum = 2, id = "copy1") %>>%
  gunion(list(
    po("colroles", id = "drop_y2_y3",
      param_vals = list(new_role = list(y2 = character(), y3 = character()))) %>>%
    po("learner_cv", learner = lrn("regr.lm"), id = "y1_learner"),
    po("nop", id = "nop1")
  )) %>>%
  po("featureunion", id = "union1")
step1$plot()
```

Training using the input `Task`, shows us how the output and the `$state` look like:

```{r regression-chains-012}
step1_out = step1$train(task)[[1]]
step1_out
step1$state
```

Within the second step we then have to define $y_{2}$ as the new target.
This can be done using `r ref("PipeOpUpdateTarget")` (note that `PipeOpUpdateTarget` currently is not exported but will be in a future version).
By default, `PipeOpUpdateTarget` drops the original target from the feature set, here $y_{1}$.

```{r regression-chains-013}
mlr_pipeops$add("update_target", mlr3pipelines:::PipeOpUpdateTarget)
```

```{r regression-chains-014}
step2 = po("update_target", id = "y2_target",
    param_vals = list(new_target_name = "y2")) %>>%
  po("copy", outnum = 2, id = "copy2") %>>%
  gunion(list(
    po("colroles", id = "drop_y3",
      param_vals = list(new_role = list(y3 = character()))) %>>%
    po("learner_cv", learner = lrn("regr.lm"), id = "y2_learner"),
    po("nop", id = "nop2")
  )) %>>%
  po("featureunion", id = "union2")
```

Again, we can train to see how the output and `$state` look like, but now using the output of `step1` as the input:

```{r regression-chains-015}
step2_out = step2$train(step1_out)[[1]]
step2_out
step2$state
```

In the final third step we define $y_{3}$ as the new target (again, `PipeOpUpdateTarget` drops the previous original target from the feature set, here $y_{2}$):

```{r regression-chains-016}
step3 = po("update_target", id = "y3_target",
    param_vals = list(new_target_name = "y3")) %>>%
  po("learner", learner = lrn("regr.lm"), id = "y3_learner")
```

Using the output of `step2` as input:

```{r regression-chains-017}
step3_out = step3$train(step2_out)[[1]]
step3_out
step3$state
```

The complete pipeline, more precisely `r ref("Graph")`, looks like the following:

```{r regression-chains-018}
graph = step1 %>>% step2 %>>% step3
graph$plot()
```

## Evaluating the Pipeline

By wrapping our `Graph` in a `r ref("GraphLearner")`, we can perform `r ref("ResamplingCV", "3-fold cross-validation")` and get an estimated average of the root-mean-square error (of course, in a real world setting splitting the data in a training and test set should have been done):

```{r regression-chains-019}
learner = GraphLearner$new(graph)
rr = resample(task, learner = learner, resampling = rsmp("cv", folds = 3))
rr$aggregate(msr("regr.mse"))
```

## Predicting with the Pipeline

For completeness, we also show how a prediction step without having any target variable data available would look like:

```{r regression-chains-020}
dat_predict = as.data.table(cbind(x1, x2, y1 = NA, y2 = NA, y3 = NA))
learner$train(task)
learner$predict_newdata(dat_predict)
```

Note that we have to initialize the `Task` with $y_{1}$ as the target but the pipeline will automatically predict $y_{3}$ in the final step as our final target, which was our ultimate goal here.
