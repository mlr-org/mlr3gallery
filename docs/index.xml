<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mlr3 gallery</title>
    <link>/</link>
    <description>Recent content on mlr3 gallery</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A pipeline for the titanic data set</title>
      <link>/basics_pipelines_titanic/</link>
      <pubDate>Thu, 12 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/basics_pipelines_titanic/</guid>
      <description>Intro First of all we are going to load required packages and the data. The data is part of the mlr3data package.
library(&amp;quot;mlr3&amp;quot;) library(&amp;quot;mlr3learners&amp;quot;) library(&amp;quot;mlr3pipelines&amp;quot;) library(&amp;quot;mlr3data&amp;quot;) library(&amp;quot;mlr3misc&amp;quot;) data(&amp;quot;titanic&amp;quot;) The titanic data is very interesting to analyze, even though it is part of many tutorials and showcases. This is because it requires many steps often required in real-world applications of machine learning techniques, such as feature engineering, missing value imputation, handling factors and others.</description>
    </item>
    
    <item>
      <title>mlr3 basics - german credit</title>
      <link>/basics_german_credit/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/basics_german_credit/</guid>
      <description>Intro This is the first part in a serial of tutorials. The other parts of this series can be found here: * Part II * Part III
mlr3 is a machine learning framework for R. Together with other packages from the same developers, mostly following the naming scheme “mlr3___“, it offers functionality around developing, tuning, and evaluating machine learning workflows.
We will walk through this tutorial interactively. The text is kept short to be followed in real time.</description>
    </item>
    
    <item>
      <title>mlr3pipelines tutorial - german credit</title>
      <link>/basics_pipelines_german_credit/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/basics_pipelines_german_credit/</guid>
      <description>Intro This is the second part in a serial of tutorials. The other parts of this series can be found here: * Part I * Part III
In this tutorial we will continue working with the German Credit Dataset. We already used different Learners on it and tried to optimize their hyperparameters. Now we will
 preprocess the data as an integrated step of the model fitting process tune the preprocessing parameters use multiple Learners in an ensemble model see some techniques that make Learners able to tackle challenging datasets that they could not handle otherwise.</description>
    </item>
    
    <item>
      <title>mlr3tuning tutorial - german credit</title>
      <link>/basics_tuning_german_credit/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/basics_tuning_german_credit/</guid>
      <description>Intro This is the third part in a serial of tutorials. The other parts of this series can be found here: * Part I * Part II
In this case we will continue working with the German Credit Dataset. Yesterday we peeked into the data set by using and comparing some learners with ther default parameters. We will now see how to:
 Tune hyperparameters for a given problem Perform nested resampling Adjust decision thresholds   Prerequisites We expect you have installed all packages from day 1.</description>
    </item>
    
    <item>
      <title>Select uncorrelated features</title>
      <link>/select_uncorrelated_features/</link>
      <pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/select_uncorrelated_features/</guid>
      <description>The following example describes a situation where we aim to remove correlated features. This in essence means, that we drop features until no features have a correlation higher then a given cutoff. This is often useful when we for example want to use linear models.
Prerequisites This tutorial assumes familiarity with the basics of mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. Additionally, we compare different cutoff values via tuning using the mlr3tuning package.</description>
    </item>
    
    <item>
      <title>Feature Engineering of Date-Time Variables</title>
      <link>/date-features/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/date-features/</guid>
      <description>In this tutorial, we demonstrate how mlr3pipelines can be used to easily engineer features based on date-time variables. Relying on the Bike Sharing Dataset and the ranger learner we compare the RMSE of a random forest using the original features (baseline), to the RMSE of a random forest using newly engineered features on top of the original ones.
Motivation A single Date-Time variable (i.e., a POSIXct column) contains plenty of information ranging from year, month, day, hour, minute and second to other features such as week of the year, or day of the week.</description>
    </item>
    
    <item>
      <title>Tuning Over Multiple Learners (Tuning Multiplexer)</title>
      <link>/tuning-over-multiple-learners/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/tuning-over-multiple-learners/</guid>
      <description>This use case shows how to tune over multiple learners for a single task. Following tasks are illustrated:
 Build a pipeline that can switch between multiple learners. Define the hyperparameter search space for the pipeline. Define transformations for single hyperparameters. Define a hierarchical order of the hyperparameters. Run a random search.  Build the Pipeline The pipeline just has a single purpose in this example: It should allow us to switch between different learners.</description>
    </item>
    
    <item>
      <title>Encode factor levels for xgboost</title>
      <link>/encode-factors-for-xgboost/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/encode-factors-for-xgboost/</guid>
      <description>The package xgboost unfortunately does not support handling of categorical features. Therefore it is required to manually convert factor columns to numerical dummy features. We show how to use mlr3pipelines to augment the xgboost learner with an automatic factor encoding.
Construct the Base Objects First, we take an example task with factors (german_credit) and create the xgboost learner:
library(mlr3) library(mlr3learners) task = tsk(&amp;quot;german_credit&amp;quot;) print(task) ## &amp;lt;TaskClassif:german_credit&amp;gt; (1000 x 21) ## * Target: credit_risk ## * Properties: twoclass ## * Features (20): ## - fct (12): credit_history, foreign_worker, housing, job, ## other_debtors, other_installment_plans, personal_status_sex, ## property, purpose, savings, status, telephone ## - dbl (7): age, amount, duration, installment_rate, number_credits, ## people_liable, present_residence ## - ord (1): employment_duration learner = lrn(&amp;quot;classif.</description>
    </item>
    
    <item>
      <title>Impute missing variables</title>
      <link>/impute-missing-variables/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/impute-missing-variables/</guid>
      <description>Prerequisites This tutorial assumes familiarity with the basics of mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. It deals with the problem of missing data.
The random forest implementation in the package ranger unfortunately does not support missing values. Therefore it is required to impute missing features before passing the data to the learner.
We show how to use mlr3pipelines to augment the ranger learner with automatic imputation.</description>
    </item>
    
    <item>
      <title>House prices in King County</title>
      <link>/house-prices-in-king-county/</link>
      <pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/house-prices-in-king-county/</guid>
      <description>The use-case illustrated below touches on the following concepts:
 Data preprocessing Task Fitting a learner Resampling Tuning  The relevant sections in the mlr3book are linked to for the reader’s convenience.
This use case shows how to model housing price data in King County. Following features are illustrated:
 Summarizing the data set Converting data to treat it as a numeric feature/factor Generating new variables Splitting data into train and test data sets Computing a first model (decision tree) Building many trees (random forest) Visualizing price data across different region Optimizing the baseline by implementing a tuner Engineering features Creating a sparser model  House Price Prediction in King County We use the kc_housing dataset contained in the package mlr3book in order to provide a use-case for the application of mlr3 on real-world data.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>This gallery provides examples and case studies for mlr3.
If you want to contribute, just create a pull request on GitHub with a blogdown article.</description>
    </item>
    
  </channel>
</rss>