<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.67.0" />


<title>mlr3tuning tutorial - german credit - mlr3 gallery</title>
<meta property="og:title" content="mlr3tuning tutorial - german credit - mlr3 gallery">


  <link href='/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/gruvbox-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">

<link rel="stylesheet" href="/css/custom.css">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="mlr logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/mlr-org/mlr3gallery">GitHub</a></li>
    
    <li><a href="https://mlr3.mlr-org.com">mlr3</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">14 min read</span>
    

    <h1 class="article-title">mlr3tuning tutorial - german credit</h1>

    
    <span class="article-date">2020-03-11 by Martin Binder &amp; Florian Pfisterer</span>
    

    <div class="article-content">
      


<div id="intro" class="section level2">
<h2>Intro</h2>
<p>This is the third part in a serial of tutorials. The other parts of this series can be found here: * <a href="/basics_german_credit/">Part I</a> * <a href="/basics_pipelines_german_credit/">Part II</a></p>
<p>In this case we will continue working with the <strong>German Credit Dataset</strong>. Yesterday we peeked into the data set by using and comparing some learners with ther default parameters. We will now see how to:</p>
<ul>
<li>Tune hyperparameters for a given problem</li>
<li>Perform nested resampling</li>
<li>Adjust decision thresholds</li>
</ul>
</div>
<div id="prerequisites" class="section level2">
<h2>Prerequisites</h2>
<p>We expect you have installed all packages from day 1. If not, load the day 1 script and run the <strong>Prerequisites</strong> install chunk.</p>
<p>Load the packages we are going to use:</p>
<pre class="r"><code>library(&quot;data.table&quot;)
library(&quot;mlr3&quot;)
library(&quot;mlr3learners&quot;)
library(&quot;ggplot2&quot;)
theme_set(theme_light())</code></pre>
<p>We use the same data as in the earlier blog posts.</p>
<pre class="r"><code>task = tsk(&quot;german_credit&quot;)</code></pre>
<p>Also, because tuning often takes a long time, we want to make more efficient use of our multicore CPUs. Don’t do this on rstudio cloud, however.</p>
<pre class="r"><code># future::plan(&quot;multiprocess&quot;)</code></pre>
<div id="evaluation" class="section level3">
<h3>Evaluation</h3>
<p>We evaluate all algorithms using 10-fold cross-validation. We use a <em>fixed</em> train-test split, i.e. the same splits for each evaluation. Otherwise, some evaluation could get unusually “hard” splits, which would make comparisons unfair.</p>
<pre class="r"><code>set.seed(8008135)
cv10_instance = rsmp(&quot;cv&quot;, folds = 10)
## fix the train-test splits using the $instantiate() method
cv10_instance$instantiate(task)
## have a look at the test set instances per fold
cv10_instance$instance
#&gt;       row_id fold
#&gt;    1:      5    1
#&gt;    2:     20    1
#&gt;    3:     28    1
#&gt;    4:     35    1
#&gt;    5:     37    1
#&gt;   ---            
#&gt;  996:    936   10
#&gt;  997:    950   10
#&gt;  998:    963   10
#&gt;  999:    985   10
#&gt; 1000:    994   10</code></pre>
</div>
</div>
<div id="simple-parameter-tuning" class="section level2">
<h2>Simple Parameter Tuning</h2>
<ul>
<li>Use the <code>paradox</code> package for search space definition of the hyperparameters</li>
<li>Use the <code>mlr3tuning</code> package for tuning the hyperparameters</li>
</ul>
<pre class="r"><code>library(&quot;mlr3tuning&quot;)
library(&quot;paradox&quot;)</code></pre>
<div id="search-space-and-problem-definition" class="section level3">
<h3>Search Space and Problem Definition</h3>
<ul>
<li>First need to decide what <code>Learner</code> to optimize. We will use <code>&quot;classif.kknn&quot;</code>, the “kernelized” k-nearest neighbor classifier. <!-- - (We could also try to optimize multiple `Learner`s simultaneously, i.e. choose the `Learner` to use automatically. Look into mlr3pipelines if you want to do that. --></li>
<li>We will use <code>kknn</code> as a normal kNN without weighting first (i.e., use the <code>rectangular</code> kernel):</li>
</ul>
<pre class="r"><code>knn = lrn(&quot;classif.kknn&quot;, predict_type = &quot;prob&quot;)
## use rectangular kernel for normal kNN
knn$param_set$values$kernel = &quot;rectangular&quot;</code></pre>
<ul>
<li>Then we decide what parameters we optimize over. What are our options?</li>
</ul>
<pre class="r"><code>knn$param_set
#&gt; ParamSet: 
#&gt;          id    class lower upper                                                         levels default
#&gt; 1:        k ParamInt     1   Inf                                                                      7
#&gt; 2: distance ParamDbl     0   Inf                                                                      2
#&gt; 3:   kernel ParamFct    NA    NA rectangular,triangular,epanechnikov,biweight,triweight,cos,... optimal
#&gt; 4:    scale ParamLgl    NA    NA                                                     TRUE,FALSE    TRUE
#&gt;          value
#&gt; 1:            
#&gt; 2:            
#&gt; 3: rectangular
#&gt; 4:</code></pre>
<ul>
<li>We first tune the <code>k</code> parameter from 3 to 20 and the <code>distance</code> function (either L1 or L2). To do so, we use the <code>paradox</code> package to define a search space (see <a href="#very-quick-paradox-primer">Appendix</a> for a short list of possible parameter types).</li>
</ul>
<pre class="r"><code>searchspace = ParamSet$new(list(
  ParamInt$new(&quot;k&quot;, lower = 3, upper = 20),
  ParamInt$new(&quot;distance&quot;, lower = 1, upper = 2)
))</code></pre>
<ul>
<li>We define a “tuning instance” that represents the problem we are trying to optimize.</li>
<li>What is the task we are optimizing for?</li>
<li>What learner are we using?</li>
<li>How do we do resampling?</li>
<li>What is the performance measure?</li>
<li>What is the search space (“parameter set”)?</li>
<li>When are we done searching? (Disregard this for now).</li>
</ul>
<pre class="r"><code>instance_grid = TuningInstance$new(
  task = task,
  learner = knn,
  resampling = cv10_instance,
  measures = msr(&quot;classif.ce&quot;),
  param_set = searchspace,
  terminator = term(&quot;none&quot;)
)</code></pre>
</div>
<div id="grid-search" class="section level3">
<h3>Grid Search</h3>
<ul>
<li>A simple tuning method is to try all possible combinations of parameters: <strong>Grid Search</strong></li>
<li>Pro: Very intuitive and simple</li>
<li>Con: Inefficient if the search space is large</li>
<li>We get the <code>&quot;grid_search&quot;</code> tuner for this</li>
</ul>
<pre class="r"><code>set.seed(1)
tuner_grid = tnr(&quot;grid_search&quot;, resolution = 18, batch_size = 36)</code></pre>
<ul>
<li>Tuning works by calling <code>$tune()</code>. Note that it <em>modifies</em> our “tuning instance”–the result can be found in the <code>instance</code> object.</li>
</ul>
<pre class="r"><code>## empty instance object:
instance_grid$result
#&gt; $tune_x
#&gt; NULL
#&gt; 
#&gt; $params
#&gt; $params$kernel
#&gt; [1] &quot;rectangular&quot;
#&gt; 
#&gt; 
#&gt; $perf
#&gt; NULL
## now run tuning:
tuner_grid$tune(instance_grid)</code></pre>
<ul>
<li>The result can be found in the <code>$result</code> slot. We can also plot the performance.</li>
</ul>
<pre class="r"><code>instance_grid$result
#&gt; $tune_x
#&gt; $tune_x$k
#&gt; [1] 9
#&gt; 
#&gt; $tune_x$distance
#&gt; [1] 2
#&gt; 
#&gt; 
#&gt; $params
#&gt; $params$kernel
#&gt; [1] &quot;rectangular&quot;
#&gt; 
#&gt; $params$k
#&gt; [1] 9
#&gt; 
#&gt; $params$distance
#&gt; [1] 2
#&gt; 
#&gt; 
#&gt; $perf
#&gt; classif.ce 
#&gt;      0.249</code></pre>
<ul>
<li>We can look at the “archive” of evaluated configurations</li>
<li>We expand the “params” (the parameters that the <code>Learner</code> actually saw)</li>
</ul>
<pre class="r"><code>perfdata = instance_grid$archive(&quot;params&quot;)
perfdata[, c(&quot;nr&quot;, &quot;k&quot;, &quot;distance&quot;, &quot;classif.ce&quot;)]
#&gt;     nr  k distance classif.ce
#&gt;  1:  1  3        1      0.276
#&gt;  2:  2  3        2      0.267
#&gt;  3:  3  4        1      0.290
#&gt;  4:  4  4        2      0.286
#&gt;  5:  5  5        1      0.268
#&gt;  6:  6  5        2      0.274
#&gt;  7:  7  6        1      0.268
#&gt;  8:  8  6        2      0.274
#&gt;  9:  9  7        1      0.261
#&gt; 10: 10  7        2      0.262
#&gt; 11: 11  8        1      0.273
#&gt; 12: 12  8        2      0.260
#&gt; 13: 13  9        1      0.264
#&gt; 14: 14  9        2      0.249
#&gt; 15: 15 10        1      0.263
#&gt; 16: 16 10        2      0.259
#&gt; 17: 17 11        1      0.255
#&gt; 18: 18 11        2      0.252
#&gt; 19: 19 12        1      0.273
#&gt; 20: 20 12        2      0.252
#&gt; 21: 21 13        1      0.262
#&gt; 22: 22 13        2      0.255
#&gt; 23: 23 14        1      0.261
#&gt; 24: 24 14        2      0.253
#&gt; 25: 25 15        1      0.260
#&gt; 26: 26 15        2      0.252
#&gt; 27: 27 16        1      0.263
#&gt; 28: 28 16        2      0.258
#&gt; 29: 29 17        1      0.267
#&gt; 30: 30 17        2      0.266
#&gt; 31: 31 18        1      0.274
#&gt; 32: 32 18        2      0.268
#&gt; 33: 33 19        1      0.270
#&gt; 34: 34 19        2      0.269
#&gt; 35: 35 20        1      0.265
#&gt; 36: 36 20        2      0.270
#&gt;     nr  k distance classif.ce</code></pre>
<pre class="r"><code>ggplot(perfdata, aes(x = k, y = classif.ce, color = as.factor(distance))) +
  geom_line() + geom_point(size = 3)</code></pre>
<p><img src="/post/mlr3tuning_tutorial_german_credit_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<ul>
<li>Euclidean distance (<code>distance</code> = 2) seems to work better, but there is much randomness introduced by the resampling instance, so you may see a different result!</li>
<li><code>k</code> between 10 and 15 perform well</li>
</ul>
</div>
<div id="random-search-and-transformation" class="section level3">
<h3>Random Search and Transformation</h3>
<ul>
<li>Let’s look at a larger search space. In fact, how about we tune <em>all</em> available parameters and limit <code>k</code> to large values (50).</li>
<li><strong>Problem 1</strong>: The difference in performance between <code>k</code> = 3 and <code>k</code> = 4 is probably larger than the difference between <code>k</code> = 49 and <code>k</code> = 50.</li>
<li>We will use a <strong>transformation function</strong> for <code>k</code> and sample values on the log-space.</li>
<li>To do so, we define the range for <code>k</code> from <code>log(3)</code> to <code>log(50)</code> and exponentiate in the transformation.</li>
<li>For <code>k</code>, we must use <code>ParamDbl</code> instead of <code>ParamInt</code> now!</li>
</ul>
<pre class="r"><code>large_searchspace = ParamSet$new(list(
  ParamDbl$new(&quot;k&quot;, lower = log(3), upper = log(50)),
  ParamDbl$new(&quot;distance&quot;, lower = 1, upper = 3),
  ParamFct$new(&quot;kernel&quot;, c(&quot;rectangular&quot;, &quot;gaussian&quot;, &quot;rank&quot;, &quot;optimal&quot;)),
  ParamLgl$new(&quot;scale&quot;)
))

large_searchspace$trafo = function(x, param_set) {
  x$k = round(exp(x$k))
  x
}</code></pre>
<ul>
<li><strong>Problem 2</strong>: Using grid search will take a long time.</li>
<li>Even trying out three different values for <code>k</code>, <code>distance</code>, <code>kernel</code>, and the two values for <code>scale</code> will take 54 evaluations.</li>
<li>We use a different search algorithm: <strong>Random Search</strong></li>
<li>The <em>tuning instance</em> must now contain a <em>termination criterion</em>: When do we stop?</li>
</ul>
<pre class="r"><code>tuner_random = tnr(&quot;random_search&quot;, batch_size = 36)

instance_random = TuningInstance$new(
  task = task,
  learner = knn,
  resampling = cv10_instance,
  measures = msr(&quot;classif.ce&quot;),
  param_set = large_searchspace,
  terminator = term(&quot;evals&quot;, n_evals = 36)
)</code></pre>
<pre class="r"><code>tuner_random$tune(instance_random)
instance_random$result
#&gt; $tune_x
#&gt; $tune_x$k
#&gt; [1] 2.883326
#&gt; 
#&gt; $tune_x$distance
#&gt; [1] 2.210032
#&gt; 
#&gt; $tune_x$kernel
#&gt; [1] &quot;rank&quot;
#&gt; 
#&gt; $tune_x$scale
#&gt; [1] TRUE
#&gt; 
#&gt; 
#&gt; $params
#&gt; $params$kernel
#&gt; [1] &quot;rank&quot;
#&gt; 
#&gt; $params$k
#&gt; [1] 18
#&gt; 
#&gt; $params$distance
#&gt; [1] 2.210032
#&gt; 
#&gt; $params$scale
#&gt; [1] TRUE
#&gt; 
#&gt; 
#&gt; $perf
#&gt; classif.ce 
#&gt;      0.252</code></pre>
<ul>
<li>We can get the “archive” in two ways: expand the <code>&quot;tune_x&quot;</code> parameters (the points we sampled on the search space, before transformation):</li>
</ul>
<pre class="r"><code>perfdata = instance_random$archive(&quot;tune_x&quot;)
perfdata[, c(&quot;k&quot;, &quot;distance&quot;, &quot;kernel&quot;, &quot;scale&quot;, &quot;classif.ce&quot;)]
#&gt;            k distance      kernel scale classif.ce
#&gt;  1: 2.161569 2.640253    gaussian FALSE      0.312
#&gt;  2: 2.627261 1.073312        rank  TRUE      0.258
#&gt;  3: 3.312797 2.251166 rectangular FALSE      0.294
#&gt;  4: 1.999786 1.866958        rank FALSE      0.347
#&gt;  5: 3.456359 1.713852     optimal  TRUE      0.258
#&gt;  6: 3.907720 2.547335    gaussian FALSE      0.293
#&gt;  7: 1.699115 1.346904 rectangular  TRUE      0.274
#&gt;  8: 2.328907 1.986418 rectangular FALSE      0.323
#&gt;  9: 2.164135 2.474378    gaussian FALSE      0.310
#&gt; 10: 3.815176 2.022002    gaussian  TRUE      0.274
#&gt; 11: 2.902336 1.950174 rectangular FALSE      0.307
#&gt; 12: 3.366602 2.175800    gaussian FALSE      0.296
#&gt; 13: 3.684312 2.145075        rank FALSE      0.294
#&gt; 14: 1.263541 2.309835        rank  TRUE      0.277
#&gt; 15: 2.442418 2.730251        rank FALSE      0.307
#&gt; 16: 3.059230 1.793693    gaussian FALSE      0.298
#&gt; 17: 1.224292 2.920232 rectangular FALSE      0.368
#&gt; 18: 3.227954 1.761755     optimal FALSE      0.303
#&gt; 19: 2.763233 1.340177        rank FALSE      0.307
#&gt; 20: 2.533847 1.463210    gaussian  TRUE      0.254
#&gt; 21: 2.371967 2.899840 rectangular  TRUE      0.262
#&gt; 22: 1.170304 1.167016 rectangular  TRUE      0.268
#&gt; 23: 3.804673 1.386088        rank FALSE      0.291
#&gt; 24: 3.877214 1.149391 rectangular  TRUE      0.274
#&gt; 25: 2.883326 2.210032        rank  TRUE      0.252
#&gt; 26: 2.295835 1.431686 rectangular  TRUE      0.260
#&gt; 27: 1.848740 2.175975     optimal  TRUE      0.270
#&gt; 28: 1.325287 1.457795     optimal FALSE      0.398
#&gt; 29: 2.027436 1.781850 rectangular  TRUE      0.258
#&gt; 30: 3.613942 1.695812        rank FALSE      0.295
#&gt; 31: 1.961416 2.836759        rank FALSE      0.341
#&gt; 32: 2.230796 1.224078     optimal FALSE      0.347
#&gt; 33: 2.712923 1.509952    gaussian FALSE      0.307
#&gt; 34: 2.778186 1.373248        rank FALSE      0.305
#&gt; 35: 2.702305 2.491087 rectangular FALSE      0.303
#&gt; 36: 1.712136 2.865938        rank  TRUE      0.265
#&gt;            k distance      kernel scale classif.ce</code></pre>
<ul>
<li>Or look at the <code>&quot;params&quot;</code> parameters (the points the <code>Learner</code> was used with—these are the <code>exp()</code>’d parameters, after transformation of the sampled points):</li>
</ul>
<pre class="r"><code>perfdata = instance_random$archive(&quot;params&quot;)
perfdata[, c(&quot;k&quot;, &quot;distance&quot;, &quot;kernel&quot;, &quot;scale&quot;, &quot;classif.ce&quot;)]
#&gt;      k distance      kernel scale classif.ce
#&gt;  1:  9 2.640253    gaussian FALSE      0.312
#&gt;  2: 14 1.073312        rank  TRUE      0.258
#&gt;  3: 27 2.251166 rectangular FALSE      0.294
#&gt;  4:  7 1.866958        rank FALSE      0.347
#&gt;  5: 32 1.713852     optimal  TRUE      0.258
#&gt;  6: 50 2.547335    gaussian FALSE      0.293
#&gt;  7:  5 1.346904 rectangular  TRUE      0.274
#&gt;  8: 10 1.986418 rectangular FALSE      0.323
#&gt;  9:  9 2.474378    gaussian FALSE      0.310
#&gt; 10: 45 2.022002    gaussian  TRUE      0.274
#&gt; 11: 18 1.950174 rectangular FALSE      0.307
#&gt; 12: 29 2.175800    gaussian FALSE      0.296
#&gt; 13: 40 2.145075        rank FALSE      0.294
#&gt; 14:  4 2.309835        rank  TRUE      0.277
#&gt; 15: 12 2.730251        rank FALSE      0.307
#&gt; 16: 21 1.793693    gaussian FALSE      0.298
#&gt; 17:  3 2.920232 rectangular FALSE      0.368
#&gt; 18: 25 1.761755     optimal FALSE      0.303
#&gt; 19: 16 1.340177        rank FALSE      0.307
#&gt; 20: 13 1.463210    gaussian  TRUE      0.254
#&gt; 21: 11 2.899840 rectangular  TRUE      0.262
#&gt; 22:  3 1.167016 rectangular  TRUE      0.268
#&gt; 23: 45 1.386088        rank FALSE      0.291
#&gt; 24: 48 1.149391 rectangular  TRUE      0.274
#&gt; 25: 18 2.210032        rank  TRUE      0.252
#&gt; 26: 10 1.431686 rectangular  TRUE      0.260
#&gt; 27:  6 2.175975     optimal  TRUE      0.270
#&gt; 28:  4 1.457795     optimal FALSE      0.398
#&gt; 29:  8 1.781850 rectangular  TRUE      0.258
#&gt; 30: 37 1.695812        rank FALSE      0.295
#&gt; 31:  7 2.836759        rank FALSE      0.341
#&gt; 32:  9 1.224078     optimal FALSE      0.347
#&gt; 33: 15 1.509952    gaussian FALSE      0.307
#&gt; 34: 16 1.373248        rank FALSE      0.305
#&gt; 35: 15 2.491087 rectangular FALSE      0.303
#&gt; 36:  6 2.865938        rank  TRUE      0.265
#&gt;      k distance      kernel scale classif.ce</code></pre>
<p>Let’s look at some plots of performance by parameter. - The following suggests that <code>scale</code> has a strong influence on performance.</p>
<pre class="r"><code>ggplot(perfdata, aes(x = k, y = classif.ce, color = scale)) +
  geom_point(size = 3) + geom_line()</code></pre>
<p><img src="/post/mlr3tuning_tutorial_german_credit_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<ul>
<li>As for the kernel, there does not seem to be a strong influence:</li>
</ul>
<pre class="r"><code>ggplot(perfdata, aes(x = k, y = classif.ce, color = kernel)) +
  geom_point(size = 3) + geom_line()</code></pre>
<p><img src="/post/mlr3tuning_tutorial_german_credit_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
</div>
<div id="nested-resampling" class="section level2">
<h2>Nested Resampling</h2>
<p>What performance do we expect from our tuned method? Naive evaluation:</p>
<pre class="r"><code>instance_random$result$perf
#&gt; classif.ce 
#&gt;      0.252
instance_grid$result$perf
#&gt; classif.ce 
#&gt;      0.249</code></pre>
<ul>
<li>Problem: <em>overtuning</em>:</li>
<li>The more we search, the more our result is likely to just be “lucky” on your training data</li>
<li>Imagine predicting random values and “tuning” the seed value. If we try enough seeds we may get good (tuning set) performance!</li>
<li>Different search spaces or search methods may introduce different amounts of randomness, so even the comparison is flawed!</li>
<li>Solution: Nested Resampling</li>
</ul>
<p>The <code>mlr3tuning</code> package provides an <code>AutoTuner</code> that acts like our tuning method is actually a <code>Learner</code>!</p>
<ul>
<li><code>$train()</code> method:</li>
<li>Tune hyperparameters on the training data, using a resampling strategy (below 5 fold CV).</li>
<li>Train a model with optimal hyperparameters on whole training data.</li>
<li><code>$predict()</code> method: use model trained on the whole training data as model.</li>
<li>This is just the workflow we use when tuning hyperparameters: Find the best parameters and use them for training.</li>
<li>The <code>AutoTuner</code> does exactly this.</li>
</ul>
<pre class="r"><code>grid_auto = AutoTuner$new(
  learner = knn,
  resampling = rsmp(&quot;cv&quot;, folds = 5), # we can NOT use fixed resampling here
  measures = msr(&quot;classif.ce&quot;),
  tune_ps = searchspace,
  terminator = term(&quot;none&quot;),
  tuner = tnr(&quot;grid_search&quot;, resolution = 18)
)</code></pre>
<ul>
<li>The autotuner behaves just like a <code>Learner</code>. It can be used to combine the steps of hyperparameter tuning and model fitting, but is especially useful for resampling and fair comparison of performance through benchmarking.</li>
</ul>
<pre class="r"><code>resample(task, grid_auto, cv10_instance)$aggregate()</code></pre>
<ul>
<li>Essentially this is the performance of a “knn with optimal hyperparameters found by grid search”</li>
</ul>
</div>
<div id="appendix" class="section level2">
<h2>Appendix</h2>
<div id="example-tuning-with-larger-budget" class="section level3">
<h3>Example: Tuning With Larger Budget</h3>
<p>It is always interesting to look at what could have been. The following dataset contains an optimization run result with 3600 evaluations–more than above by a factor of 100.</p>
<pre class="r"><code>perfdata # obtained from mlr-outreach github repository
#&gt;         nr batch_nr    task_id   learner_id resampling_id iters warnings errors classif.ce tune_x      kernel
#&gt;    1:    1        1 new_credit classif.kknn            cv    10        0      0      0.348 &lt;list&gt;        rank
#&gt;    2:    2        1 new_credit classif.kknn            cv    10        0      0      0.297 &lt;list&gt;        rank
#&gt;    3:    3        1 new_credit classif.kknn            cv    10        0      0      0.321 &lt;list&gt;    gaussian
#&gt;    4:    4        1 new_credit classif.kknn            cv    10        0      0      0.256 &lt;list&gt;        rank
#&gt;    5:    5        1 new_credit classif.kknn            cv    10        0      0      0.299 &lt;list&gt;     optimal
#&gt;   ---                                                                                                        
#&gt; 3596: 3596        1 new_credit classif.kknn            cv    10        0      0      0.297 &lt;list&gt; rectangular
#&gt; 3597: 3597        1 new_credit classif.kknn            cv    10        0      0      0.342 &lt;list&gt;        rank
#&gt; 3598: 3598        1 new_credit classif.kknn            cv    10        0      0      0.311 &lt;list&gt;     optimal
#&gt; 3599: 3599        1 new_credit classif.kknn            cv    10        0      0      0.273 &lt;list&gt; rectangular
#&gt; 3600: 3600        1 new_credit classif.kknn            cv    10        0      0      0.351 &lt;list&gt;        rank
#&gt;        k distance scale
#&gt;    1:  6 1.069610 FALSE
#&gt;    2: 40 1.077027 FALSE
#&gt;    3: 10 2.890817 FALSE
#&gt;    4:  5 2.541628  TRUE
#&gt;    5: 45 2.921566 FALSE
#&gt;   ---                  
#&gt; 3596: 30 1.185884 FALSE
#&gt; 3597:  7 1.678671 FALSE
#&gt; 3598: 22 1.688389 FALSE
#&gt; 3599: 28 1.940716  TRUE
#&gt; 3600:  3 2.683639 FALSE</code></pre>
<ul>
<li>The scale effect is just as visible.</li>
</ul>
<pre class="r"><code>ggplot(perfdata, aes(x = k, y = classif.ce, color = scale)) +
  geom_point(size = 2, alpha = 0.3)</code></pre>
<p><img src="/post/mlr3tuning_tutorial_german_credit_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<ul>
<li>There seems to be a pattern by kernel as well…</li>
</ul>
<pre class="r"><code>ggplot(perfdata, aes(x = k, y = classif.ce, color = kernel)) +
  geom_point(size = 2, alpha = 0.3)</code></pre>
<p><img src="/post/mlr3tuning_tutorial_german_credit_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<ul>
<li>In fact, if we zoom in to <code>(5, 30)</code> x <code>(0.2, 0.3)</code> and do loess smoothing we see that different kernels have their optimum at different <code>k</code>.</li>
</ul>
<pre class="r"><code>ggplot(perfdata, aes(x = k, y = classif.ce, color = kernel,
  group = interaction(kernel, scale))) +
  geom_point(size = 2, alpha = 0.3) + geom_smooth() +
  xlim(5, 30) + ylim(0.2, 0.3)
#&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/mlr3tuning_tutorial_german_credit_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<ul>
<li>What about the <code>distance</code> parameter? If we select all results with <code>k</code> between 10 and 20 and plot distance and kernel we see an approximate relationship</li>
</ul>
<pre class="r"><code>ggplot(perfdata[k &gt; 10 &amp; k &lt; 20 &amp; scale == TRUE],
  aes(x = distance, y = classif.ce, color = kernel)) +
  geom_point(size = 2) + geom_smooth()
#&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/mlr3tuning_tutorial_german_credit_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<ul>
<li>Observations:</li>
<li>The <code>scale</code> makes a lot of difference</li>
<li>The <code>distance</code> seems to make the least difference</li>
<li>Had we done grid search, we would have wasted a lot of evaluations on trying different <code>distance</code> values that usually give similar results. This is why random search works well.</li>
<li>An even more intelligent approach would be to observe that <code>scale = FALSE</code> performs badly and not try out so many points with that one.</li>
</ul>
</div>
<div id="very-quick-paradox-primer" class="section level3">
<h3>Very quick <code>paradox</code> primer</h3>
<p>Initialization:</p>
<pre class="r"><code>ParamSet$new(list( &lt;PARAMETERS&gt; ))</code></pre>
<p>Possible parameter types:</p>
<pre class="r"><code>## - logical (values TRUE, FALSE)
ParamLgl$new(&quot;parameter_id&quot;)
## - factorial (discrete values from a list of &#39;levels&#39;)
ParamFct$new(&quot;parameter_id&quot;, c(&quot;value1&quot;, &quot;value2&quot;, &quot;value3&quot;))
## - integer (from &#39;lower&#39; to &#39;upper&#39; bound)
ParamInt$new(&quot;parameter_id&quot;, lower = 0, upper = 10)
## - numeric (from &#39;lower&#39; to &#39;upper&#39; bound)
## - unfortunately named after the storage type, &quot;double precision floating point&quot;
ParamDbl$new(&quot;parameter_id&quot;, lower = 0, upper = 10)

## Also possible: &quot;untyped&quot;, but we can not tune with this!
ParamUty$new(&quot;parameter_id&quot;)</code></pre>
<p>So an example parameter set with one logical parameter <code>&quot;flag&quot;</code> and one integer parameter <code>&quot;count&quot;</code>:</p>
<pre class="r"><code>ParamSet$new(list(
  ParamLgl$new(&quot;flag&quot;),
  ParamInt$new(&quot;count&quot;, lower = 0, upper = 10)
))
#&gt; ParamSet: 
#&gt;       id    class lower upper      levels     default value
#&gt; 1:  flag ParamLgl    NA    NA  TRUE,FALSE &lt;NoDefault&gt;      
#&gt; 2: count ParamInt     0    10             &lt;NoDefault&gt;</code></pre>
<p>See the <a href="https://mlr3book.mlr-org.com/paradox.html">online vignette</a> of <code>paradox</code> for a more complete introduction.</p>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

