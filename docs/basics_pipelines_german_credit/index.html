<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.67.0" />


<title>mlr3pipelines tutorial - german credit - mlr3 gallery</title>
<meta property="og:title" content="mlr3pipelines tutorial - german credit - mlr3 gallery">


  <link href='/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/gruvbox-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">

<link rel="stylesheet" href="/css/custom.css">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="mlr logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/mlr-org/mlr3gallery">GitHub</a></li>
    
    <li><a href="https://mlr3.mlr-org.com">mlr3</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">11 min read</span>
    

    <h1 class="article-title">mlr3pipelines tutorial - german credit</h1>

    
    <span class="article-date">2020-03-11 by Martin Binder &amp; Florian Pfisterer</span>
    

    <div class="article-content">
      
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<link href="/rmarkdown-libs/vis/vis.css" rel="stylesheet" />
<script src="/rmarkdown-libs/vis/vis.min.js"></script>
<script src="/rmarkdown-libs/visNetwork-binding/visNetwork.js"></script>


<div id="intro" class="section level2">
<h2>Intro</h2>
<p>This is the second part in a serial of tutorials. The other parts of this series can be found here: * <a href="/basics_german_credit/">Part I</a> * <a href="/basics_tuning_german_credit/">Part III</a></p>
<p>In this tutorial we will continue working with the <strong>German Credit Dataset</strong>. We already used different <code>Learner</code>s on it and tried to optimize their hyperparameters. Now we will</p>
<ul>
<li>preprocess the data as an integrated step of the model fitting process</li>
<li>tune the preprocessing parameters</li>
<li>use multiple <code>Learners</code> in an <em>ensemble</em> model</li>
<li>see some techniques that make <code>Learner</code>s able to tackle challenging datasets that they could not handle otherwise.</li>
</ul>
</div>
<div id="prerequisites" class="section level2">
<h2>Prerequisites</h2>
<pre class="r"><code>library(&quot;data.table&quot;)
library(&quot;mlr3&quot;)
library(&quot;mlr3learners&quot;)
library(&quot;ggplot2&quot;)
theme_set(theme_light())
library(&quot;mlr3tuning&quot;)</code></pre>
<p>We use the same data as in the mlr3_basics_german_credit example, but will restrict ourselves to the <em>numerical features</em>. To make things interesting, we introduce <em>missing values</em> in the dataset.</p>
<pre class="r"><code>task = tsk(&quot;german_credit&quot;)
credit_full = task$data()
credit = credit_full[, sapply(credit_full, is.numeric), with = FALSE]

set.seed(20191101)
## turn 10% of values into an NA
credit = credit[, lapply(.SD, function(x)
  x[sample(c(TRUE, NA), length(x), replace = TRUE, prob = c(.9, .1))])]
credit$credit_risk = credit_full$credit_risk
task = TaskClassif$new(&quot;GermanCredit&quot;, credit, &quot;credit_risk&quot;)</code></pre>
<ul>
<li>We instantiate a resampling instance for this task to be able to compare resampling performance.</li>
</ul>
<pre class="r"><code>set.seed(20191101)
cv10_instance = rsmp(&quot;cv&quot;)$instantiate(task)</code></pre>
<p>Uncomment the following line if you are running this locally (i.e. not on RStudio Cloud).</p>
<pre class="r"><code># future::plan(&quot;multiprocess&quot;)</code></pre>
</div>
<div id="intro-1" class="section level2">
<h2>Intro</h2>
<p>In this tutorial we will take a look at composite machine learning algorithms that may incorporate data preprocessing or the combination of multiple <code>Learner</code>s (“ensemble methods”).</p>
<ul>
<li>The package we use is <strong>mlr3pipelines</strong>, which enables us to chain “<code>PipeOp</code>” objects into data flow graphs. Load the package using</li>
</ul>
<pre class="r"><code>library(&quot;mlr3pipelines&quot;)</code></pre>
<ul>
<li>Available <code>PipeOp</code>s are enumareted in the <code>mlr_pipeops</code> dictionary.</li>
</ul>
<pre class="r"><code>mlr_pipeops
#&gt; &lt;DictionaryPipeOp&gt; with 43 stored values
#&gt; Keys: boxcox, branch, chunk, classbalancing, classifavg, classweights, colapply, collapsefactors,
#&gt;   copy, datefeatures, encode, encodeimpact, encodelmer, featureunion, filter, fixfactors, histbin,
#&gt;   ica, imputehist, imputemean, imputemedian, imputenewlvl, imputesample, kernelpca, learner,
#&gt;   learner_cv, missind, modelmatrix, mutate, nop, pca, quantilebin, regravg, removeconstants,
#&gt;   scale, scalemaxabs, scalerange, select, smote, spatialsign, subsample, unbranch, yeojohnson</code></pre>
</div>
<div id="missing-value-imputation" class="section level2">
<h2>Missing Value Imputation</h2>
<ul>
<li>Trying to train a Random Forest fails because the model can not handle missing values.</li>
</ul>
<pre class="r"><code>ranger = lrn(&quot;classif.ranger&quot;)
ranger$train(task)
#&gt; Error: Missing data in columns: age, amount, duration, installment_rate, number_credits, people_liable, present_residence.</code></pre>
<ul>
<li>We can impute using a <code>PipeOp</code>. What are the imputation <code>PipeOp</code>s?</li>
</ul>
<pre class="r"><code>mlr_pipeops$keys(&quot;^impute&quot;)
#&gt; [1] &quot;imputehist&quot;   &quot;imputemean&quot;   &quot;imputemedian&quot; &quot;imputenewlvl&quot; &quot;imputesample&quot;</code></pre>
<ul>
<li>We choose to impute numeric features by their median. If we had factorial features, we could also add an imputer for them, e.g. <code>po(&quot;imputenewlvl&quot;)</code>.</li>
<li>Let’s use the <code>PipeOp</code> itself to create an imputed task. This shows us how the <code>PipeOp</code> works.</li>
</ul>
<pre class="r"><code>imputer = po(&quot;imputemean&quot;)

task_imputed = imputer$train(list(task))[[1]]

task_imputed$head()  # no missing values
#&gt;    credit_risk age   amount duration installment_rate number_credits people_liable present_residence
#&gt; 1:        good  67 1169.000        6          4.00000              2             1                 4
#&gt; 2:         bad  22 5951.000       48          2.99011              1             1                 2
#&gt; 3:        good  49 2096.000       12          2.99011              1             2                 3
#&gt; 4:        good  45 7882.000       42          2.00000              1             2                 4
#&gt; 5:         bad  53 4870.000       24          3.00000              2             2                 4
#&gt; 6:        good  35 3285.826       36          2.00000              1             2                 4</code></pre>
<ul>
<li>The <code>$state$model</code> slot contains the medians of all columns. The <code>PipeOp</code> needs to remember these to impute missing values in new data during the <code>$predict()</code> phase.</li>
</ul>
<pre class="r"><code>imputer$state$model
#&gt; $age
#&gt; [1] 35.66329
#&gt; 
#&gt; $amount
#&gt; [1] 3285.826
#&gt; 
#&gt; $duration
#&gt; [1] 20.94919
#&gt; 
#&gt; $installment_rate
#&gt; [1] 2.99011
#&gt; 
#&gt; $number_credits
#&gt; [1] 1.406285
#&gt; 
#&gt; $people_liable
#&gt; [1] 1.153761
#&gt; 
#&gt; $present_residence
#&gt; [1] 2.854645</code></pre>
<ul>
<li>If we used the imputed task for resampling, we would leak information from the test set into the training set. Therefore it is mandatory to attach the imputation operator to the <code>Learner</code> itself, creating a <code>GraphLearner</code>.</li>
</ul>
<pre class="r"><code>imp_ranger = GraphLearner$new(po(&quot;imputemean&quot;) %&gt;&gt;% ranger)

imp_ranger$train(task)  # runs without error: training succeeds</code></pre>
<ul>
<li>This can be used for resampling.</li>
</ul>
<pre class="r"><code>rr = resample(task, imp_ranger, cv10_instance)
rr$aggregate()
#&gt; classif.ce 
#&gt;      0.298</code></pre>
</div>
<div id="feature-filtering" class="section level2">
<h2>Feature Filtering</h2>
<ul>
<li>Sometimes having fewer features is desirable (interpretability, cost of acquiring data, possibly even better performance)</li>
<li>Use <em>feature filter</em> to preferentially keep features with most information</li>
</ul>
<pre class="r"><code>library(&quot;mlr3filters&quot;)
mlr_filters
#&gt; &lt;DictionaryFilter&gt; with 16 stored values
#&gt; Keys: anova, auc, carscore, cmim, correlation, disr, importance, information_gain, jmi, jmim,
#&gt;   kruskal_test, mim, mrmr, njmim, performance, variance</code></pre>
<ul>
<li>We use the <code>&quot;anova&quot;</code> filter. It uses an F-test for values in different target classes (equivalent to a t-test in the binary classification case).</li>
</ul>
<pre class="r"><code>filter = flt(&quot;anova&quot;)
filter$calculate(task_imputed)$scores
#&gt;          duration            amount               age  installment_rate    number_credits present_residence 
#&gt;       10.71452277        5.70482677        2.62411784        1.14183395        0.81379060        0.12841352 
#&gt;     people_liable 
#&gt;        0.07345883</code></pre>
<ul>
<li>What is the tradeoff between features and performance? Let’s find out by tuning.</li>
<li>We incorporate our filtering in the pipeline using the <code>&quot;filter&quot;</code> <code>PipeOp</code></li>
<li>We remember that we also need to do imputation.</li>
</ul>
<pre class="r"><code>fpipe = po(&quot;imputemean&quot;) %&gt;&gt;% po(&quot;filter&quot;, filter, filter.nfeat = 3)

fpipe$train(task)[[1]]$head()
#&gt;    credit_risk age   amount duration
#&gt; 1:        good  67 1169.000        6
#&gt; 2:         bad  22 5951.000       48
#&gt; 3:        good  49 2096.000       12
#&gt; 4:        good  45 7882.000       42
#&gt; 5:         bad  53 4870.000       24
#&gt; 6:        good  35 3285.826       36</code></pre>
<ul>
<li>We are going to tune over the <code>anova.filter.nfeat</code> parameter; it regulates how many features are kept by the filter.</li>
</ul>
<pre class="r"><code>library(&quot;paradox&quot;)
searchspace = ParamSet$new(list(
  ParamInt$new(&quot;anova.filter.nfeat&quot;, lower = 1, upper = length(task$feature_names))
))</code></pre>
<ul>
<li>Because this is only one parameter, we will use grid search. For higher dimensions, random search is more appropriate.</li>
</ul>
<pre class="r"><code>inst = TuningInstance$new(
  task, fpipe %&gt;&gt;% lrn(&quot;classif.ranger&quot;), cv10_instance, msr(&quot;classif.ce&quot;),
  searchspace, term(&quot;none&quot;)
)
tuner = tnr(&quot;grid_search&quot;)</code></pre>
<ul>
<li>Tuning may take a while…</li>
</ul>
<pre class="r"><code>tuner$tune(inst)</code></pre>
<ul>
<li>If we plot the performance over the number of features, we see the possible tradeoffs between sparsity and predictive performance.</li>
</ul>
<pre class="r"><code>arx = inst$archive(&quot;params&quot;)
ggplot(arx, aes(x = anova.filter.nfeat, y = classif.ce)) + geom_line()</code></pre>
<p><img src="/post/mlr3pipelines_tutorial_german_credit_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
<div id="stacking" class="section level2">
<h2>Stacking</h2>
<ul>
<li>We build a model on the predictions of learners</li>
<li>This needs the <code>&quot;learner_cv&quot;</code> PipeOp, because predictions need to be available during training already</li>
<li>the <code>&quot;learner_cv&quot;</code> PipeOp performs crossvalidation during the training phase and emits the cross validated predictions.</li>
<li>We use <code>&quot;prob&quot;</code> prediction because it carries more information than response prediction</li>
</ul>
<pre class="r"><code>stackgraph = po(&quot;imputemean&quot;) %&gt;&gt;%
  list(
    po(&quot;learner_cv&quot;, lrn(&quot;classif.ranger&quot;, predict_type = &quot;prob&quot;)),
    po(&quot;learner_cv&quot;, lrn(&quot;classif.kknn&quot;, predict_type = &quot;prob&quot;))) %&gt;&gt;%
  po(&quot;featureunion&quot;) %&gt;&gt;% lrn(&quot;classif.log_reg&quot;)</code></pre>
<ul>
<li>What does this <code>Graph</code> look like? We can plot it!</li>
</ul>
<pre class="r"><code>stackgraph$plot(html = TRUE)</code></pre>
<div id="htmlwidget-1" style="width:50%;height:400px;" class="visNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"nodes":{"id":["imputemean","classif.ranger","classif.kknn","featureunion","<INPUT>","classif.log_reg","<OUTPUT>"],"label":["imputemean","classif.ranger","classif.kknn","featureunion","<INPUT>","classif.log_reg","<OUTPUT>"],"shape":["box","box","box","box","database","box","ellipse"],"color":["lightblue","lightblue","lightblue","lightblue","rgba(0,204,102,0.2)","lightblue","rgba(255,51,51,0.2)"],"value":[1,1,1,1,0.8,1,0.8],"title":["<p>PipeOp: <b>imputemean<\/b> (not trained)<br>values: <b>list()<\/b><br>Input channels <b>name [train type, predict type]<\/b>:<br>  input [Task,Task]<br>Output channels <b>name [train type, predict type]<\/b>:<br>  output [Task,Task]<\/p>","<p>PipeOp: <b>classif.ranger<\/b> (not trained)<br>values: <b>resampling.method=cv, resampling.folds=3, resampling.keep_response=FALSE<\/b><br>Input channels <b>name [train type, predict type]<\/b>:<br>  input [TaskClassif,TaskClassif]<br>Output channels <b>name [train type, predict type]<\/b>:<br>  output [TaskClassif,TaskClassif]<\/p>","<p>PipeOp: <b>classif.kknn<\/b> (not trained)<br>values: <b>resampling.method=cv, resampling.folds=3, resampling.keep_response=FALSE<\/b><br>Input channels <b>name [train type, predict type]<\/b>:<br>  input [TaskClassif,TaskClassif]<br>Output channels <b>name [train type, predict type]<\/b>:<br>  output [TaskClassif,TaskClassif]<\/p>","<p>PipeOp: <b>featureunion<\/b> (not trained)<br>values: <b>list()<\/b><br>Input channels <b>name [train type, predict type]<\/b>:<br>  ... [Task,Task]<br>Output channels <b>name [train type, predict type]<\/b>:<br>  output [Task,Task]<\/p>","<p>Input:<br>Name: imputemean.input<br>Train: Task<br>Predict: Task<\/p>","<p>PipeOp: <b>classif.log_reg<\/b> (not trained)<br>values: <b>list()<\/b><br>Input channels <b>name [train type, predict type]<\/b>:<br>  input [TaskClassif,TaskClassif]<br>Output channels <b>name [train type, predict type]<\/b>:<br>  output [NULL,PredictionClassif]<\/p>","<p>Output:<br>Name: classif.log_reg.output<br>Train: NULL<br>Predict: PredictionClassif<\/p>"],"x":[0,-1,1,0,0,0,0],"y":[-0.6,-0.2,-0.2,0.2,-1,0.6,1]},"edges":{"from":["imputemean","imputemean","classif.ranger","classif.kknn","featureunion","<INPUT>","classif.log_reg"],"to":["classif.ranger","classif.kknn","featureunion","featureunion","classif.log_reg","imputemean","<OUTPUT>"],"color":["lightblue","lightblue","lightblue","lightblue","lightblue","lightblue","lightblue"]},"nodesToDataframe":true,"edgesToDataframe":true,"options":{"width":"100%","height":"100%","nodes":{"shape":"dot","physics":false},"manipulation":{"enabled":false},"edges":{"arrows":"to","smooth":{"enabled":false,"forceDirection":"vertical"}},"physics":{"stabilization":false}},"groups":null,"width":"50%","height":"400px","idselection":{"enabled":false},"byselection":{"enabled":false},"main":null,"submain":null,"footer":null,"background":"rgba(0, 0, 0, 0)","igraphlayout":{"type":"full"}},"evals":[],"jsHooks":[]}</script>
<pre class="r"><code>rr = resample(task, stackgraph, cv10_instance, store_model = TRUE)
rr$aggregate()
#&gt; classif.ce 
#&gt;      0.293</code></pre>
<ul>
<li>Compare this to performance of individual <code>Learner</code>s. Note, however, that the difference is smaller than the variation in CV estimate.</li>
</ul>
<pre class="r"><code>bmr = benchmark(data.table(task = list(task),
  learner = list(GraphLearner$new(po(&quot;imputemean&quot;) %&gt;&gt;% lrn(&quot;classif.ranger&quot;)),
    GraphLearner$new(po(&quot;imputemean&quot;) %&gt;&gt;% lrn(&quot;classif.kknn&quot;)),
    GraphLearner$new(po(&quot;imputemean&quot;) %&gt;&gt;% lrn(&quot;classif.log_reg&quot;))),
  resampling = list(cv10_instance)))
bmr$aggregate()[, c(&quot;learner_id&quot;, &quot;classif.ce&quot;)]
#&gt;                    learner_id classif.ce
#&gt; 1:  imputemean.classif.ranger      0.290
#&gt; 2:    imputemean.classif.kknn      0.350
#&gt; 3: imputemean.classif.log_reg      0.291</code></pre>
<ul>
<li>If we train the stacked <code>Learner</code> and look into the model, we can see how “important” each of the stacked models is</li>
</ul>
<pre class="r"><code>stackgraph$train(task)
#&gt; $classif.log_reg.output
#&gt; NULL

summary(stackgraph$pipeops$classif.log_reg$state$model)
#&gt; 
#&gt; Call:
#&gt; stats::glm(formula = task$formula(), family = &quot;binomial&quot;, data = task$data(), 
#&gt;     model = FALSE)
#&gt; 
#&gt; Deviance Residuals: 
#&gt;     Min       1Q   Median       3Q      Max  
#&gt; -1.5521  -0.8489  -0.6985   1.2559   1.9143  
#&gt; 
#&gt; Coefficients: (2 not defined because of singularities)
#&gt;                          Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)                1.4389     0.3261   4.412 1.02e-05 ***
#&gt; classif.ranger.prob.good  -2.5411     0.5309  -4.787 1.69e-06 ***
#&gt; classif.ranger.prob.bad        NA         NA      NA       NA    
#&gt; classif.kknn.prob.good    -0.8290     0.3420  -2.424   0.0154 *  
#&gt; classif.kknn.prob.bad          NA         NA      NA       NA    
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 1221.7  on 999  degrees of freedom
#&gt; Residual deviance: 1168.3  on 997  degrees of freedom
#&gt; AIC: 1174.3
#&gt; 
#&gt; Number of Fisher Scoring iterations: 4</code></pre>
<ul>
<li>The Random Forest (<code>ranger</code>) contributes more to the outcome, as one would expect, because it is generally a stronger model.</li>
</ul>
</div>
<div id="robustify-preventing-new-prediction-factor-levels-and-other-problems" class="section level2">
<h2>Robustify: Preventing new Prediction Factor Levels and other Problems</h2>
<ul>
<li>Let’s shift contexts: We take the full German Credit dataset.</li>
</ul>
<pre class="r"><code>
task = tsk(&quot;german_credit&quot;)

task$head()
#&gt;    credit_risk age amount                           credit_history duration employment_duration
#&gt; 1:        good  67   1169  critical account/other credits existing        6      ... &gt;= 7 years
#&gt; 2:         bad  22   5951 existing credits paid back duly till now       48  1 &lt;= ... &lt; 4 years
#&gt; 3:        good  49   2096  critical account/other credits existing       12  4 &lt;= ... &lt; 7 years
#&gt; 4:        good  45   7882 existing credits paid back duly till now       42  4 &lt;= ... &lt; 7 years
#&gt; 5:         bad  53   4870          delay in paying off in the past       24  1 &lt;= ... &lt; 4 years
#&gt; 6:        good  35   9055 existing credits paid back duly till now       36  1 &lt;= ... &lt; 4 years
#&gt;    foreign_worker  housing installment_rate                       job number_credits other_debtors
#&gt; 1:            yes      own                4 skilled employee/official              2          none
#&gt; 2:            yes      own                2 skilled employee/official              1          none
#&gt; 3:            yes      own                2      unskilled - resident              1          none
#&gt; 4:            yes for free                2 skilled employee/official              1     guarantor
#&gt; 5:            yes for free                3 skilled employee/official              2          none
#&gt; 6:            yes for free                2      unskilled - resident              1          none
#&gt;    other_installment_plans people_liable                 personal_status_sex present_residence
#&gt; 1:                    none             1                       male : single                 4
#&gt; 2:                    none             1 female : divorced/separated/married                 2
#&gt; 3:                    none             2                       male : single                 3
#&gt; 4:                    none             2                       male : single                 4
#&gt; 5:                    none             2                       male : single                 4
#&gt; 6:                    none             2                       male : single                 4
#&gt;                                             property             purpose                    savings
#&gt; 1:                                       real estate domestic appliances unknown/no savings account
#&gt; 2:                                       real estate domestic appliances               ... &lt; 100 DM
#&gt; 3:                                       real estate          retraining               ... &lt; 100 DM
#&gt; 4: building society savings agreement/life insurance    radio/television               ... &lt; 100 DM
#&gt; 5:                               unknown/no property           car (new)               ... &lt; 100 DM
#&gt; 6:                               unknown/no property          retraining unknown/no savings account
#&gt;                 status telephone
#&gt; 1:          ... &lt; 0 DM       yes
#&gt; 2:   0 &lt;= ... &lt; 200 DM        no
#&gt; 3: no checking account        no
#&gt; 4:          ... &lt; 0 DM        no
#&gt; 5:          ... &lt; 0 DM        no
#&gt; 6: no checking account       yes</code></pre>
<ul>
<li>When training with a small datasset, or datasets with many factor levels, it is possible that not all possible factor levels are visible to the <code>Learner</code> during training. Prediction then fails because the <code>Learner</code> does not know how to handle unseen factor levels.</li>
</ul>
<pre class="r"><code>logreg = lrn(&quot;classif.log_reg&quot;)
logreg$train(task$clone()$filter(1:30))
logreg$predict(task)
#&gt; Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor job has new levels unemployed/unskilled - non-resident</code></pre>
<ul>
<li>Many <code>Learner</code>s can not handle new levels during prediction <span class="math inline">\(\Rightarrow\)</span> we use the <code>&quot;fixfactors&quot;</code> <code>PipeOp</code> to prevent that</li>
<li><code>&quot;fixfactors&quot;</code> introduces <code>NA</code> values; we may need to impute afterwards.</li>
<li><p><span class="math inline">\(\Rightarrow\)</span> We use <code>&quot;imputesample&quot;</code>, but with <code>affect_cols</code> set to only <em>factorial</em> features.</p></li>
<li><p>Columns that are all-constant may also be a problem:</p></li>
</ul>
<pre class="r"><code>logreg = lrn(&quot;classif.log_reg&quot;)
logreg$train(task$clone()$filter(1:2))
#&gt; Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]): contrasts can be applied only to factors with 2 or more levels</code></pre>
<ul>
<li>This can be fixed using <code>&quot;removeconstants&quot;</code></li>
<li>We get the following robustification pipeline:</li>
</ul>
<pre class="r"><code>robustify = po(&quot;fixfactors&quot;) %&gt;&gt;%
  po(&quot;removeconstants&quot;) %&gt;&gt;%
  po(&quot;imputesample&quot;, affect_columns = selector_type(c(&quot;ordered&quot;, &quot;factor&quot;)))

robustify$plot(html = TRUE)</code></pre>
<div id="htmlwidget-2" style="width:50%;height:400px;" class="visNetwork html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"nodes":{"id":["fixfactors","removeconstants","<INPUT>","imputesample","<OUTPUT>"],"label":["fixfactors","removeconstants","<INPUT>","imputesample","<OUTPUT>"],"shape":["box","box","database","box","ellipse"],"color":["lightblue","lightblue","rgba(0,204,102,0.2)","lightblue","rgba(255,51,51,0.2)"],"value":[1,1,0.8,1,0.8],"title":["<p>PipeOp: <b>fixfactors<\/b> (not trained)<br>values: <b>droplevels=TRUE<\/b><br>Input channels <b>name [train type, predict type]<\/b>:<br>  input [Task,Task]<br>Output channels <b>name [train type, predict type]<\/b>:<br>  output [Task,Task]<\/p>","<p>PipeOp: <b>removeconstants<\/b> (not trained)<br>values: <b>ratio=0, rel_tol=1e-08, abs_tol=1e-08, na_ignore=TRUE<\/b><br>Input channels <b>name [train type, predict type]<\/b>:<br>  input [Task,Task]<br>Output channels <b>name [train type, predict type]<\/b>:<br>  output [Task,Task]<\/p>","<p>Input:<br>Name: fixfactors.input<br>Train: Task<br>Predict: Task<\/p>","<p>PipeOp: <b>imputesample<\/b> (not trained)<br>values: <b>affect_columns=<Selector><\/b><br>Input channels <b>name [train type, predict type]<\/b>:<br>  input [Task,Task]<br>Output channels <b>name [train type, predict type]<\/b>:<br>  output [Task,Task]<\/p>","<p>Output:<br>Name: imputesample.output<br>Train: Task<br>Predict: Task<\/p>"],"x":[0,0,0,0,0],"y":[-0.5,0,-1,0.5,1]},"edges":{"from":["fixfactors","removeconstants","<INPUT>","imputesample"],"to":["removeconstants","imputesample","fixfactors","<OUTPUT>"],"color":["lightblue","lightblue","lightblue","lightblue"]},"nodesToDataframe":true,"edgesToDataframe":true,"options":{"width":"100%","height":"100%","nodes":{"shape":"dot","physics":false},"manipulation":{"enabled":false},"edges":{"arrows":"to","smooth":{"enabled":false,"forceDirection":"vertical"}},"physics":{"stabilization":false}},"groups":null,"width":"50%","height":"400px","idselection":{"enabled":false},"byselection":{"enabled":false},"main":null,"submain":null,"footer":null,"background":"rgba(0, 0, 0, 0)","igraphlayout":{"type":"full"}},"evals":[],"jsHooks":[]}</script>
<ul>
<li>This works even in very pathological conditions.</li>
<li>You may need to combine it with imputation if the data could have missing values.</li>
</ul>
<pre class="r"><code>roblogreg = GraphLearner$new(robustify %&gt;&gt;% logreg)

roblogreg$train(task$clone()$filter(1:2))
roblogreg$predict(task)
#&gt; &lt;PredictionClassif&gt; for 1000 observations:
#&gt;     row_id truth response
#&gt;          1  good     good
#&gt;          2   bad      bad
#&gt;          3  good     good
#&gt; ---                      
#&gt;        998  good      bad
#&gt;        999   bad      bad
#&gt;       1000  good      bad</code></pre>
</div>
<div id="encoding-categorical-features" class="section level2">
<h2>Encoding Categorical Features</h2>
<p>Some <code>Learner</code>s, even important ones like <code>xgboost</code>, can not handle categorical features.</p>
<pre class="r"><code>xgb = lrn(&quot;classif.xgboost&quot;)
xgb$train(task)
#&gt; Error: &lt;TaskClassif:german_credit&gt; has the following unsupported feature types: factor, ordered</code></pre>
<ul>
<li>Use <code>po(&quot;encode&quot;)</code>, or <code>po(&quot;encodeimpact&quot;)</code> to perform factor encoding.</li>
<li><code>&quot;encode&quot;</code> does one-hot encoding or similar</li>
<li><code>&quot;encodeimpact&quot;</code> does impact-encoding, which may work better for features with many factor levels</li>
</ul>
<pre class="r"><code>xgb_all = GraphLearner$new(po(&quot;encode&quot;) %&gt;&gt;% xgb)

xgb_all$train(task)  # runs without error</code></pre>
</div>
<div id="your-ideas" class="section level2">
<h2>Your Ideas!</h2>
<ul>
<li>Try different methods for preprocessing and training</li>
<li>Some hints:</li>
<li>It is not allowed to have two <code>PipeOp</code>s with the same <code>ID</code> in a <code>Graph</code>. Initialize a <code>PipeOp</code> with <code>po(&quot;...&quot;, id = &quot;xyz&quot;)</code> to change its ID on construction</li>
<li>If you build large <code>Graph</code>s involving complicated optimizations, like too many <code>&quot;learner_cv&quot;</code>, then they may need a long time to train</li>
<li>Use the <code>affect_columns</code> parameter if you want a <code>PipeOp</code> to only operate on part of the data. Use <code>po(&quot;select&quot;)</code> if you want to remove certain columns (possibly only along a single branch of multiple parallel branches). Both take <code>selector_XXX()</code> arguments, e.g. <code>selector_type(&quot;integer&quot;)</code></li>
<li>You may get the best performance if you actually inspect the features and see what kind of transformations work best for them.</li>
<li>See what <code>PipeOp</code>s are available by inspecting <code>mlr_pipeops$keys()</code>, and get help about them using <code>?mlr_pipeops_XXX</code>.</li>
</ul>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

