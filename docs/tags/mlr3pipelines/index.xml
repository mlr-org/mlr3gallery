<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mlr3pipelines on mlr3 gallery</title>
    <link>/tags/mlr3pipelines/</link>
    <description>Recent content in mlr3pipelines on mlr3 gallery</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/mlr3pipelines/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A pipeline for the titanic data set</title>
      <link>/basics_pipelines_titanic/</link>
      <pubDate>Thu, 12 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/basics_pipelines_titanic/</guid>
      <description>Intro First of all we are going to load required packages and the data. The data is part of the mlr3data package.
library(&amp;quot;mlr3&amp;quot;) library(&amp;quot;mlr3learners&amp;quot;) library(&amp;quot;mlr3pipelines&amp;quot;) library(&amp;quot;mlr3data&amp;quot;) library(&amp;quot;mlr3misc&amp;quot;) data(&amp;quot;titanic&amp;quot;) The titanic data is very interesting to analyze, even though it is part of many tutorials and showcases. This is because it requires many steps often required in real-world applications of machine learning techniques, such as feature engineering, missing value imputation, handling factors and others.</description>
    </item>
    
    <item>
      <title>Select uncorrelated features</title>
      <link>/select_uncorrelated_features/</link>
      <pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/select_uncorrelated_features/</guid>
      <description>The following example describes a situation where we aim to remove correlated features. This in essence means, that we drop features until no features have a correlation higher then a given cutoff. This is often useful when we for example want to use linear models.
Prerequisites This tutorial assumes familiarity with the basics of mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. Additionally, we compare different cutoff values via tuning using the mlr3tuning package.</description>
    </item>
    
    <item>
      <title>Feature Engineering of Date-Time Variables</title>
      <link>/date-features/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/date-features/</guid>
      <description>In this tutorial, we demonstrate how mlr3pipelines can be used to easily engineer features based on date-time variables. Relying on the Bike Sharing Dataset and the ranger learner we compare the RMSE of a random forest using the original features (baseline), to the RMSE of a random forest using newly engineered features on top of the original ones.
Motivation A single Date-Time variable (i.e., a POSIXct column) contains plenty of information ranging from year, month, day, hour, minute and second to other features such as week of the year, or day of the week.</description>
    </item>
    
    <item>
      <title>Encode factor levels for xgboost</title>
      <link>/encode-factors-for-xgboost/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/encode-factors-for-xgboost/</guid>
      <description>The package xgboost unfortunately does not support handling of categorical features. Therefore it is required to manually convert factor columns to numerical dummy features. We show how to use mlr3pipelines to augment the xgboost learner with an automatic factor encoding.
Construct the Base Objects First, we take an example task with factors (german_credit) and create the xgboost learner:
library(mlr3) library(mlr3learners) task = tsk(&amp;quot;german_credit&amp;quot;) print(task) ## &amp;lt;TaskClassif:german_credit&amp;gt; (1000 x 21) ## * Target: credit_risk ## * Properties: twoclass ## * Features (20): ## - fct (12): credit_history, foreign_worker, housing, job, ## other_debtors, other_installment_plans, personal_status_sex, ## property, purpose, savings, status, telephone ## - dbl (7): age, amount, duration, installment_rate, number_credits, ## people_liable, present_residence ## - ord (1): employment_duration learner = lrn(&amp;quot;classif.</description>
    </item>
    
    <item>
      <title>Impute missing variables</title>
      <link>/impute-missing-variables/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/impute-missing-variables/</guid>
      <description>Prerequisites This tutorial assumes familiarity with the basics of mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. It deals with the problem of missing data.
The random forest implementation in the package ranger unfortunately does not support missing values. Therefore it is required to impute missing features before passing the data to the learner.
We show how to use mlr3pipelines to augment the ranger learner with automatic imputation.</description>
    </item>
    
  </channel>
</rss>