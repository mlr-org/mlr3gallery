<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tuning on mlr3 gallery</title>
    <link>/tags/tuning/</link>
    <description>Recent content in tuning on mlr3 gallery</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/tuning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>mlr3pipelines tutorial - german credit</title>
      <link>/basics_pipelines_german_credit/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/basics_pipelines_german_credit/</guid>
      <description>Intro This is the second part in a serial of tutorials. The other parts of this series can be found here: * Part I * Part III
In this tutorial we will continue working with the German Credit Dataset. We already used different Learners on it and tried to optimize their hyperparameters. Now we will
 preprocess the data as an integrated step of the model fitting process tune the preprocessing parameters use multiple Learners in an ensemble model see some techniques that make Learners able to tackle challenging datasets that they could not handle otherwise.</description>
    </item>
    
    <item>
      <title>mlr3tuning tutorial - german credit</title>
      <link>/basics_tuning_german_credit/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/basics_tuning_german_credit/</guid>
      <description>Intro This is the third part in a serial of tutorials. The other parts of this series can be found here: * Part I * Part II
In this case we will continue working with the German Credit Dataset. Yesterday we peeked into the data set by using and comparing some learners with ther default parameters. We will now see how to:
 Tune hyperparameters for a given problem Perform nested resampling Adjust decision thresholds   Prerequisites We expect you have installed all packages from day 1.</description>
    </item>
    
    <item>
      <title>Select uncorrelated features</title>
      <link>/select_uncorrelated_features/</link>
      <pubDate>Tue, 25 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/select_uncorrelated_features/</guid>
      <description>The following example describes a situation where we aim to remove correlated features. This in essence means, that we drop features until no features have a correlation higher then a given cutoff. This is often useful when we for example want to use linear models.
Prerequisites This tutorial assumes familiarity with the basics of mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. Additionally, we compare different cutoff values via tuning using the mlr3tuning package.</description>
    </item>
    
    <item>
      <title>Tuning Over Multiple Learners (Tuning Multiplexer)</title>
      <link>/tuning-over-multiple-learners/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/tuning-over-multiple-learners/</guid>
      <description>This use case shows how to tune over multiple learners for a single task. Following tasks are illustrated:
 Build a pipeline that can switch between multiple learners. Define the hyperparameter search space for the pipeline. Define transformations for single hyperparameters. Define a hierarchical order of the hyperparameters. Run a random search.  Build the Pipeline The pipeline just has a single purpose in this example: It should allow us to switch between different learners.</description>
    </item>
    
    <item>
      <title>House prices in King County</title>
      <link>/house-prices-in-king-county/</link>
      <pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/house-prices-in-king-county/</guid>
      <description>The use-case illustrated below touches on the following concepts:
 Data preprocessing Task Fitting a learner Resampling Tuning  The relevant sections in the mlr3book are linked to for the readerâ€™s convenience.
This use case shows how to model housing price data in King County. Following features are illustrated:
 Summarizing the data set Converting data to treat it as a numeric feature/factor Generating new variables Splitting data into train and test data sets Computing a first model (decision tree) Building many trees (random forest) Visualizing price data across different region Optimizing the baseline by implementing a tuner Engineering features Creating a sparser model  House Price Prediction in King County We use the kc_housing dataset contained in the package mlr3book in order to provide a use-case for the application of mlr3 on real-world data.</description>
    </item>
    
  </channel>
</rss>